{"prompt": "Create a simple 2-layer CNN that takes 64 input channels and outputs 128 channels", "code": "class SimpleCNN(nn.Module):\n    def __init__(self, in_channels=64, out_channels=128):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, 96, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(96, out_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        return x", "test_code": "", "difficulty": "easy", "category": "cnn", "source": "manual"}
{"prompt": "Create a residual block with skip connection, batch normalization, and ReLU activation", "code": "class ResBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        residual = x\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        return self.relu(out + residual)", "test_code": "", "difficulty": "medium", "category": "cnn", "source": "manual"}
{"prompt": "Create a multi-head self-attention layer with 8 heads and configurable embedding dimension", "code": "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model=256, n_heads=8):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, x):\n        B, L, _ = x.shape\n        \n        q = self.W_q(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n        k = self.W_k(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n        v = self.W_v(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n        \n        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)\n        attn = torch.softmax(scores, dim=-1)\n        \n        out = torch.matmul(attn, v)\n        out = out.transpose(1, 2).contiguous().view(B, L, self.d_model)\n        return self.W_o(out)", "test_code": "", "difficulty": "medium", "category": "attention", "source": "manual"}
{"prompt": "Create a bottleneck residual block used in ResNet-50 with 1x1, 3x3, 1x1 convolutions", "code": "class Bottleneck(nn.Module):\n    def __init__(self, in_channels, bottleneck_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, 1)\n        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv2 = nn.Conv2d(bottleneck_channels, bottleneck_channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        self.conv3 = nn.Conv2d(bottleneck_channels, out_channels, 1)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n        \n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        residual = self.shortcut(x)\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        return self.relu(out + residual)", "test_code": "", "difficulty": "hard", "category": "cnn", "source": "manual"}
{"prompt": "Create a depthwise separable convolution block like in MobileNet", "code": "class DepthwiseSeparable(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.bn1(self.depthwise(x)))\n        x = self.relu(self.bn2(self.pointwise(x)))\n        return x", "test_code": "", "difficulty": "medium", "category": "cnn", "source": "manual"}
{"prompt": "Create a squeeze-and-excitation block for channel attention", "code": "class SEBlock(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction),\n            nn.ReLU(),\n            nn.Linear(channels // reduction, channels),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        B, C, _, _ = x.shape\n        y = self.avg_pool(x).view(B, C)\n        y = self.fc(y).view(B, C, 1, 1)\n        return x * y", "test_code": "", "difficulty": "medium", "category": "attention", "source": "manual"}
{"prompt": "Create a transformer encoder block with multi-head attention, layer norm, and feedforward network", "code": "class TransformerBlock(nn.Module):\n    def __init__(self, d_model=256, n_heads=8, d_ff=1024, dropout=0.1):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)\n        ff_out = self.ff(x)\n        x = self.norm2(x + ff_out)\n        return x", "test_code": "", "difficulty": "hard", "category": "transformer", "source": "manual"}
{"prompt": "Create a convolutional block attention module (CBAM) with both channel and spatial attention", "code": "class CBAM(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        # Channel attention\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction),\n            nn.ReLU(),\n            nn.Linear(channels // reduction, channels)\n        )\n        # Spatial attention\n        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n    \n    def forward(self, x):\n        B, C, H, W = x.shape\n        # Channel attention\n        avg_out = self.fc(self.avg_pool(x).view(B, C))\n        max_out = self.fc(self.max_pool(x).view(B, C))\n        channel_att = torch.sigmoid(avg_out + max_out).view(B, C, 1, 1)\n        x = x * channel_att\n        # Spatial attention\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        spatial_att = torch.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1)))\n        return x * spatial_att", "test_code": "", "difficulty": "hard", "category": "attention", "source": "manual"}
{"prompt": "Create an inverted residual block like in MobileNetV2 with expansion, depthwise conv, and projection", "code": "class InvertedResidual(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, expand_ratio=6):\n        super().__init__()\n        hidden = in_channels * expand_ratio\n        self.use_residual = stride == 1 and in_channels == out_channels\n        \n        layers = []\n        if expand_ratio != 1:\n            layers.extend([\n                nn.Conv2d(in_channels, hidden, 1),\n                nn.BatchNorm2d(hidden),\n                nn.ReLU6()\n            ])\n        layers.extend([\n            nn.Conv2d(hidden, hidden, 3, stride, 1, groups=hidden),\n            nn.BatchNorm2d(hidden),\n            nn.ReLU6(),\n            nn.Conv2d(hidden, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        ])\n        self.conv = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        if self.use_residual:\n            return x + self.conv(x)\n        return self.conv(x)", "test_code": "", "difficulty": "hard", "category": "cnn", "source": "manual"}
{"prompt": "Create a simple GRU-based sequence model for text classification", "code": "class GRUClassifier(nn.Module):\n    def __init__(self, vocab_size=10000, embed_dim=128, hidden_dim=256, num_classes=10):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        _, hidden = self.gru(x)\n        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n        return self.fc(hidden)", "test_code": "", "difficulty": "medium", "category": "rnn", "source": "manual"}
