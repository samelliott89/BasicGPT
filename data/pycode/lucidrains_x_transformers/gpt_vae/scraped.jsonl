{"prompt": "Create a gptvae neural network module", "code": "class GPTVAE(Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        depth,\n        enc_depth,\n        max_seq_len,\n        dim_latent = None,\n        attn_dim_head = 64,\n        heads = 8,\n        enc_kwargs: dict = dict(),\n        dec_kwargs: dict = dict(),\n        vae_kl_loss_weight = 1.,\n        vae_kl_div_floor = 0.,      # what was done in free transformer, which in turn came from Kingma 2016\n        latents_dropout_prob = 0.5, # what percentage of the time to dropout the latents completely\n        pad_id = -1,\n        encoder: Module | None = None,\n        **kwargs\n    ):\n        super().__init__()\n        dim_latent = default(dim_latent, dim)\n\n        if not exists(encoder):\n            encoder = TransformerWrapper(\n                num_tokens = num_tokens,\n                max_seq_len = max_seq_len + 1,\n                return_only_embed = True,\n                average_pool_embed = True,\n                attn_layers = Encoder(\n                    dim = dim,\n                    depth = enc_depth,\n                    attn_dim_head = attn_dim_head,\n                    heads = heads,\n                    **kwargs,\n                    **enc_kwargs\n                ),\n            )\n\n        self.encoder = encoder\n\n        self.to_latent_mean_log_variance = nn.Sequential(\n            nn.Linear(dim, dim_latent * 2),\n            Rearrange('b (two d) -> two b d', two = 2)\n        )\n\n        self.from_latent_to_prepend_token = nn.Sequential(\n            nn.Linear(dim_latent, dim),\n            Rearrange('b d -> b 1 d')\n        )\n\n        self.decoder = TransformerWrapper(\n            num_tokens = num_tokens,\n            max_seq_len = max_seq_len,\n            attn_layers = Decoder(\n                dim = dim,\n                depth = depth,\n                attn_dim_head = attn_dim_head,\n                heads = heads,\n                **kwargs,\n                **dec_kwargs\n            ),\n        )\n\n        self.ar_wrapped_decoder = AutoregressiveWrapper(self.decoder, ignore_index = pad_id)\n\n        self.pad_id = pad_id\n\n        # loss weights - vae kl loss\n\n        self.vae_kl_div_floor = vae_kl_div_floor\n        self.vae_kl_loss_weight = vae_kl_loss_weight\n\n        self.latents_dropout = nn.Dropout(latents_dropout_prob)\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def encode_to_latents(\n        self,\n        seq,\n        return_mean_log_var = False\n    ):\n        mask = seq != self.pad_id\n        pooled = self.encoder(seq, mask = mask)\n\n        latents_mean, latents_log_var = self.to_latent_mean_log_variance(pooled)\n        latents_std = (0.5 * latents_log_var).exp()\n\n        # reparam trick\n\n        latents = latents_mean + latents_std * torch.randn_like(latents_mean)\n\n        if not return_mean_log_var:\n            return latents\n\n        return latents, (latents_mean, latents_log_var)\n\n    @torch.no_grad()\n    def generate(\n        self,\n        prompts,\n        seq_len,\n        latents = None,\n        seq_for_latents = None,\n        **generate_kwargs\n    ):\n        assert prompts.ndim in {1, 2}\n        batch = prompts.shape[0] if prompts.ndim == 2 else 1\n\n        # if seq_for_latents passed in, derive latents from it\n\n        if exists(seq_for_latents):\n            assert not exists(latents), 'latents should not be passed in if given the seq from which to derive them'\n\n            latents = self.encode_to_latents(seq_for_latents)\n\n        # prepend embeds\n\n        prepend_embeds = None\n        if exists(latents):\n            if not is_tensor(latents):\n                latents = tensor(latents, device = self.device)\n\n            if latents.ndim == 1: # repeat latents\n                latents = repeat(latents, 'd -> b d', b = batch)\n\n            prepend_embeds = self.from_latent_to_prepend_token(latents)\n\n        # generated\n\n        generated = self.ar_wrapped_decoder.generate(\n            prompts,\n            seq_len,\n            prepend_embeds = prepend_embeds,\n            **generate_kwargs\n        )\n\n        return generated\n\n    def forward(\n        self,\n        seq,\n        seq_for_latents = None,\n        return_all_losses = False\n    ):\n        batch, device = seq.shape[0], seq.device\n\n        seq_for_latents = default(seq_for_latents, seq)\n\n        latents, (latents_mean, latents_log_var) = self.encode_to_latents(seq_for_latents, return_mean_log_var = True)\n\n        dropped_latents = ~self.latents_dropout(torch.ones((batch,), device = device)).bool()\n\n        prepend_embeds = self.from_latent_to_prepend_token(latents)\n\n        ar_loss = self.ar_wrapped_decoder(\n            seq,\n            prepend_embeds = prepend_embeds,\n            seq_start_pos = dropped_latents.long() # sequence starts at 1 and does not attend to the first style latent\n        )\n\n        # vae kl loss\n\n        vae_kl_loss = 0.5 * (\n            latents_log_var.exp()\n            + latents_mean.square()\n            - latents_log_var\n            - 1.\n        )\n\n        vae_kl_loss = F.relu(vae_kl_loss - self.vae_kl_div_floor)\n\n        vae_kl_loss = vae_kl_loss.sum(dim = -1).mean()\n\n        # return losses\n\n        total_loss = (\n            ar_loss +\n            vae_kl_loss * self.vae_kl_loss_weight\n        )\n\n        if not return_all_losses:\n            return total_loss\n\n        losses = (ar_loss, vae_kl_loss)\n\n        return total_loss, losses", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/gpt_vae.py", "name": "GPTVAE", "line": 32}}
