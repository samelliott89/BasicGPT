{"prompt": "Create a dpo neural network module", "code": "class DPO(Module):\n    def __init__(\n        self,\n        model: TransformerWrapper,\n        *,\n        beta = 0.1,\n        pad_id = None\n    ):\n        super().__init__()\n        self.policy_model = model\n\n        self.ref_model = deepcopy(model)\n        freeze_all_layers_(self.ref_model)\n\n        self.beta = beta\n        self.pad_id = pad_id\n\n    def parameters(self):\n        return self.policy_model.parameters()\n\n    def forward(\n        self,\n        preferred_seq,\n        unpreferred_seq,\n        *,\n        prompt_mask,\n        preferred_seq_mask = None,\n        unpreferred_seq_mask = None,\n    ):\n        assert preferred_seq.ndim == 2\n        assert preferred_seq.shape == unpreferred_seq.shape\n\n        if exists(self.pad_id):\n            if not exists(preferred_seq_mask):\n                preferred_seq_mask = preferred_seq != self.pad_id\n\n            if not exists(unpreferred_seq_mask):\n                unpreferred_seq_mask = unpreferred_seq != self.pad_id\n\n        \"\"\"\n        Following Appendix B in https://arxiv.org/abs/2305.18290\n        \"\"\"\n\n        with torch.no_grad():\n            self.ref_model.eval()\n            ref_preferred_logprob = log_prob_from_model_and_seq(self.ref_model, preferred_seq)\n            ref_unpreferred_logprob = log_prob_from_model_and_seq(self.ref_model, unpreferred_seq)\n\n        policy_preferred_logprob = log_prob_from_model_and_seq(self.policy_model, preferred_seq)\n        policy_unpreferred_logprob = log_prob_from_model_and_seq(self.policy_model, unpreferred_seq)\n\n        # masked mean of log probs\n\n        preferred_seq_mask = maybe_and_mask(~prompt_mask, preferred_seq_mask)\n        unpreferred_seq_mask = maybe_and_mask(~prompt_mask, unpreferred_seq_mask)\n\n        ref_preferred_logprob, policy_preferred_logprob = map(lambda t: masked_mean(t, preferred_seq_mask), (ref_preferred_logprob, policy_preferred_logprob))\n        ref_unpreferred_logprob, policy_unpreferred_logprob = map(lambda t: masked_mean(t, unpreferred_seq_mask), (ref_unpreferred_logprob, policy_unpreferred_logprob))\n\n        # main dpo formula\n\n        policy_logratios = policy_preferred_logprob - policy_unpreferred_logprob\n        ref_logratios = ref_preferred_logprob - ref_unpreferred_logprob\n\n        losses = -F.logsigmoid(self.beta * (policy_logratios - ref_logratios))\n\n        return losses.mean()", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/dpo.py", "name": "DPO", "line": 51}}
