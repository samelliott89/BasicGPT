{"prompt": "Create a random fourier embed neural network module", "code": "class RandomFourierEmbed(Module):\n\n    def __init__(self, dim):\n        super().__init__()\n        self.proj = nn.Linear(1, dim)\n        self.proj.requires_grad_(False)\n\n    def forward(\n        self,\n        times,\n    ):\n\n        times = rearrange(times, '... -> ... 1')\n        rand_proj = self.proj(times)\n        return torch.cos(2 * pi * rand_proj)", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/neo_mlp.py", "name": "RandomFourierEmbed", "line": 24}}
{"prompt": "https://openreview.net/forum?id=A8Vuf2e8y6", "code": "class NeoMLP(Module):\n    \"\"\" https://openreview.net/forum?id=A8Vuf2e8y6 \"\"\"\n    \"\"\" https://haian-jin.github.io/projects/LVSM/ \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_hidden,\n        dim_out,\n        dim_model,\n        depth,\n        encoder_kwargs: dict = dict(\n            attn_dim_head = 16,\n            heads = 4\n        )\n    ):\n        super().__init__()\n\n        # input and output embeddings\n\n        self.input_embed = nn.Parameter(torch.zeros(dim_in, dim_model))\n        self.hidden_embed = nn.Parameter(torch.zeros(dim_hidden, dim_model))\n        self.output_embed = nn.Parameter(torch.zeros(dim_out, dim_model))\n\n        nn.init.normal_(self.input_embed, std = 0.02)\n        nn.init.normal_(self.hidden_embed, std = 0.02)\n        nn.init.normal_(self.output_embed, std = 0.02)\n\n        # they use random fourier for continuous features\n\n        self.random_fourier = nn.Sequential(\n            RandomFourierEmbed(dim_model),\n            nn.Linear(dim_model, dim_model)\n        )\n\n        # hidden dimensions of mlp replaced with nodes with message passing\n        # which comes back to self attention as a fully connected graph.\n\n        self.transformer = Encoder(\n            dim = dim_model,\n            depth = depth,\n            **encoder_kwargs\n        )\n\n        # output\n\n        self.to_output_weights = nn.Parameter(torch.randn(dim_out, dim_model))\n        self.to_output_bias = nn.Parameter(torch.zeros(dim_out))\n\n    def forward(\n        self,\n        x,\n        return_embeds = False\n    ):\n        no_batch = x.ndim == 1\n\n        if no_batch:\n            x = rearrange(x, '... -> 1 ...')\n\n        batch = x.shape[0]\n\n        fouriered_input = self.random_fourier(x)\n\n        # add fouriered input to the input embedding\n\n        input_embed = fouriered_input + self.input_embed\n\n        hidden_embed, output_embed = tuple(repeat(t, '... -> b ...', b = batch) for t in (self.hidden_embed, self.output_embed))\n\n        # pack all the inputs into one string of tokens for self attention\n\n        embed, packed_shape = pack([input_embed, hidden_embed, output_embed], 'b * d')\n\n        # attention is all you need\n\n        embed = self.transformer(embed)\n\n        # unpack\n\n        input_embed, hidden_embed, output_embed = unpack(embed, packed_shape, 'b * d')\n\n        # project for output\n\n        output = einsum(output_embed, self.to_output_weights, 'b n d, n d -> b n')\n        output = output + self.to_output_bias\n\n        if no_batch:\n            output = rearrange(output, '1 ... -> ...')\n\n        if not return_embeds:\n            return output\n\n        return output, (input_embed, hidden_embed, output_embed)", "test_code": "", "difficulty": "medium", "category": "mlp", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/neo_mlp.py", "name": "NeoMLP", "line": 42}}
