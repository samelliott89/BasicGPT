{"prompt": "Create a multi input transformer wrapper neural network module", "code": "class MultiInputTransformerWrapper(Module):\n    def __init__(\n        self,\n        *,\n        num_tokens: Dict[str, int] = dict(),\n        max_seq_len,\n        attn_layers: AttentionLayers,\n        emb_dim = None,\n        max_mem_len = 0,\n        shift_mem_down = 0,\n        emb_dropout = 0.,\n        post_emb_norm = False,\n        num_memory_tokens = None,\n        memory_tokens_interspersed_every = None,\n        return_only_embed = False,\n        use_abs_pos_emb = True,\n        scaled_sinu_pos_emb = False,\n        emb_frac_gradient = 1., # GLM-130B and Cogview successfully used this, set at 0.1\n        attn_z_loss_weight = 1e-4,\n    ):\n        super().__init__()\n\n        dim = attn_layers.dim\n        emb_dim = default(emb_dim, dim)\n        self.emb_dim = emb_dim\n\n        self.max_seq_len = max_seq_len\n        self.max_mem_len = max_mem_len\n        self.shift_mem_down = shift_mem_down\n\n        no_abs_pos_emb = max_seq_len == 0 or not (use_abs_pos_emb and not attn_layers.disable_abs_pos_emb)\n\n        if no_abs_pos_emb:\n            self.pos_emb = always(0)\n        elif scaled_sinu_pos_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(emb_dim)\n        else:\n            self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len)\n\n        # additional embeddings - say type embedding from BERT        \n\n        self.embeds = ModuleDict({f'{name}_embed': nn.Embedding(one_num_tokens, emb_dim) for name, one_num_tokens in num_tokens.items()})\n\n        # fraction of the gradient that should go to the embedding, https://arxiv.org/abs/2105.13290\n\n        self.emb_frac_gradient = emb_frac_gradient\n\n        self.post_emb_norm = LayerNorm(emb_dim) if post_emb_norm else nn.Identity()\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n        self.attn_layers = attn_layers\n\n        # output head, usually to logits of num_tokens\n\n        if return_only_embed:\n            self.to_logits = None\n        else:\n            self.to_logits = ModuleDict({name: nn.Linear(dim, logits_dim, bias = False) for name, logits_dim in num_tokens.items()})\n\n        # memory tokens (like [cls]) from Memory Transformers paper\n\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.num_memory_tokens = num_memory_tokens\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n\n        self.memory_tokens_interspersed_every = memory_tokens_interspersed_every\n\n        # whether can do cached kv decoding\n\n        self.can_cache_kv = self.num_memory_tokens == 0\n        self.can_cache_kv_outside_max_seq_len = no_abs_pos_emb\n\n    def forward(\n        self,\n        x: Dict[str, Tensor],\n        return_embeddings = False,\n        return_logits_and_embeddings = False,\n        return_intermediates = False,\n        mask = None,\n        return_mems = False,\n        return_attn = False,\n        mems = None,\n        mem_masks = None,\n        pos = None,\n        prepend_embeds = None,\n        prepend_mask = None,\n        sum_embeds = None,\n        return_attn_z_loss = False,\n        attn_z_loss_weight = 1e-4,\n        seq_start_pos = None,\n        cache: LayerIntermediates | None = None,\n        **kwargs\n    ):\n        assert not is_empty(x)\n        first_input = list(x.values())[0]\n\n        b, n, device, num_mems, has_memory_tokens, emb_frac_gradient = *first_input.shape, first_input.device, self.num_memory_tokens, self.num_memory_tokens > 0, self.emb_frac_gradient\n\n        return_hiddens = return_mems | return_attn | return_intermediates | return_attn_z_loss\n        return_embeddings = return_embeddings | (not exists(self.to_logits))\n\n        # token embedding\n\n        assert len(x) == len(self.embeds)\n\n        token_emb = 0.\n\n        for name, embed_id in x.items():\n            embed_key = f'{name}_embed'\n\n            assert embed_key in self.embeds\n            embed = self.embeds[embed_key](embed_id)\n\n            token_emb = token_emb + embed\n\n        # absolute positional embedding\n\n        external_pos_emb = exists(pos) and pos.dtype != torch.long\n        pos_emb = self.pos_emb(first_input, pos = pos, seq_start_pos = seq_start_pos) if not external_pos_emb else pos        \n\n        token_emb = token_emb + pos_emb\n\n        # for summing embeddings passed externally - needs this for self-conditioning in non-autoregressive training\n\n        if exists(sum_embeds):\n            token_emb = token_emb + sum_embeds\n\n        # set back to `x`\n\n        x = token_emb\n\n        # post embedding norm, purportedly leads to greater stabilization\n\n        x = self.post_emb_norm(x)\n\n        # whether to append embeds, as in PaLI, for image embeddings\n\n        if exists(prepend_embeds):\n            prepend_seq, prepend_dim = prepend_embeds.shape[1:]\n            assert prepend_dim == x.shape[-1], 'prepended embeddings need to have same dimensions as text model dimensions'\n\n            x = torch.cat((prepend_embeds, x), dim = -2)\n\n            if exists(prepend_mask) or exists(mask):\n                mask = default(mask, lambda: torch.ones((b, n), device = device, dtype = torch.bool))\n                prepend_mask = default(prepend_mask, lambda: torch.ones((b, prepend_seq), device = device, dtype = torch.bool))\n\n                mask = torch.cat((prepend_mask, mask), dim = -1)\n\n        # whether to reduce the gradient going to the embedding, from cogview paper, corroborated by GLM-130B model\n\n        if emb_frac_gradient < 1:\n            assert emb_frac_gradient > 0\n            x = x * emb_frac_gradient + x.detach() * (1 - emb_frac_gradient)\n\n        # embedding dropout\n\n        x = self.emb_dropout(x)\n\n        x = self.project_emb(x)\n\n        if has_memory_tokens:\n            mem_every = self.memory_tokens_interspersed_every\n\n            if exists(mem_every):\n                assert mem_every > 0\n                assert isinstance(self.attn_layers, Decoder), 'only for decoder'\n                next_seq_len = math.ceil(n / mem_every) * mem_every\n\n                x = pad_at_dim(x, (0, next_seq_len - n), dim = -2, value = 0.)\n                x = rearrange(x, 'b (n m) d -> (b n) m d', m = mem_every)\n\n            mem = repeat(self.memory_tokens, 'n d -> b n d', b = x.shape[0])\n            x, mem_packed_shape = pack((mem, x), 'b * d')\n\n            # auto-handle masking after appending memory tokens\n            if not exists(mem_every) and exists(mask):\n                mask = pad_at_dim(mask, (num_mems, 0), dim = -1, value = True)\n\n            if exists(mem_every):\n                x = rearrange(x, '(b n) m d -> b (n m) d', b = b)\n\n        if self.shift_mem_down and exists(mems):\n            mems_l, mems_r = mems[:self.shift_mem_down], mems[self.shift_mem_down:]\n            mems = [*mems_r, *mems_l]\n\n        x, intermediates = self.attn_layers(x, mask = mask, mems = mems, mem_masks = mem_masks, cache = cache, return_hiddens = True, seq_start_pos = seq_start_pos, **kwargs)\n\n        # handle memories post-attention\n\n        if has_memory_tokens:\n            if exists(mem_every):\n                x = rearrange(x, 'b (n m) d -> (b n) m d', m = (mem_every + num_mems))\n\n            mem, x = unpack(x, mem_packed_shape, 'b * d')\n\n            intermediates.memory_tokens = mem\n\n            if exists(mem_every):\n                x = rearrange(x, '(b n) m d -> b (n m) d', b = b)\n\n            x = x[:, :n]\n\n        # projecting to logits\n\n        if not return_embeddings:\n            logits = {name: fn(x) for name, fn in self.to_logits.items()}\n\n        # different returns\n\n        if return_logits_and_embeddings:\n            out = (logits, x)\n        elif return_embeddings:\n            out = x\n        else:\n            out = logits\n\n        # aux loss\n\n        if return_attn_z_loss:\n            pre_softmax_attns = [t.pre_softmax_attn for t in intermediates.attn_intermediates]\n            intermediates.attn_z_loss = calc_z_loss(pre_softmax_attns, weight = attn_z_loss_weight)\n            return_intermediates = True\n\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = [torch.cat(pair, dim = -2) for pair in zip(mems, hiddens)] if exists(mems) else hiddens\n            new_mems = [t[..., -self.max_mem_len:, :].detach() for t in new_mems]\n\n            if not return_intermediates:\n                return out, new_mems\n\n            intermediates.mems = new_mems\n\n        if return_intermediates:\n            return out, intermediates\n\n        if return_attn:\n            attn_maps = [t.post_softmax_attn for t in intermediates.attn_intermediates]\n            return out, attn_maps\n\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/multi_input.py", "name": "MultiInputTransformerWrapper", "line": 34}}
