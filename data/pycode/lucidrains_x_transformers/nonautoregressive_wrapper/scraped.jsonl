{"prompt": "Create a self critic neural network module", "code": "class SelfCritic(Module):\n    def __init__(self, net):\n        super().__init__()\n        self.net = net\n\n        dim = net.attn_layers.dim\n        self.to_logits = nn.Linear(dim, 1)\n\n    def forward(self, x):\n        embed = self.net(x, return_embeddings = True)\n        return self.to_logits(embed)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/nonautoregressive_wrapper.py", "name": "SelfCritic", "line": 87}}
{"prompt": "https://arxiv.org/abs/1904.09324", "code": "class NonAutoregressiveWrapper(Module):\n    \"\"\"\n    https://arxiv.org/abs/1904.09324\n    https://arxiv.org/abs/2202.04200\n    \"\"\"\n\n    def __init__(\n        self,\n        net,\n        *,\n        mask_id,\n        steps = 18,\n        self_cond = False,\n        self_cond_train_prob = 0.75,\n        no_replace_prob = 0.15,          # which percentage of the tokens masked will stay the same, done in original MLM paper\n        random_token_prob = 0.1,         # which percentage of tokens to be replaced with random token, done in original MLM paper\n        schedule = 'linear',\n        can_mask_prev_unmasked = False,  # when unmasking, whether it can remask previously unmasked\n        token_critic: TransformerWrapper | None = None,\n        self_token_critic = False,\n        critic_loss_weight = 1.,\n        use_simple_mdlm_loss_weight = True # Sahoo et al. https://arxiv.org/abs/2406.07524\n    ):\n        super().__init__()\n        assert not (self_token_critic and exists(token_critic))\n\n        self.net = net\n\n        dim = net.emb_dim\n        self.dim = dim\n        self.num_tokens = net.num_tokens\n\n        self.mask_id = mask_id\n\n        # afaict, maskgit paper did not do this\n        # but may help for self conditioning, as used successfully in original BERT\n\n        self.no_replace_prob = no_replace_prob\n        self.random_token_prob = random_token_prob\n\n        self.max_seq_len = net.max_seq_len\n        self.steps = steps\n\n        if callable(schedule):\n            self.schedule_fn = schedule\n        if schedule == 'linear':\n            self.schedule_fn = linear_schedule\n        elif schedule == 'cosine':\n            self.schedule_fn = cosine_schedule\n        else:\n            raise ValueError(f'invalid schedule {schedule}')\n\n        # whether to use the loss weighting proposed in simple diffusion lm paper\n\n        self.loss_weight_fn = None\n\n        if use_simple_mdlm_loss_weight:\n            grad_and_value_schedule_fn = vmap(grad_and_value(self.schedule_fn))\n\n            # eq (10)\n\n            def loss_weight_fn(times):\n                grad, value = grad_and_value_schedule_fn(times)\n                return grad / (1. - value)\n\n            self.loss_weight_fn = loss_weight_fn\n\n        # whether to mask previous - in the simple mdlm paper, they chose not to\n\n        self.can_mask_prev_unmasked = can_mask_prev_unmasked\n\n        # self conditioning\n\n        self.self_cond = self_cond\n\n        if self_cond:\n            self.null_embed = nn.Parameter(torch.randn(dim))\n            self.to_self_cond = nn.Linear(dim, dim, bias = False) if self_cond else None\n            self.self_cond_train_prob = self_cond_train_prob\n\n        # token critic\n\n        self.token_critic = token_critic\n\n        if self_token_critic:\n            self.token_critic = SelfCritic(net)\n\n        self.critic_loss_weight = critic_loss_weight\n\n    @torch.no_grad()\n    def generate(\n        self,\n        batch_size = None,\n        start_temperature = 1.,\n        filter_thres = 0.7,\n        noise_level_scale = 1.,\n        **kwargs\n    ):\n        sample_one = not exists(batch_size)\n        batch_size = default(batch_size, 1)\n\n        device = next(self.net.parameters()).device\n\n        was_training = self.training\n        self.eval()\n\n        times = torch.linspace(0., 1., self.steps + 1)\n\n        # sequence starts off as all masked\n\n        shape = (batch_size, self.max_seq_len)\n\n        seq = torch.full(shape, self.mask_id, device = device)\n        mask = torch.full(shape, True, device = device)\n\n        # slowly demask\n\n        all_mask_num_tokens = (self.schedule_fn(times[1:]) * self.max_seq_len).long()\n\n        # self conditioning\n\n        has_self_cond = self.self_cond\n        last_embed = self.null_embed if has_self_cond else None\n\n        for mask_num_tokens, steps_until_x0 in zip(all_mask_num_tokens.tolist(), reversed(range(self.steps))):\n\n            self_cond = self.to_self_cond(last_embed) if has_self_cond else None\n\n            logits, embeds = self.net(\n                seq,\n                sum_embeds = self_cond,\n                return_logits_and_embeddings = True,\n                **kwargs\n            )\n\n            if has_self_cond:\n                last_embed = embeds\n\n            if exists(filter_thres):\n                logits = top_k(logits, filter_thres)\n\n            annealing_scale = steps_until_x0 / self.steps\n            temperature = start_temperature * annealing_scale\n\n            probs = (logits / max(temperature, 1e-3)).softmax(dim = -1)\n\n            sampled_ids = gumbel_sample(logits, temperature = max(temperature, 1e-3))\n\n            seq = torch.where(mask, sampled_ids, seq)\n\n            if exists(self.token_critic):\n                scores = self.token_critic(seq)\n                scores = rearrange(scores, 'b n 1 -> b n')\n                scores = scores + noise_level_scale * gumbel_noise(scores) * annealing_scale\n            else:\n                scores = 1 - logits.softmax(dim = -1)\n                scores = scores.gather(2, rearrange(sampled_ids, 'b n -> b n 1'))\n                scores = rearrange(scores, 'b n 1 -> b n')\n\n            if mask_num_tokens == 0:\n                pass\n\n            if not self.can_mask_prev_unmasked:\n                scores = scores.masked_fill(~mask, -torch.finfo(scores.dtype).max)\n\n            mask_indices = scores.topk(mask_num_tokens, dim = -1).indices\n            mask = torch.zeros_like(scores, dtype = torch.bool).scatter(1, mask_indices, True)\n            seq = seq.masked_fill(mask, self.mask_id)\n\n        self.train(was_training)\n\n        if sample_one:\n            seq = rearrange(seq, '1 n -> n')\n\n        return seq\n\n    def forward(\n        self,\n        x,\n        only_train_generator = False,\n        only_train_critic = False,\n        generator_sample_temperature = None,\n        **kwargs\n    ):\n        b, n, device = *x.shape, x.device\n        assert n == self.max_seq_len\n\n        orig_seq = x.clone()\n\n        rand_times = torch.empty(b, device = device).uniform_(0, 1)\n        batched_randperm = torch.rand((b, n), device = device).argsort(dim = -1).float()\n\n        rand_probs = self.schedule_fn(rand_times)\n        num_tokens_mask = (rand_probs * n).clamp(min = 1.)\n        mask = batched_randperm < rearrange(num_tokens_mask, 'b -> b 1')\n\n        # to ensure all tokens produce embeddings, instead of just the ones with [mask] input, as done in seminal BERT MLM paper\n        # potentially needed for self-conditioning (on embedding) to work well\n\n        replace_mask_id_mask = mask.clone()\n        frac_seq_left = 1.\n\n        if self.no_replace_prob > 0. and coin_flip():\n            frac_seq_left -= self.no_replace_prob\n\n            no_replace_prob_mask = get_mask_subset_prob(mask, self.no_replace_prob)\n            replace_mask_id_mask &= ~no_replace_prob_mask\n\n        if self.random_token_prob > 0. and coin_flip():\n            random_token_prob_mask = get_mask_subset_prob(replace_mask_id_mask, self.random_token_prob * frac_seq_left)\n            random_tokens = torch.randint(0, self.num_tokens, (b, n), device = device)\n\n            x = torch.where(random_token_prob_mask, random_tokens, x)\n            replace_mask_id_mask &= ~random_token_prob_mask\n\n        masked = torch.where(replace_mask_id_mask, self.mask_id, x)\n\n        # self conditioning\n\n        if self.self_cond:\n            self_cond = self.null_embed\n\n            if sample_prob(self.self_cond_train_prob):\n                with torch.no_grad():\n                    self_cond = self.net(masked, return_embeddings = True, **kwargs).detach()\n\n            kwargs.update(sum_embeds = self.to_self_cond(self_cond))\n\n        # logits\n\n        context = torch.no_grad if only_train_critic else nullcontext\n\n        with context():\n            logits = self.net(masked, **kwargs)\n\n        loss_fn = F.cross_entropy if not self.net.output_is_log_prob else F.nll_loss\n\n        # loss\n\n        if exists(self.loss_weight_fn):\n            # using simple mdlm loss weighting\n\n            loss = loss_fn(\n                rearrange(logits, 'b n l -> b l n'),\n                orig_seq,\n                reduction = 'none'\n            )\n\n            loss_weights = self.loss_weight_fn(rand_times)     # calculate loss weight\n            loss = einx.multiply('b n, b', loss, loss_weights) # apply loss weights\n\n            loss = loss[mask].mean()\n\n        else:\n            loss = loss_fn(\n                logits[mask],\n                orig_seq[mask],\n            )\n\n        if not exists(self.token_critic) or only_train_generator:\n            return Losses(loss, loss, None)\n\n        sampled_ids = gumbel_sample(logits, temperature = default(generator_sample_temperature, random()))\n        generated = torch.where(mask, sampled_ids, orig_seq)\n\n        critic_logits = self.token_critic(generated)\n        critic_labels = (sampled_ids != orig_seq).float()\n\n        critic_loss = F.binary_cross_entropy_with_logits(\n            rearrange(critic_logits, '... 1 -> ...'),\n            critic_labels\n        )\n\n        # determine losses to be returned based on what researcher wants to train\n\n        if only_train_critic:\n            total_loss = critic_loss\n            loss = None\n        else:\n            total_loss = loss + critic_loss * self.critic_loss_weight\n\n        return Losses(total_loss, loss,  critic_loss)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/nonautoregressive_wrapper.py", "name": "NonAutoregressiveWrapper", "line": 99}}
