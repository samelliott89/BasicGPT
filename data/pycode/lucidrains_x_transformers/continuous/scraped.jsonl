{"prompt": "Create a continuous transformer wrapper neural network module", "code": "class ContinuousTransformerWrapper(Module):\n    def __init__(\n        self,\n        *,\n        max_seq_len,\n        attn_layers: AttentionLayers,\n        dim_in = None,\n        dim_out = None,\n        emb_dim = None,\n        max_mem_len = 0,\n        num_memory_tokens = None,\n        post_emb_norm = False,\n        emb_dropout = 0.,\n        use_abs_pos_emb = True,\n        scaled_sinu_pos_emb = False,\n        average_pool_embed = False,\n        probabilistic = False,\n    ):\n        super().__init__()\n        dim = attn_layers.dim\n\n        self.max_seq_len = max_seq_len\n\n        self.max_mem_len = max_mem_len\n        \n        no_abs_pos_emb = max_seq_len == 0 or not (use_abs_pos_emb and not attn_layers.disable_abs_pos_emb)\n\n        if no_abs_pos_emb:\n            self.pos_emb = always(0)\n        elif scaled_sinu_pos_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(dim)\n        else:\n            self.pos_emb = AbsolutePositionalEmbedding(dim, max_seq_len)\n\n        self.post_emb_norm = LayerNorm(dim) if post_emb_norm else nn.Identity()\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        # memory tokens\n\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.has_memory_tokens = num_memory_tokens > 0\n\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n\n        # attention layers\n\n        self.attn_layers = attn_layers\n\n        # average pool\n\n        self.average_pool_embed = average_pool_embed\n\n        # project in and out\n\n        self.project_in = nn.Linear(dim_in, dim, bias = False) if exists(dim_in) else nn.Identity()\n\n        # output is multipled by 2 for outputting mean and log variance\n\n        self.probabilistic = probabilistic\n\n        self.project_out = nn.Linear(dim, dim_out * (2 if probabilistic else 1), bias = False) if exists(dim_out) else nn.Identity()\n\n        # can cache kv\n\n        self.can_cache_kv = all([module.can_cache_kv for module in self.modules() if isinstance(module, Attention)])\n\n    def forward(\n        self,\n        x,\n        return_embeddings = False,\n        return_intermediates = False,\n        return_mems = False,\n        mask = None,\n        lens = None,\n        return_attn = False,\n        mems = None,\n        mem_masks = None,\n        pos = None,\n        sum_embeds = None,\n        prepend_embeds = None,\n        prepend_mask = None,\n        cache: LayerIntermediates | None = None,\n        input_not_include_cache = False,\n        seq_start_pos = None,\n        **kwargs\n    ):\n        batch, seq, orig_mask, device = *x.shape[:2], mask, x.device\n\n        # maybe seq lengths passed in\n\n        if exists(lens):\n            assert not exists(mask), 'either `mask` or `lens` passed in, but not both'\n            seq_arange = arange(seq, device = device)\n\n            mask = einx.less('j, i -> i j', seq_arange, lens)\n\n        # take care of position embedding offsets in the presence of cache and sequence is less than cache length (not full sequence)\n\n        seq_pos_offset = 0\n\n        if exists(cache) and input_not_include_cache:\n            seq_pos_offset = cache.cache_length\n\n        # project in + positional embedding\n\n        x = self.project_in(x)\n        x = x + self.pos_emb(x, pos = pos, seq_start_pos = seq_start_pos, offset = seq_pos_offset)\n\n        if exists(sum_embeds):\n            x = x + sum_embeds\n\n        x = self.post_emb_norm(x)\n\n        # memory tokens\n\n        if self.has_memory_tokens:\n            m = repeat(self.memory_tokens, 'm d -> b m d', b = batch)\n            x, mem_ps = pack([m, x], 'b * d')\n\n            if exists(mask):\n                num_mems = m.shape[-2]\n                mask = pad_at_dim(mask, (num_mems, 0), dim = -1, value = True)\n\n        # whether to append embeds, as in PaLI, for image embeddings\n\n        if exists(prepend_embeds):\n            prepend_seq, prepend_dim = prepend_embeds.shape[1:]\n\n            assert prepend_dim == x.shape[-1], 'prepended embeddings need to have same dimensions as model dimensions'\n\n            x = cat((prepend_embeds, x), dim = -2)\n\n            if exists(prepend_mask) or exists(mask):\n                mask = default(mask, lambda: torch.ones((batch, seq), device = device, dtype = torch.bool))\n                prepend_mask = default(prepend_mask, lambda: torch.ones((batch, prepend_seq), device = device, dtype = torch.bool))\n\n                mask = cat((prepend_mask, mask), dim = -1)\n\n        x = self.emb_dropout(x)\n\n        # attention layers\n\n        x, intermediates = self.attn_layers(x, mask = mask, mems = mems, mem_masks = mem_masks, cache = cache, input_not_include_cache = input_not_include_cache, seq_pos_offset = seq_pos_offset, return_hiddens = True, **kwargs)\n\n        # splice out memory tokens\n\n        if self.has_memory_tokens:\n            m, x = unpack(x, mem_ps, 'b * d')\n            intermediates.memory_tokens = m\n\n        if self.average_pool_embed:\n            x = masked_mean(x, mask = orig_mask)\n\n        # maybe linear project out\n\n        out = self.project_out(x) if not return_embeddings else x\n\n        if not return_embeddings and self.probabilistic:\n            mean, log_var = rearrange(out, '... (d mean_log_var) -> mean_log_var ... d', mean_log_var = 2)\n            variance = log_var.exp()\n            out = stack((mean, variance))\n\n        if return_intermediates:\n            return out, intermediates\n\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = tuple(t[..., -self.max_mem_len:, :].detach() for t in hiddens)\n            return out, new_mems\n\n        if return_attn:\n            attn_maps = tuple(t.post_softmax_attn for t in intermediates.attn_intermediates)\n            return out, attn_maps\n\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/continuous.py", "name": "ContinuousTransformerWrapper", "line": 62}}
{"prompt": "Create a continuous autoregressive wrapper neural network module", "code": "class ContinuousAutoregressiveWrapper(Module):\n    def __init__(\n        self,\n        net: ContinuousTransformerWrapper,\n        loss_fn: Module | None = None,\n        use_l1_loss = False,\n        equal_loss_weight_batch = False,  # setting this to True, if the mask is passed in and sequences are variable in length, each sequence will be weighted the same (as opposed to each token)\n    ):\n        super().__init__()\n        self.net = net\n        self.max_seq_len = net.max_seq_len\n\n        probabilistic = net.probabilistic\n        self.probabilistic = probabilistic\n\n        # default loss function\n\n        if not exists(loss_fn):\n            if probabilistic:\n                loss_fn = GaussianNLL()\n            elif use_l1_loss:\n                loss_fn = nn.L1Loss(reduction = 'none')\n            else:\n                loss_fn = nn.MSELoss(reduction = 'none')\n\n        self.loss_fn = loss_fn\n        self.equal_loss_weight_batch = equal_loss_weight_batch\n\n    @torch.no_grad()\n    def generate(\n        self,\n        start_tokens,\n        seq_len,\n        temperature = 1.,\n        cache_kv = True,\n        **kwargs\n    ):\n        should_cache_kv = cache_kv and self.net.can_cache_kv\n        device = start_tokens.device\n\n        was_training = self.net.training\n        num_dims = start_tokens.ndim\n\n        assert num_dims >= 2, 'number of dimensions of your start tokens must be greater or equal to 2'\n        no_batch = num_dims == 2\n\n        if no_batch:\n            start_tokens = rearrange(start_tokens, 'n d -> 1 n d')\n\n        b, t, _, device = *start_tokens.shape, start_tokens.device\n\n        self.net.eval()\n        out = start_tokens\n\n        cache = None\n\n        for _ in range(seq_len):\n            x = out[:, -self.max_seq_len:]\n\n            net_out, new_cache = self.net(x, cache = cache, return_intermediates = True, **kwargs)\n\n            last_output = net_out[..., -1:, :]\n\n            if self.probabilistic:\n                mean, var = last_output\n                last_output = sample_from_mean_variance(mean, var, temperature = temperature)\n\n            out = cat((out, last_output), dim = -2)\n\n            if should_cache_kv:\n                cache = new_cache\n\n        out = out[:, t:]\n\n        if no_batch:\n            out = rearrange(out, '1 n d -> n d')\n\n        self.net.train(was_training)\n        return out\n\n    def forward_rollout(\n        self,\n        x,\n        rollout_steps = 2,\n        **kwargs\n    ):\n        assert rollout_steps > 1\n\n        steps = rollout_steps\n\n        device = x.device\n\n        # assert inputs\n\n        assert 'prepend_embeds' not in kwargs\n\n        # lens\n\n        lens = kwargs.pop('lens', None)\n\n        if exists(lens):\n            assert 'mask' not in kwargs, 'either `mask` or `lens` passed in, but not both'\n            seq_len, device = inp.shape[1], inp.device\n            seq_arange = arange(seq_len, device = device)\n            mask = einx.less('j, i -> i j', seq_arange, lens)\n            kwargs['mask'] = mask\n\n        if not exists(lens):\n            batch, seq_len = x.shape[:2]\n            lens = torch.full((batch,), seq_len, device = device)\n\n        # handle mask manually\n\n        mask = kwargs.pop('mask', None)\n\n        # pick a random range for each batch sample and aligh the sequence to the right for rollout loss\n\n        valid_tokens_for_rollout = (lens - steps).clamp(min = 0)\n        valid_sample = valid_tokens_for_rollout > 0\n\n        x = x[valid_sample] # remove invalid sequence (lens less than rollout steps)\n\n        if exists(mask):\n            mask = mask[valid_sample]\n\n        batch = x.shape[0]\n        seq_start_pos = (torch.rand((batch,), device = device) * valid_tokens_for_rollout).floor().long()\n\n        batch_arange = torch.arange(batch, device = device)\n        batch_arange = rearrange(batch_arange, 'b -> b 1')\n\n        # crop out sequence to use\n\n        seq_end_pos = seq_start_pos + steps\n        max_end_pos = seq_end_pos.amax().item()\n        x = x[:, :max_end_pos]\n\n        x = align_right(x, seq_end_pos)\n\n        # get the input\n\n        inp, targets = x[:, :-steps], x[:, -steps:]\n\n        # maybe rollout\n\n        cache = None\n        preds = []\n\n        for _ in range(steps):\n\n            out, cache = self.net(\n                inp,\n                seq_start_pos = seq_start_pos,\n                return_intermediates = True,\n                **kwargs\n            )\n\n            last_pred = out[..., -1:, :]\n\n            if self.probabilistic:\n                mean, var = last_pred\n                inp = sample_from_mean_variance(mean, var)\n            else:\n                inp = last_pred\n\n            preds.append(last_pred)\n\n        # stack for predictions\n\n        preds = cat(preds, dim = 1)\n\n        # loss\n\n        loss = self.loss_fn(preds, targets)\n\n        return loss.mean()\n\n    def forward(\n        self,\n        x,\n        rollout_steps = 1, # they used 2 rollout steps in a successful world model paper https://ai.meta.com/vjepa/\n        **kwargs\n    ):\n        if rollout_steps > 1:\n            return self.forward_rollout(x, rollout_steps = rollout_steps, **kwargs)\n\n        inp, target = x[:, :-1], x[:, 1:]\n\n        assert 'prepend_embeds' not in kwargs\n\n        # lens\n\n        lens = kwargs.pop('lens', None)\n\n        if exists(lens):\n            assert 'mask' not in kwargs, 'either `mask` or `lens` passed in, but not both'\n            seq_len, device = inp.shape[1], inp.device\n            seq_arange = torch.arange(seq_len, device = device)\n            mask = einx.less('j, i -> i j', seq_arange, lens)\n\n            kwargs['mask'] = mask\n\n        # mask\n\n        mask = kwargs.get('mask', None)\n\n        if exists(mask) and mask.shape[1] == x.shape[1]:\n            mask = mask[:, :-1]\n            kwargs['mask'] = mask\n\n        out = self.net(inp, **kwargs)\n\n        loss = self.loss_fn(out, target)\n\n        if exists(mask):\n            assert loss.ndim > 1, 'loss should not be reduced if mask is passed in'\n\n            if self.equal_loss_weight_batch:\n                loss = masked_mean(loss, mask)\n            else:\n                loss = loss[mask]\n\n        return loss.mean()", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/continuous.py", "name": "ContinuousAutoregressiveWrapper", "line": 239}}
