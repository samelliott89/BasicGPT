{"prompt": "Create a entropy based tokenizer neural network module", "code": "class EntropyBasedTokenizer(Module):\n    def __init__(\n        self,\n        decoder: Module,\n        entropy_threshold: float,\n        max_token_size: int | None = None\n    ):\n        super().__init__()\n        self.decoder = decoder\n        self.entropy_threshold = entropy_threshold\n\n        self.max_token_size = max_token_size\n\n    @torch.no_grad()\n    def forward(\n        self,\n        seq,            # Float['b n'] | Float['n']\n        lens = None,    # Int['b']\n        return_segmented_seq = False,\n        decoder_forward_kwargs: dict = dict()\n    ):\n        no_batch_dim = seq.ndim == 1\n        seq, maybe_batch_ps = pack((seq,), '* n')\n\n        self.decoder.eval()\n\n        is_var_length = exists(lens)\n        batch, seq_len, device, max_token_size = *seq.shape, seq.device, self.max_token_size\n\n        arange = torch.arange(seq_len, device = device)\n\n        # forward through a small trained decoder and get the entropies of the logits\n\n        logits = self.decoder(seq, **decoder_forward_kwargs)\n\n        entropies = calc_entropy_from_logits(logits)\n\n        # get length mask for boundaries\n\n        mask = tensor(True, device = device)\n\n        if is_var_length:\n            mask = einx.less('n, b -> b n', arange, lens)\n\n        # the mask for tokens that were of a sufficient surprise level\n\n        over_thres_mask = (entropies >= self.entropy_threshold) & mask\n\n        # needed for selecting out indices at entropy threshold mask\n\n        arange_plus_one = arange + 1\n        arange_plus_one = repeat(arange_plus_one, 'n -> b n', b = batch)\n\n        # get a tensor of Int['b num_tokens'] with the token lengths, zero padded\n\n        boundaries = over_thres_mask.clone()\n\n        # set the boundary of the last token\n\n        # if `lens` not given, assume always last token\n        # but if `lens` were given, then properly set the index\n\n        if not is_var_length:\n            boundaries[..., -1] = True\n        else:\n            scatter_indices = rearrange(lens - 1, 'b -> b 1')\n            boundaries.scatter_(-1, scatter_indices, True)\n\n        # handle max token size - technique has the flaw that repeating subsequences are grouped into one large token\n\n        if exists(max_token_size):\n            token_ids = boundaries.cumsum(dim = -1)\n            token_ids = F.pad(token_ids, (1, -1), value = 0)\n\n            max_num_tokens = boundaries.sum(dim = -1).amax().item()\n            token_ids_seq = torch.arange(max_num_tokens, device = device)\n\n            token_mask = einx.equal('j, b i -> b j i', token_ids_seq, token_ids)\n\n            token_sub_seq_arange = token_mask.cumsum(dim = -1)\n\n            sub_seq_boundaries = (token_sub_seq_arange % max_token_size == 0)\n            sub_seq_boundaries = (sub_seq_boundaries & token_mask).any(dim = 1)\n\n            boundaries = boundaries | sub_seq_boundaries\n\n            if exists(mask):\n                boundaries = boundaries & mask\n\n        # number of tokens\n\n        num_tokens = boundaries.sum(dim = -1)\n\n        # get number of tokens as well as derived indices\n\n        indices = arange_plus_one[boundaries].split(num_tokens.tolist())\n\n        # get the token lengths\n\n        token_lengths = []\n\n        for one_indices in indices:\n            padded_indices = F.pad(one_indices, (1, 0), value = 0.)\n            one_token_lengths = padded_indices[1:] - padded_indices[:-1]\n\n            token_lengths.append(one_token_lengths)\n\n        token_lengths = pad_sequence(token_lengths, batch_first = True)\n\n        # early return\n\n        if not return_segmented_seq:\n            token_lengths, = unpack(token_lengths, maybe_batch_ps, '* num_tokens')\n\n            return token_lengths\n\n        # segment the sequence based on the token lengths\n\n        lens = default(lens, (None,))\n        segmented_seq = []\n\n        for one_seq, one_len, one_token_length in zip_longest(seq, lens, token_lengths):\n\n            if exists(one_len):\n                one_seq = one_seq[:one_len]\n\n            one_token_length = one_token_length[one_token_length > 0]\n\n            splitted_seq = one_seq.split(one_token_length.tolist())\n            segmented_seq.append(splitted_seq)\n\n        if no_batch_dim:\n            segmented_seq = segmented_seq[0]\n\n        return segmented_seq", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/entropy_based_tokenizer.py", "name": "EntropyBasedTokenizer", "line": 33}}
