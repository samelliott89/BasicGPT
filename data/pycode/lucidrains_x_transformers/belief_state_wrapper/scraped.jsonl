{"prompt": "Figure 13. in https://arxiv.org/abs/2410.23506", "code": "class BeliefStateWrapper(Module):\n    \"\"\"\n    Figure 13. in https://arxiv.org/abs/2410.23506\n    \"\"\"\n\n    def __init__(\n        self,\n        forward_decoder: TransformerWrapper,\n        backward_decoder: TransformerWrapper | None = None,\n        train_frac_forward_backward_pairs: float = 1.,\n        text_head: Module | None = None,\n        backward_ar_loss_weight: float = 1., # can weigh the training of the backwards decoder differently, perhaps fwd/bwd have a shared backbone etc etc\n        pred_distance = False,\n        pred_distance_loss_weight: float = 1.,\n        cond_on_distance = False,\n        cond_on_distance_prob = 0.5,\n        max_pred_distance = None\n    ):\n        super().__init__()\n        backward_decoder = default(backward_decoder, forward_decoder) # if backward decoder not set, use the same transformer, assume it knows how to switch gears based on suffix token\n\n        assert forward_decoder.emb_dim == backward_decoder.emb_dim, 'forward and backwards model must have the same embedding dimension'\n        assert forward_decoder.num_tokens == backward_decoder.num_tokens, 'forward and backwards model must have the same number of tokens'\n\n        dim = forward_decoder.emb_dim\n        num_tokens = forward_decoder.num_tokens\n        max_seq_len = forward_decoder.max_seq_len\n\n        self.num_tokens = num_tokens\n\n        # the suffix token\n\n        self.suffix_token = nn.Parameter(torch.zeros(dim))\n        nn.init.normal_(self.suffix_token, std = 0.02)\n\n        # the text prediction head, which predicts for the combinations of prefix and suffix the next and previous token for forwards and backward sequences\n\n        if not exists(text_head):\n            text_head = nn.Sequential(\n                nn.Linear(dim * 2, dim),\n                nn.LeakyReLU(),\n                nn.Linear(dim, num_tokens * 2),\n            )\n\n        self.text_head = text_head\n\n        # predicting terminal state (when suffix and prefix predict the same token)\n\n        self.max_pred_distance = default(max_pred_distance, max_seq_len)\n\n        self.to_distance_logits = nn.Sequential(\n            nn.Linear(dim * 2, dim),\n            nn.LeakyReLU(),\n            nn.Linear(dim, self.max_pred_distance),\n        ) if pred_distance else None\n\n        self.pred_distance_loss_weight = pred_distance_loss_weight\n\n        # conditioning on distance\n\n        assert 0. < cond_on_distance_prob < 1.\n\n        self.cond_on_distance = cond_on_distance\n        self.cond_on_distance_prob = cond_on_distance_prob\n\n        if cond_on_distance:\n            self.to_distance_cond = nn.Sequential(\n                Rearrange('... -> ... 1'),\n                nn.Linear(1, dim),\n                nn.LeakyReLU(),\n                nn.Linear(dim, dim * 2),\n            )\n\n        # the two decoders, one which is causal forward, the other causal backwards\n\n        self.forward_decoder = forward_decoder\n        self.backward_decoder = backward_decoder\n\n        # what fraction of forward backward pairs to train on\n        # for further memory efficiency\n\n        assert 0 < train_frac_forward_backward_pairs <= 1.\n        self.train_frac_fb_pairs = train_frac_forward_backward_pairs\n        self.needs_subsample_fb_pairs = train_frac_forward_backward_pairs < 1.\n\n        # loss weighting\n\n        self.backward_ar_loss_weight = backward_ar_loss_weight\n        self.needs_loss_weight = backward_ar_loss_weight != 1.\n\n        self.register_buffer('loss_weights', tensor([1., self.backward_ar_loss_weight]))\n\n        # sampling\n\n        self.max_seq_len = self.forward_decoder.max_seq_len\n\n    @torch.no_grad()\n    @eval_decorator\n    def generate_with_suffix_cond(\n        self,\n        prompts,\n        seq_len,\n        temperature = 1.25,\n        cache_kv = False,\n        suffix: Tensor | None = None, # the goal conditioning\n        filter_logits_fn = min_p,\n        filter_kwargs = dict(\n            min_p = 0.1\n        ),\n        decode_backwards = False,\n        **kwargs\n    ):\n        max_seq_len, greedy, device = self.max_seq_len, temperature == 0., prompts.device\n\n        prompts, batch_ps = pack([prompts], '* d')\n\n        batch, orig_seq_len = prompts.shape\n\n        # allow for decoding backwards, to make sure it is working\n\n        main_decoder = self.forward_decoder\n\n        if decode_backwards:\n            prompts = prompts.flip(1)\n            main_decoder = self.backward_decoder\n\n        out = prompts\n\n        # kv caches\n\n        cache = None\n\n        # get the encoded suffix token once\n\n        suffix_sos_tokens = rearrange(self.suffix_token, 'd -> 1 1 d')\n\n        suffix_sos_tokens = repeat(suffix_sos_tokens, '1 1 d -> b 1 d', b = batch)\n\n        if not decode_backwards:\n            if exists(suffix):\n                if suffix.ndim == 1:\n                    suffix = repeat(suffix, 'n -> b n', b = batch)\n\n                suffix = suffix.flip(1) # reverse autoregressive\n\n            suffix_embed = self.backward_decoder(\n                suffix,\n                prepend_embeds = suffix_sos_tokens,\n                return_embeddings = True\n            )\n\n            # pick out the last embedding for fill in the middle\n\n            suffix_embed = suffix_embed[:, -1:]\n\n        else:\n            # just grab a random token for now for prefix\n\n            prefix_embed = torch.randint(0, self.num_tokens, (batch, 1), device = device)\n\n            prefix_embed = self.forward_decoder(prefix_embed, return_embeddings = True)\n\n        # sampling up to seq_len\n\n        for _ in range(seq_len):\n\n            embeds, new_cache = main_decoder(\n                out,\n                prepend_embeds = suffix_sos_tokens if decode_backwards else None,\n                return_intermediates = True,\n                return_embeddings = True,\n                cache = cache,\n                **kwargs\n            )\n\n            last_embeds = embeds[:, -1:]\n\n            if not decode_backwards:\n                embeds = cat((last_embeds, suffix_embed), dim = -1)\n            else:\n                embeds = cat((prefix_embed, last_embeds), dim = -1)\n\n            if cache_kv and self.forward_decoder.can_cache_kv:\n                cache = new_cache\n\n            forward_logits, backward_logits = self.text_head(embeds).chunk(2, dim = -1)\n\n            logits = forward_logits if not decode_backwards else backward_logits\n\n            logits = logits[:, -1]\n\n            if greedy:\n                sample = logits.argmax(dim = -1, keepdim = True)\n            else:\n                filtered_logits = filter_logits_fn(logits, **filter_kwargs)\n                probs = F.softmax(filtered_logits / temperature, dim = -1)\n                sample = torch.multinomial(probs, 1)\n\n            # concat sample\n\n            out = torch.cat((out, sample), dim = -1)\n\n        out = out[:, orig_seq_len:]\n\n        out, = unpack(out, batch_ps, '* n')\n\n        return out\n\n    def forward(\n        self,\n        seq,\n        lens: Tensor | None = None, # Int['b']\n        loss_weight_by_fb_indices: callable | None = None\n    ):\n        batch, seq_len, device = *seq.shape, seq.device\n\n        # handle variable length sequences\n\n        seq_for_labels = seq\n\n        if exists(lens):\n            mask = einx.less('j, i -> i j', arange(seq_len, device = device), lens)\n            seq_for_labels = torch.where(mask, seq, -1)\n\n        # forward autoregressive\n\n        forward_embeds = self.forward_decoder(seq, return_embeddings = True)\n\n        # backward autoregressive\n\n        backward_seq = flip(seq, lens = lens)\n\n        suffix_tokens = repeat(self.suffix_token, 'd -> b 1 d', b = batch)\n\n        backward_embeds = self.backward_decoder(\n            backward_seq,\n            prepend_embeds = suffix_tokens,\n            return_embeddings = True\n        )\n\n        backward_embeds = flip(backward_embeds, lens = lens)\n\n        # trick to reduce memory on backwards pass\n\n        forward_embeds, backward_embeds = detach_multiple(forward_embeds, backward_embeds)\n\n        # belief state objective\n\n        seq_arange = arange(seq_len, device = device)\n\n        fb_pairs = cartesian_prod(seq_arange, seq_arange + 1) # plus one for suffix token\n\n        # filter down to valid pairs, as in figure 11\n        # f - forward, b - backward, i - indices\n\n        fi, bi = fb_pairs.unbind(dim = -1)\n\n        valid_mask = (bi - fi) >= 2\n\n        fb_pairs = fb_pairs[valid_mask]\n\n        # maybe subsample fb pairs\n\n        if self.needs_subsample_fb_pairs:\n            num_pairs = fb_pairs.shape[0]\n\n            num_subsampled = max(int(num_pairs * self.train_frac_fb_pairs), 1)\n\n            rand_subsampled_indices = torch.randperm(num_pairs, device = device)[:num_subsampled]\n\n            fb_pairs = fb_pairs[rand_subsampled_indices]\n\n        # get labels for both\n\n        fi, bi = fb_pairs.unbind(dim = -1)\n\n        labels_fi, labels_bi = (fi + 1), (bi - 1)\n\n        forward_labels, backward_labels = seq_for_labels[:, labels_fi], seq_for_labels[:, labels_bi]\n\n        labels = cat((forward_labels, backward_labels), dim = -1)\n\n        # get the forward and backward embedding pairs and feed them through the text head for both forward and backward predictions\n\n        fb_embeds = cat((\n            forward_embeds[:, fi],\n            backward_embeds[:, bi]\n        ), dim = -1)\n\n        logits = self.text_head(fb_embeds)\n\n        # cross entropy loss\n\n        loss = F.cross_entropy(\n            rearrange(logits, 'b n (fb l) -> b l (fb n)', fb = 2),\n            labels,\n            reduction = 'none' if self.needs_loss_weight else 'mean',\n            ignore_index = -1\n        )\n\n        # maybe condition on distance\n\n        cond_on_distance = self.cond_on_distance and (random() < self.cond_on_distance_prob)\n\n        if cond_on_distance:\n            distance = (bi - fi).float()\n            distance_cond = self.to_distance_cond(distance)\n\n            fb_embeds = fb_embeds * distance_cond\n\n        # maybe predict distance\n\n        if exists(self.to_distance_logits) and not cond_on_distance:\n            distance_logits = self.to_distance_logits(fb_embeds)\n\n            distance_labels = (bi - fi).clamp(max = self.max_pred_distance - 1)\n            distance_labels = repeat(distance_labels, 'n -> b n', b = batch)\n\n            pred_dist_loss = F.cross_entropy(\n                rearrange(distance_logits, 'b n l -> b l n'),\n                distance_labels\n            )\n\n            loss = (\n                loss +\n                pred_dist_loss * self.pred_distance_loss_weight\n            )\n\n        # maybe loss weighting\n\n        needs_loss_weight = default(self.needs_loss_weight, exists(loss_weight_by_fb_indices))\n\n        if needs_loss_weight:\n            loss = rearrange(loss, 'b (fb n) -> b fb n', fb = 2)\n\n            if self.needs_loss_weight:\n                loss = einx.multiply('b fb n, fb', loss, self.loss_weights)\n\n            # allow researcher to pass in a function that acts on the the forward backward indices Int['n fb']\n            # the reason this may be needed is because the earlier tokens will have more eligible pairs for training, and perhaps this could be normalized\n\n            if exists(loss_weight_by_fb_indices):\n                loss_weight = loss_weight_by_fb_indices(fb_pairs)\n\n                if loss_weight.ndim == 1:\n                    loss = einx.multiply('b fb n, n', loss, loss_weight)\n                elif loss_weight.ndim == 2:\n                    loss = einx.multiply('b fb n, n fb', loss, loss_weight)\n                else:\n                    raise ValueError('invalid loss weight dims')\n\n            loss = loss.mean()\n\n        return loss", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/belief_state_wrapper.py", "name": "BeliefStateWrapper", "line": 79}}
