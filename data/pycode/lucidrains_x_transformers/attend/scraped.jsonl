{"prompt": "Create a attend neural network module", "code": "class Attend(Module):\n    def __init__(\n        self,\n        *,\n        dropout = 0.,\n        causal = False,\n        heads = None,\n        pre_talking_heads = False,\n        post_talking_heads = False,\n        pre_scale_post_talking_heads = False,\n        sparse_topk = None,\n        sparse_topk_straight_through = False, # https://arxiv.org/abs/2505.22074\n        scale = None,\n        qk_norm = False,\n        l2_distance = False,\n        sigmoid = False,\n        gumbel_softmax = False,\n        gumbel_softmax_temp = 1.,\n        gumbel_softmax_hard = True,\n        cog_signed = False,\n        custom_attn_fn: Callable | None = None,\n        flash = False,\n        softclamp_logits = False,\n        logit_softclamp_value = 50.,\n        add_zero_kv = False,\n        head_learned_sink = False,\n        selective = False,\n        hard = False,\n        cope = None,\n        onnxable = False,\n        sdp_kwargs: dict = dict(\n            enable_flash = True,\n            enable_math = True,\n            enable_mem_efficient = True\n        )\n    ):\n        super().__init__()\n        self.scale = scale\n\n        # causal related\n\n        self.causal = causal\n        self.create_causal_mask = onnx_create_causal_mask if onnxable else create_causal_mask\n\n        # attention type\n\n        is_sparse_topk_attn = exists(sparse_topk)\n\n        assert not (flash and sigmoid), 'sigmoid attention not available for flash'\n        assert not (flash and hard), 'hard attention not available for flash'\n        assert not (flash and is_sparse_topk_attn), 'topk attention not available for flash'\n\n        assert at_most_one_of(sigmoid, hard, l2_distance, gumbel_softmax, is_sparse_topk_attn)\n\n        if exists(custom_attn_fn):\n            self.attn_fn = custom_attn_fn\n        elif sigmoid:\n            self.attn_fn = F.sigmoid\n        elif hard:\n            self.attn_fn = one_hot_straight_through\n        elif is_sparse_topk_attn:\n            self.attn_fn = partial(sparse_topk_attn, sparse_topk = sparse_topk, straight_through = sparse_topk_straight_through)\n        elif gumbel_softmax:\n            self.attn_fn = partial(F.gumbel_softmax, dim = -1, tau = gumbel_softmax_temp, hard = gumbel_softmax_hard)\n        else:\n            softmax_fn = partial(F.softmax, dim = -1)\n            self.attn_fn = partial(softmax_fn, dtype = torch.float32) if not qk_norm else softmax_fn\n\n        # dropouts\n\n        self.dropout = dropout\n        self.attn_dropout = nn.Dropout(dropout)\n\n        # talking heads\n\n        assert not (flash and (pre_talking_heads or post_talking_heads or pre_scale_post_talking_heads)), 'talking heads not compatible with flash attention'\n\n        self.pre_softmax_talking_heads = nn.Conv2d(heads, heads, 1, bias = False) if pre_talking_heads else None\n        self.post_softmax_talking_heads = nn.Conv2d(heads, heads, 1, bias = False) if post_talking_heads else None\n        self.pre_scale_post_talking_heads = nn.Conv2d(heads, heads, 1, bias = False) if pre_scale_post_talking_heads else None\n\n        if exists(self.pre_softmax_talking_heads):\n            nn.init.dirac_(self.pre_softmax_talking_heads.weight)\n\n        if exists(self.post_softmax_talking_heads):\n            nn.init.dirac_(self.post_softmax_talking_heads.weight)\n\n        if exists(self.pre_scale_post_talking_heads):\n            # an improvisation where heads are combined pre-softmax attention, then used to scale post-softmax attention\n            nn.init.dirac_(self.pre_scale_post_talking_heads.weight)\n\n        # selective attention\n\n        assert not (flash and selective), 'selective attention cannot work on flash attention'\n        assert not (selective and not causal), 'selective attention is designed for autoregressive'\n        self.selective = selective\n\n        # cog attention - negative weights for expressiveness\n        # https://openreview.net/forum?id=ezRrwwbxd0\n\n        assert not (flash and cog_signed), 'cog attention not available for flash'\n        self.cog_signed = cog_signed\n\n        # l2 distance attention\n\n        self.l2_distance = l2_distance\n\n        # add a key / value token composed of zeros\n        # in case this helps controlling outliers, proposed by https://www.evanmiller.org/attention-is-off-by-one.html\n\n        self.add_zero_kv = add_zero_kv\n\n        # learned sink concatted pre-softmax, working solution from gpt-oss\n\n        assert not (head_learned_sink and flash), f'not supported for flash attention yet'\n\n        self.head_learned_sink = head_learned_sink\n        self.head_attn_sink = Parameter(torch.zeros(heads)) if head_learned_sink else None\n\n        # soft clamp attention logit value\n\n        if softclamp_logits:\n            assert not flash, 'flash attention not compatible with logit softclamp value yet'\n            assert logit_softclamp_value > 0.\n\n        self.softclamp_logits = softclamp_logits\n        self.logit_softclamp_value = logit_softclamp_value\n\n        # contextual positional encoding\n\n        self.cope = cope\n\n        # flash attention\n\n        self.flash = flash\n\n        torch_version = version.parse(torch.__version__)\n        assert not (flash and torch_version < version.parse('2.0.0')), 'in order to use flash attention, you must be using pytorch 2.0 or above'\n\n        # torch 2.3 uses new backend and context manager\n\n        if self.flash:\n            if torch_version >= version.parse('2.3'):\n                from torch.nn.attention import SDPBackend\n\n                str_to_backend = dict(\n                    enable_flash = SDPBackend.FLASH_ATTENTION,\n                    enable_mem_efficient = SDPBackend.EFFICIENT_ATTENTION,\n                    enable_math = SDPBackend.MATH,\n                    enable_cudnn = SDPBackend.CUDNN_ATTENTION\n                )\n\n                sdpa_backends = [str_to_backend[enable_str] for enable_str, enable in sdp_kwargs.items() if enable]\n\n                self.sdp_context_manager = partial(torch.nn.attention.sdpa_kernel, sdpa_backends)\n            else:\n                self.sdp_context_manager = partial(torch.backends.cuda.sdp_kernel, **sdp_kwargs)\n\n    def flash_attn(\n        self,\n        q, k, v,\n        mask = None,\n        attn_bias = None\n    ):\n        batch, heads, q_len, _, k_len, is_cuda, device = *q.shape, k.shape[-2], q.is_cuda, q.device\n\n        # Recommended for multi-query single-key-value attention by Tri Dao\n        # kv shape torch.Size([1, 512, 64]) -> torch.Size([1, 8, 512, 64])\n\n        if k.ndim == 3:\n            k = repeat(k, 'b ... -> b h ...', h = q.shape[1])\n\n        if v.ndim == 3:\n            v = repeat(v, 'b ... -> b h ...', h = q.shape[1])\n\n        # handle maybe l2 distance\n\n        if self.l2_distance:\n            k_norm_sq = k.norm(dim = -1, keepdim = True) ** 2\n            k = F.pad(k, (0, 1), value = -1.)\n            k = cat((k, k_norm_sq), dim = -1)\n\n            q_norm_sq = q.norm(dim = -1, keepdim = True) ** 2\n            q = cat((2 * q, q_norm_sq), dim = -1)\n            q = F.pad(q, (0, 1), value = -1.)\n\n        # handle scale - by default they scale by dim_head ** -0.5, but need to take care if using cosine sim attention\n\n        if exists(self.scale):\n            default_scale = q.shape[-1] ** -0.5\n            q = q * (self.scale / default_scale)\n\n        # Check if mask exists and expand to compatible shape\n        # The mask is B L, so it would have to be expanded to B H N L\n\n        causal = self.causal\n\n        # in the case of kv caching with one token (q_len == 1), just turn off causal masking\n        # in speculative decoding, this may go up to 5-6, so right aligned causal mask will be needed there\n\n        if q_len == 1 and causal:\n            causal = False\n\n        # expand key padding mask\n\n        if exists(mask):\n            assert mask.ndim == 4\n            mask = mask.expand(batch, heads, q_len, k_len)\n\n        # handle kv cache - this should be bypassable in updated flash attention 2\n\n        if k_len > q_len and causal:\n            causal_mask = self.create_causal_mask(q_len, k_len, device = device)\n            if not exists(mask):\n                mask = ~causal_mask\n            else:\n                mask = mask & ~causal_mask\n            causal = False\n\n        # manually handle causal mask, if another mask was given\n\n        if exists(mask) and causal:\n            causal_mask = self.create_causal_mask(q_len, k_len, device = device)\n            mask = mask & ~causal_mask\n            causal = False\n\n        # protect against an entire row being masked out\n\n        row_is_entirely_masked = None\n\n        if exists(mask):\n            row_is_entirely_masked = ~mask.any(dim = -1)\n\n        # handle alibi positional bias\n        # convert from bool to float\n\n        if exists(attn_bias):\n            attn_bias = attn_bias.expand(batch, heads, -1, -1)\n\n            # if mask given, the mask would already contain the causal mask from above logic\n            # otherwise, if no mask given but still causal, mask out alibi positional bias to a large negative number\n\n            mask_value = -torch.finfo(q.dtype).max\n\n            if exists(mask):\n                attn_bias = attn_bias.masked_fill(~mask, mask_value // 2)\n            elif causal:\n                causal_mask = self.create_causal_mask(q_len, k_len, device = device)\n                attn_bias = attn_bias.masked_fill(causal_mask, mask_value // 2)\n                causal = False\n\n            # scaled_dot_product_attention handles attn_mask either as bool or additive bias\n            # make it an additive bias here\n\n            mask = attn_bias\n\n        # pytorch 2.0 flash attn: q, k, v, mask, dropout, causal, softmax_scale\n\n        with self.sdp_context_manager():\n            out = F.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask = mask,\n                dropout_p = self.dropout if self.training else 0., \n                is_causal = causal\n            )\n\n        # for a row that is entirely masked out, should zero out the output of that row token\n\n        if exists(row_is_entirely_masked) and row_is_entirely_masked.any():\n            out = out.masked_fill(row_is_entirely_masked[..., None], 0.)\n\n        return out, Intermediates()\n\n    def forward(\n        self,\n        q, k, v,\n        mask = None,\n        attn_bias = None,\n        prev_attn = None\n    ):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        n, heads, kv_heads, device = q.shape[-2], q.shape[1], k.shape[1], q.device\n\n        scale = default(self.scale, q.shape[-1] ** -0.5)\n\n        causal = self.causal\n\n        # handle key padding mask\n\n        if exists(mask) and mask.ndim == 2:\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n\n        # handle kv cached decoding\n\n        if n == 1 and causal:\n            causal = False\n\n        # handle grouped multi-query attention\n\n        if kv_heads == 1:\n            k, v = tuple(rearrange(t, 'b 1 n d -> b n d') for t in (k, v))\n        elif kv_heads < heads:\n            k, v = tuple(repeat(t, 'b kvh n d -> b (r kvh) n d', r = heads // kv_heads) for t in (k, v))\n\n        # handle zero kv, as means for allowing network to attend to nothing\n\n        if self.add_zero_kv:\n            k, v = tuple(F.pad(t, (0, 0, 1, 0), value = 0.) for t in (k, v))\n\n            if exists(mask):\n                mask = F.pad(mask, (1, 0), value = True)\n\n            if exists(attn_bias):\n                attn_bias = F.pad(attn_bias, (1, 0), value = 0.)\n\n        if self.flash:\n            assert not exists(prev_attn), 'residual attention not compatible with flash attention'\n            return self.flash_attn(q, k, v, mask = mask, attn_bias = attn_bias)\n\n        kv_einsum_eq = 'b j d' if k.ndim == 3 else 'b h j d'\n\n        if not self.l2_distance:\n            sim = einsum(f'b h i d, {kv_einsum_eq} -> b h i j', q, k)\n        else:\n            sim = -qk_l2_dist_squared(q, k)\n\n        sim = sim * scale\n\n        if exists(prev_attn):\n            sim = sim + prev_attn\n\n        qk_similarities = sim.clone()\n\n        if exists(self.pre_scale_post_talking_heads):\n            pre_to_post_scale = self.pre_scale_post_talking_heads(sim)\n\n        if exists(self.pre_softmax_talking_heads):\n            sim = sim + self.pre_softmax_talking_heads(sim)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        if self.softclamp_logits:\n            sim = softclamp(sim, self.logit_softclamp_value)\n\n        # pre-masking - handle cog by storing sign\n\n        if self.cog_signed:\n            sim_sign = sim.sign()\n            sim = sim.abs()\n\n        # masking\n\n        i, j, dtype = *sim.shape[-2:], sim.dtype\n\n        mask_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            sim = sim.masked_fill(~mask, mask_value)\n\n        if causal:\n            causal_mask = self.create_causal_mask(i, j, device = device)\n            sim = sim.masked_fill(causal_mask, mask_value)\n\n        row_is_entirely_masked = None\n\n        if exists(mask):\n            row_is_entirely_masked = ~mask.any(dim = -1)\n\n        if exists(self.cope):\n            sim = sim + self.cope(q, sim)\n\n        if self.selective:\n            sim = selective_attn(sim)\n\n        if self.head_learned_sink:\n            # add learned attention sink\n            attn_sink = repeat(self.head_attn_sink, 'h -> b h i 1', b = sim.shape[0], i = sim.shape[2])\n\n            if self.cog_signed:\n                attn_sink, attn_sink_sign = attn_sink.abs(), attn_sink.sign()\n                sim_sign = cat((attn_sink_sign, sim_sign), dim = -1)\n\n            sim = cat((attn_sink, sim), dim = -1)\n\n        pre_softmax_attn = sim\n\n        attn = self.attn_fn(sim)\n\n        attn = attn.type(dtype)\n\n        # add back the sign\n\n        if self.cog_signed:\n            attn = attn * sim_sign\n\n        post_softmax_attn = attn\n\n        if self.head_learned_sink:\n            # remove attention sink\n            attn = attn[..., 1:]\n\n        attn = self.attn_dropout(attn)\n\n        if exists(self.post_softmax_talking_heads):\n            attn = self.post_softmax_talking_heads(attn)\n\n        if exists(self.pre_scale_post_talking_heads):\n            attn = attn * pre_to_post_scale\n\n        out = einsum(f'b h i j, {kv_einsum_eq} -> b h i d', attn, v)\n\n        intermediates = Intermediates(\n            qk_similarities = qk_similarities,\n            pre_softmax_attn = pre_softmax_attn,\n            post_softmax_attn = post_softmax_attn\n        )\n\n        if exists(row_is_entirely_masked) and row_is_entirely_masked.any():\n            out = out.masked_fill(row_is_entirely_masked[..., None], 0.)\n\n        return out, intermediates", "test_code": "", "difficulty": "medium", "category": "cnn", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/attend.py", "name": "Attend", "line": 167}}
