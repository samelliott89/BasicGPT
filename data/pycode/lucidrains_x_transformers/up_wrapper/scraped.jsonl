{"prompt": "Create a synthetic data generator neural network module", "code": "class SyntheticDataGenerator(Module):\n    def __init__(\n        self,\n        dim,\n        num_tokens,\n        max_seq_len = 512,\n        hidden_size = None,\n        use_gru = False,\n        network_klass = None\n    ):\n        super().__init__()\n\n        self.max_seq_len = max_seq_len\n\n        self.embed = nn.Embedding(num_tokens, dim)\n\n        hidden_size = default(hidden_size, dim)\n\n        default_network_klass = partial(LSTM if not use_gru else GRU, batch_first = True)\n        network_klass = default(network_klass, default_network_klass)\n\n        self.net = network_klass(dim, hidden_size)\n\n        self.to_logits = nn.Linear(dim, num_tokens, bias = False)\n\n        self.apply(self.init_)\n\n    def reset_(self):\n        for m in self.modules():\n            if hasattr(m, 'reset_parameters'):\n                m.reset_parameters()\n\n        self.apply(self.init_)\n\n    @torch.no_grad()\n    def init_(self, m):\n        if isinstance(m, nn.Linear):\n            m.weight *= uniform(0., 1.1) # he scales the lstm weights from 0 to 1.1\n\n    @torch.inference_mode()\n    @torch.compile\n    def generate(\n        self,\n        length,\n        seed = None,\n        condition = None,\n        temperature = 1e-4 # he uses a near greedy temperature\n    ):\n        assert exists(seed) or exists(condition)\n        prefix = [*filter(exists, (seed, condition))]\n        seq_len = self.max_seq_len\n\n        seq = torch.cat(prefix, dim = -1)\n\n        net_input = seq\n        hiddens = None\n\n        for _ in range(length):\n\n            logits, hiddens = self.forward(net_input, hiddens)\n\n            last_logit = logits[:, -1]\n            prob = (last_logit / temperature).softmax(dim = -1)\n\n            sampled = torch.multinomial(prob, 1)\n            net_input = sampled\n\n            seq = torch.cat((seq, sampled), dim = -1)\n\n        return seq[:, -seq_len:]\n\n    def forward(\n        self,\n        input,\n        hiddens = None\n    ):\n\n        tokens = self.embed(input)\n\n        embed, hidden = self.net(tokens, hiddens)\n\n        logits = self.to_logits(embed)\n\n        return logits, hidden", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/up_wrapper.py", "name": "SyntheticDataGenerator", "line": 58}}
{"prompt": "Create a universal pretrain wrapper neural network module", "code": "class UniversalPretrainWrapper(Module):\n    def __init__(\n        self,\n        model: TransformerWrapper,\n        data_generator: SyntheticDataGenerator | Module | None = None,\n        buffer_size = None,\n        num_reset = 20,\n        batch_size = 32,\n        seq_len = 512,\n        seed_length = 8,\n        reset_turing_machine_every = 0,\n        keep_buffer_on_cpu = False\n    ):\n        super().__init__()\n\n        self.model = model\n        self.ar_wrapped = AutoregressiveWrapper(model)\n\n        assert model.attn_layers.causal\n\n        num_tokens = model.num_tokens\n        dim = model.attn_layers.dim\n\n        if not exists(data_generator):\n            data_generator = SyntheticDataGenerator(\n                num_tokens = num_tokens,\n                dim = dim,\n                max_seq_len = seq_len\n            )\n\n        self.reset_turing_machine_every = reset_turing_machine_every\n\n        self.seq_len = seq_len\n        self.data_generator = data_generator\n\n        self.seed_length = seed_length\n        self.batch_size = batch_size\n\n        buffer_size = default(buffer_size, batch_size * 20)\n        assert buffer_size > batch_size, f'data buffer size must be greater than batch size'\n\n        assert divisible_by(num_reset, 2)\n        self.num_reset = num_reset\n\n        self.buffer_size = buffer_size\n\n        self.random_sequences_fn = partial(random_sequences, num_tokens, seq_len)\n\n        init_data_buffer = self.random_sequences_fn(buffer_size // 2, buffer_size // 2)\n\n        if keep_buffer_on_cpu:\n            self.synth_data_buffer = init_data_buffer\n        else:\n            self.register_buffer('synth_data_buffer', init_data_buffer)\n\n        self.register_buffer('step', tensor(0))\n\n    @property\n    def device(self):\n        return self.step.device\n\n    def get_rand_sequences_from_buffer(self, size = None):\n        size = default(size, self.batch_size)\n        rand_indices = randperm(self.buffer_size, device = self.device)[:size]\n        return self.synth_data_buffer[rand_indices]\n\n    def forward(self):\n        # following algorithm 1.\n\n        conditions = self.get_rand_sequences_from_buffer()\n\n        # get seeds, which appears to be random sequences with random crops of seed length\n\n        seeds = self.get_rand_sequences_from_buffer()\n\n        seq_arange = torch.arange(self.seed_length)\n        rand_offset = torch.randint(0, self.seq_len - self.seed_length, (self.batch_size,))\n        seq_start_pos = rand_offset[:, None] + seq_arange\n\n        batch_arange = torch.arange(self.batch_size, device = self.device)[:, None]\n        seeds = seeds[batch_arange, seq_start_pos]\n\n        # seed, condition to turing machine\n\n        generated = self.data_generator.generate(\n            self.seq_len,\n            condition = conditions.to(self.device),\n            seed = seeds.to(self.device)\n        )\n\n        self.step.add_(1)\n\n        # maybe reset turing machine\n\n        if self.reset_turing_machine_every > 0 and divisible_by(self.step.item(), self.reset_turing_machine_every):\n            self.data_generator.reset_()\n\n        # reset\n\n        if self.num_reset > 0:\n            buffer_to_reset = self.get_rand_sequences_from_buffer(self.num_reset)\n\n            with torch.no_grad():\n                reset_sequences = self.random_sequences_fn(self.num_reset // 2, self.num_reset // 2, device = self.device)\n                buffer_to_reset.copy_(reset_sequences)\n\n        # place \"enriched\" random generated sequences back\n\n        with torch.no_grad():\n            conditions.copy_(generated)\n\n        # sample yet again according to pseudocode\n\n        data = self.get_rand_sequences_from_buffer().to(self.device)\n\n        return self.ar_wrapped(data)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/up_wrapper.py", "name": "UniversalPretrainWrapper", "line": 145}}
