{"prompt": "Write a Python program that uses PyTorch to implement a solution for a specific problem.", "code": "class SoLU(Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.norm = LayerNorm(dim)\n\n    def forward(self, x):\n        activated = x.softmax(dim = -1) * x\n        return self.norm(activated)", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "Create a so lu neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "To build a class named TokenEmbedding in PyTorch, which is a subclass of torch.nn.Module. This class is designed to handle token embeddings. The main function of this class is to convert input tokens into embeddings, either in L2-norm form or not.", "code": "class TokenEmbedding(Module):\n    def __init__(self, dim, num_tokens, l2norm_embed = False):\n        super().__init__()\n        self.l2norm_embed = l2norm_embed\n        self.emb = nn.Embedding(num_tokens, dim)\n\n    def forward(self, x):\n        token_emb = self.emb(x.long())\n        return l2norm(token_emb) if self.l2norm_embed else token_emb\n\n    def init_(self):\n        if self.l2norm_embed:\n            nn.init.normal_(self.emb.weight, std=1e-5)\n            return\n        nn.init.kaiming_normal_(self.emb.weight)", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "synthetic", "metadata": {"original_prompt": "Create a token embedding neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "The code defines a custom PyTorch module for creating absolute positional embeddings. The positional embedding is created by embedding a sequence length into a specified dimension, with positional information passed in as an optional parameter. If the `l2norm_embed` flag is set to True, the embeddings are normalized to L2 norm.", "code": "class AbsolutePositionalEmbedding(Module):\n    def __init__(self, dim, max_seq_len, l2norm_embed = False):\n        super().__init__()\n        self.scale = dim ** -0.5 if not l2norm_embed else 1.\n        self.max_seq_len = max_seq_len\n        self.l2norm_embed = l2norm_embed\n        self.emb = nn.Embedding(max_seq_len, dim)\n\n    def forward(\n        self,\n        x,\n        pos = None,\n        seq_start_pos = None,\n        offset = 0\n    ):\n        seq_len, device = x.shape[1], x.device\n        assert seq_len <= self.max_seq_len, f'you are passing in a sequence length of {seq_len} but your absolute positional embedding has a max sequence length of {self.max_seq_len}'\n\n        if not exists(pos):\n            pos = arange(seq_len, device = device) + offset\n\n        if exists(seq_start_pos):\n            pos = (pos - seq_start_pos[..., None]).clamp(min = 0)\n\n        pos_emb = self.emb(pos)\n        pos_emb = pos_emb * self.scale\n        return l2norm(pos_emb) if self.l2norm_embed else pos_emb", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "synthetic", "metadata": {"original_prompt": "Create a absolute positional embedding neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch module that generates a scaled sinusoidal embedding. The output is a PyTorch module that generates an embedding of a sequence of data.", "code": "class ScaledSinusoidalEmbedding(Module):\n    def __init__(self, dim, theta = 10000):\n        super().__init__()\n        assert divisible_by(dim, 2)\n        self.scale = nn.Parameter(torch.ones(1) * dim ** -0.5)\n\n        half_dim = dim // 2\n        freq_seq = arange(half_dim).float() / half_dim\n        inv_freq = theta ** -freq_seq\n        self.register_buffer('inv_freq', inv_freq, persistent = False)\n\n    def forward(\n        self,\n        x,\n        pos = None,\n        seq_start_pos = None,\n        offset = 0\n    ):\n        seq_len, device = x.shape[1], x.device\n\n        if not exists(pos):\n            pos = arange(seq_len, device = device) + offset\n\n        if exists(seq_start_pos):\n            pos = pos - seq_start_pos[..., None]\n\n        emb = einsum('i, j -> i j', pos, self.inv_freq)\n        emb = cat((emb.sin(), emb.cos()), dim = -1)\n        return emb * self.scale", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "synthetic", "metadata": {"original_prompt": "Create a scaled sinusoidal embedding neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Write a PyTorch code snippet that represents a class named `RelativePositionBias`. The class should have a method `forward` that takes two arguments: `i` and `j`, which represent the start and end indices of the attention sequence. The method should calculate and return a bias vector that is used in a self-attention mechanism.", "code": "class RelativePositionBias(Module):\n    def __init__(self, scale, causal = False, num_buckets = 32, max_distance = 128, heads = 8):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, causal = True, num_buckets = 32, max_distance = 128):\n        ret = 0\n        n = -relative_position\n        if not causal:\n            num_buckets //= 2\n            ret += (n < 0).long() * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = max_exact + (\n            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n        ).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, i, j):\n        device = self.device\n        q_pos = arange(j - i, j, dtype = torch.long, device = device)\n        k_pos = arange(j, dtype = torch.long, device = device)\n        rel_pos = einx.subtract('j, i -> i j', k_pos, q_pos)\n        rp_bucket = self._relative_position_bucket(rel_pos, causal = self.causal, num_buckets = self.num_buckets, max_distance = self.max_distance)\n        values = self.relative_attention_bias(rp_bucket)\n        bias = rearrange(values, 'i j h -> h i j')\n        return bias * self.scale", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "synthetic", "metadata": {"original_prompt": "Create a relative position bias neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a model to implement the CoPE (Coordinate Positional Encoding) model described in the Appendix B of the paper.", "code": "class CoPE(Module):\n    \"\"\"\n    Appendix B of https://arxiv.org/abs/2405.18719\n    \"\"\"\n    def __init__ (\n        self,\n        dim,\n        heads,\n        max_pos,\n        soft_onehot = False,\n        talking_heads = False,\n        soft_onehot_temp = 5e-2\n    ):\n        super () . __init__ ()\n        self.max_pos = max_pos\n        self.pos_emb = nn.Parameter(torch.zeros(max_pos, dim))\n\n        self.talking_heads = nn.Conv2d(heads, heads, 1, bias = False) if talking_heads else None\n        self.soft_onehot = soft_onehot\n        self.soft_onehot_temp = soft_onehot_temp\n\n        if not soft_onehot:\n            return\n\n        self.register_buffer('positions', arange(max_pos))\n\n    def forward(self, query, attn_logits):\n\n        if exists(self.talking_heads):\n            i, j = attn_logits.shape[-2:]\n            causal_mask = attn_logits.new_ones(i, j).triu_(j - i + 1).bool()\n\n            attn_logits = self.talking_heads(attn_logits)\n\n            attn_logits = attn_logits.masked_fill(causal_mask, -torch.finfo(attn_logits.dtype).max)\n\n        # compute positions\n\n        gates = attn_logits.sigmoid()\n\n        pos = gates.flip(-1).cumsum(dim = -1).flip(-1)\n        pos = pos.clamp(max = self.max_pos - 1)\n\n        logits_int = einsum('b h n d, p d -> b h n p', query, self.pos_emb)\n\n        if self.soft_onehot:\n            diff_pos = einx.subtract('i, j -> i j', pos, self.positions).abs()\n            soft_onehot_pos = F.softmax(-diff_pos / self.soft_onehot_temp, dim = -1)\n            cope_pos_emb = einsum('b h i j p, b h i p -> b h i j', soft_onehot_pos, logits_int)\n        else:\n            # interpolate from integer positions\n            pos_ceil = pos.ceil().long()\n            pos_floor = pos.floor().long()\n            logits_ceil = logits_int.gather(-1, pos_ceil)\n            logits_floor = logits_int.gather(-1, pos_floor)\n\n            w = pos - pos_floor\n            cope_pos_emb = logits_ceil * w + logits_floor * (1 - w)\n\n        return cope_pos_emb", "test_code": "", "difficulty": "medium", "category": "cnn", "source": "synthetic", "metadata": {"original_prompt": "Appendix B of https://arxiv.org/abs/2405.18719", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "This code defines a PyTorch module that uses a Multi-Layer Perceptron (MLP) for dynamic position bias. The MLP is designed to calculate the distance between sequences (or more generally, positions in a sequence) and uses this as input to a layer normalization and activation function. The final layer of the MLP outputs the position bias.", "code": "class DynamicPositionBias(Module):\n    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n        super().__init__()\n        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n        self.log_distance = log_distance\n\n        self.mlp = ModuleList([])\n\n        self.mlp.append(Sequential(\n            nn.Linear(1, dim),\n            LayerNorm(dim) if norm else None,\n            nn.SiLU()\n        ))\n\n        for _ in range(depth - 1):\n            self.mlp.append(Sequential(\n                nn.Linear(dim, dim),\n                nn.LayerNorm(dim) if norm else None,\n                nn.SiLU()\n            ))\n\n        self.mlp.append(nn.Linear(dim, heads))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, i, j):\n        n, device = j, self.device\n\n        # get the (n x n) matrix of distances\n        seq_arange = arange(j - i, j, device = device)\n        context_arange = arange(j, device = device)\n        indices = einx.subtract('i, j -> i j', seq_arange, context_arange)\n        indices += (j - 1)\n\n        # input to continuous positions MLP\n        pos = arange(-j + 1, j, device = device).float()\n        pos = rearrange(pos, '... -> ... 1')\n\n        if self.log_distance:\n            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n\n        for layer in self.mlp:\n            pos = layer(pos)\n\n        # get position biases        \n        bias = pos[indices]\n        bias = rearrange(bias, 'i j h -> h i j')\n        return bias", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "synthetic", "metadata": {"original_prompt": "Create a dynamic position bias neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch model that can generate a positional bias. The positional bias is the difference between the position of the current element in the sequence and the position of the previous element in the sequence. The model takes two arguments: the number of heads and the total number of elements. The positional bias is calculated using the formula: bias = -(position - previous_position).abs() where position is the current position and previous_position is the previous position. The model", "code": "class AlibiPositionalBias(Module):\n    def __init__(\n        self,\n        heads,\n        total_heads = None,\n        slopes: list[int] | None = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.heads = heads\n        self.total_heads = default(total_heads, heads)\n\n        slopes = Tensor(default(slopes, self._get_slopes(heads)))\n        slopes = rearrange(slopes, 'h -> h 1 1')\n\n        self.register_buffer('slopes', slopes, persistent = False)\n        self.register_buffer('bias', None, persistent = False)\n    \n    @property\n    def device(self):\n        return next(self.buffers()).device\n\n    @staticmethod\n    def _get_slopes(heads):\n        def get_slopes_power_of_2(n):\n            start = (2**(-2**-(math.log2(n)-3)))\n            ratio = start\n            return [start*ratio**i for i in range(n)]\n\n        if math.log2(heads).is_integer():\n            return get_slopes_power_of_2(heads)\n\n        closest_power_of_2 = 2 ** math.floor(math.log2(heads))\n        return get_slopes_power_of_2(closest_power_of_2) + get_slopes_power_of_2(2 * closest_power_of_2)[0::2][:heads-closest_power_of_2]\n\n    def forward_custom_pos(\n        self,\n        pos_i: Tensor,\n        pos_j: Tensor | None = None\n    ):\n        h, device = self.total_heads, self.device\n\n        pos_j = default(pos_j, pos_i)\n        bias = -einx.subtract('... j, ... i -> ... i j', pos_j, pos_i).abs()\n\n        if bias.ndim == 3:\n            bias = rearrange(bias, 'b i j -> b 1 i j')\n\n        bias = bias * self.slopes\n        num_heads_unalibied = h - bias.shape[-3]\n        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim = -3)\n\n        return bias\n\n    def forward(self, i, j):\n        h, device = self.total_heads, self.device\n\n        if exists(self.bias) and self.bias.shape[-1] >= j and self.bias.shape[-2] >= i:\n            return self.bias[..., -i:, -j:]\n\n        seq_arange = arange(j - i, j, device = device)\n        context_arange = arange(j, device = device)\n        bias = -einx.subtract('j, i -> 1 i j', context_arange, seq_arange).abs()\n\n        bias = bias * self.slopes\n        num_heads_unalibied = h - bias.shape[-3]\n        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim = -3)\n\n        self.register_buffer('bias', bias, persistent = False)\n        return self.bias", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "synthetic", "metadata": {"original_prompt": "Create a alibi positional bias neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Create a PyTorch module that implements a mechanism for generating \"alibi\" data based on input data. The \"alibi\" data is a modification of the input data that has been intentionally manipulated in such a way that it's harder to learn. This mechanism is called \"DataDependentAlibi\".", "code": "class DataDependentAlibi(Module):\n    \"\"\" https://openreview.net/forum?id=q2Lnyegkr8 \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        heads,\n        causal = True,\n        bias_init = 5.,\n        post_log_scale = 1.,\n    ):\n        super().__init__()\n\n        self.causal = causal\n\n        linear = nn.Linear(dim, heads * (1 if causal else 2))\n\n        self.to_forget_gates = nn.Sequential(\n            linear,\n            Rearrange('b n h -> b h n'),\n            nn.LogSigmoid()\n        )\n\n        nn.init.constant_(linear.bias, bias_init)\n        self.post_log_scale = post_log_scale\n\n    def forward(self, x):\n        bidirectional = not self.causal\n\n        forget_gates = self.to_forget_gates(x) * self.post_log_scale\n\n        forget_gates = forget_gates.cumsum(dim = -1)\n\n        if bidirectional:\n            forget_gates, forget_gates_reversed = forget_gates.chunk(2, dim = 1)\n\n        forget_gates = einx.subtract('b h i, b h j -> b h i j', forget_gates, forget_gates)\n\n        if bidirectional:\n            forget_gates_reversed = einx.subtract('b h j, b h i -> b h i j', forget_gates_reversed, forget_gates_reversed)\n            forget_gates = forget_gates.tril() + forget_gates_reversed.triu()\n\n        return forget_gates", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "https://openreview.net/forum?id=q2Lnyegkr8", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Write a PyTorch module that implements a self-attention mechanism where the forgetting gates are derived from queries and keys with a small head dimension.", "code": "class PerRowDataDependentAlibi(Module):\n    \"\"\" same as data dependent alibi from forgetting transformer, but the forgetting gates are also derived by a queries and keys with a small head dimension \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        heads,\n        causal = True,\n        dim_head = 8,\n        post_log_scale = 1.\n    ):\n        super().__init__()\n        assert causal, 'bidirectional not supported yet'\n\n        self.scale = dim_head ** -0.5\n\n        linear = nn.Linear(dim, heads * dim_head * 2, bias = False)\n\n        self.to_forget_gates = nn.Sequential(\n            linear,\n            Rearrange('b n (qk h d) -> qk b h n d', qk = 2, d = dim_head)\n        )\n\n        self.post_log_scale = post_log_scale\n\n    def forward(self, x):\n        q, k = self.to_forget_gates(x)\n        forget_gates = einsum('... i d, ... j d -> ... i j', q, k) * self.scale\n\n        forget_gates = F.logsigmoid(forget_gates) * self.post_log_scale\n\n        # mask out upper triangle + diagonal\n\n        n = x.shape[-2]\n        causal_mask = torch.ones((n, n), dtype = torch.bool, device = x.device).triu()\n\n        forget_gates = forget_gates.masked_fill(causal_mask, 0.)\n\n        # reverse cumsum\n\n        forget_gates = forget_gates.flip(dims = (-1,))\n        forget_gates = forget_gates.cumsum(dim = -1)\n        forget_gates = forget_gates.flip(dims = (-1,))\n\n        return forget_gates", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "same as data dependent alibi from forgetting transformer, but the forgetting gates are also derived by a queries and keys with a small head dimension", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "I'm implementing a PyTorch module that scales the output of a function by a given value. This is a simple function that scales the input to a PyTorch module.", "code": "class Scale(Module):\n    def __init__(self, value, fn):\n        super().__init__()\n        self.value = value\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n        scale_fn = lambda t: t * self.value\n\n        if not isinstance(out, tuple):\n            return scale_fn(out)\n\n        return (scale_fn(out[0]), *out[1:])", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "Create a scale neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch layer normalization layer.", "code": "class LayerNorm(Module):\n    def __init__(\n        self,\n        dim,\n        unit_offset = False\n    ):\n        \"\"\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \"\"\"\n        super().__init__()\n        self.unit_offset = unit_offset\n\n        self.ln = nn.LayerNorm(dim, elementwise_affine = False)\n        self.gamma = nn.Parameter(torch.ones(dim))\n        nn.init.constant_(self.gamma, 1. - float(unit_offset))\n\n    def forward(self, x):\n        normed = self.ln(x)\n        gamma = self.gamma + float(self.unit_offset)\n        return normed * gamma", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "synthetic", "metadata": {"original_prompt": "Create a layer norm neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch model class that implements an adaptive layer normalization (ALN). ALN normalizes each layer's output independently, with a learned gamma and beta parameters. This class should accept a dimension, and optionally a condition dimension. If no condition dimension is provided, the default is the same as the input dimension.", "code": "class AdaptiveLayerNorm(Module):\n    def __init__(\n        self,\n        dim,\n        dim_condition = None\n    ):\n        super().__init__()\n        dim_condition = default(dim_condition, dim)\n\n        self.ln = nn.LayerNorm(dim, elementwise_affine = False)\n        self.to_gamma = LinearNoBias(dim_condition, dim)\n        nn.init.zeros_(self.to_gamma.weight)\n\n    def forward(self, x, *, condition):\n        if condition.ndim == 2:\n            condition = rearrange(condition, 'b d -> b 1 d')\n\n        normed = self.ln(x)\n        gamma = self.to_gamma(condition)\n        return normed * (gamma + 1.)", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "synthetic", "metadata": {"original_prompt": "Create a adaptive layer norm neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Write a PyTorch module named `ScaleNorm` that normalizes a given input tensor. This normalization is done in such a way that the output tensor has the same distribution as the input tensor, but with each dimension's scale and offset adjusted accordingly.", "code": "class ScaleNorm(Module):\n    def __init__(\n        self,\n        dim,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.unit_offset = unit_offset\n        self.scale = dim ** 0.5\n\n        self.g = nn.Parameter(torch.zeros(1))\n        nn.init.constant_(self.g, 1. - float(unit_offset))\n\n    def forward(self, x):\n        gamma = self.g + float(self.unit_offset)\n        return F.normalize(x, dim = -1) * self.scale * gamma", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "synthetic", "metadata": {"original_prompt": "Create a scale norm neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch module that implements a Root Mean Square (RMS) normalization.", "code": "class RMSNorm(Module):\n    def __init__(\n        self,\n        dim,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.unit_offset = unit_offset\n        self.scale = dim ** 0.5\n\n        self.g = nn.Parameter(torch.zeros(dim))\n        nn.init.constant_(self.g, 1. - float(unit_offset))\n\n    def forward(self, x):\n        gamma = self.g + float(self.unit_offset)\n        return F.normalize(x, dim = -1) * self.scale * gamma", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "synthetic", "metadata": {"original_prompt": "Create a rms norm neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Write: \"A PyTorch class that implements the Adaptive RMS Norm (ARMN) layer.\"", "code": "class AdaptiveRMSNorm(Module):\n    def __init__(\n        self,\n        dim,\n        dim_condition = None\n    ):\n        super().__init__()\n        self.scale = dim ** 0.5\n        dim_condition = default(dim_condition, dim)\n\n        self.to_gamma = LinearNoBias(dim_condition, dim)\n        nn.init.zeros_(self.to_gamma.weight)\n\n    def forward(self, x, *, condition):\n        if condition.ndim == 2:\n            condition = rearrange(condition, 'b d -> b 1 d')\n\n        normed = F.normalize(x, dim = -1)\n        gamma = self.to_gamma(condition)\n        return normed * self.scale * (gamma + 1.)", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "synthetic", "metadata": {"original_prompt": "Create a adaptive rms norm neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch module named \"SimpleRMSNorm\" that normalizes the input tensor \"x\" along the dimension -1. The normalization is performed by scaling the tensor by its reciprocal square root. The result is a tensor with the same shape as the input tensor \"x\" but with each element scaled by the reciprocal square root of its corresponding element in the original tensor.", "code": "class SimpleRMSNorm(Module):\n    def __init__(\n        self,\n        dim,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim ** 0.5\n\n    def forward(self, x):\n        return F.normalize(x, dim = -1) * self.scale", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "synthetic", "metadata": {"original_prompt": "Create a simple rms norm neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Write a PyTorch implementation of a Multihead RMSNorm layer.", "code": "class MultiheadRMSNorm(Module):\n    def __init__(self, dim, heads):\n        super().__init__()\n        self.rmsnorm = SimpleRMSNorm(dim)\n        self.gamma = nn.Parameter(torch.zeros(heads, 1, dim))\n\n    def forward(self, x):\n        return self.rmsnorm(x) * (self.gamma + 1.)", "test_code": "", "difficulty": "medium", "category": "attention", "source": "synthetic", "metadata": {"original_prompt": "Create a multihead rms norm neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch module named \"DynamicTanh\" that implements a dynamic tanh function. The dynamic part comes from the equation \"y = x * pre_tanh_scale * tanh(x) * gamma + beta\". The parameters of the module include \"dim\", \"init_alpha\", \"gamma\", \"beta\", and \"unit_offset\". The pre_tanh_scale is initialized with \"init_alpha\" or \"0\" depending on the", "code": "class DynamicTanh(Module):\n    \"\"\" https://arxiv.org/abs/2503.10622 \"\"\"\n    def __init__(\n        self,\n        dim,\n        init_alpha = 1.,\n        gamma = 1.,\n        beta = 0.,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.pre_tanh_scale = nn.Parameter(tensor(init_alpha))\n\n        self.gamma = nn.Parameter(torch.ones(dim))\n        self.beta = nn.Parameter(torch.zeros(dim))\n\n        self.pre_tanh_scale_offset = init_alpha if unit_offset else 0.\n        self.gamma_offset = float(unit_offset)\n\n        nn.init.constant_(self.pre_tanh_scale, 0 if unit_offset else init_alpha)\n        nn.init.constant_(self.gamma, 1. - float(unit_offset))\n\n    def forward(self, x):\n        pre_tanh_scale = self.pre_tanh_scale + self.pre_tanh_scale_offset\n        gamma = self.gamma + self.gamma_offset\n        return (x * pre_tanh_scale).tanh() * gamma + self.beta", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "https://arxiv.org/abs/2503.10622", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "The code defines a PyTorch module that includes a residual block. The residual block is a class that takes an input x and a residual, and returns the original input x plus the residual scaled by a scalar factor. The residual is scaled by a factor determined by a parameter and a constant. The output of the block is the original input x plus the scaled residual.", "code": "class Residual(Module):\n    def __init__(self, dim, scale_residual = False, scale_residual_constant = 1., **kwargs):\n        super().__init__()\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n        self.scale_residual_constant = scale_residual_constant\n\n    def prepare(self, residual):\n        return residual, residual, dict()\n\n    def forward(self, x, residual, **kwargs):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n\n        if self.scale_residual_constant != 1:\n            residual = residual * self.scale_residual_constant\n\n        return x + residual", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "Create a residual neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch model that implements a Gated Recurrent Unit (GRU). This GRU should be used to process sequences of data.", "code": "class GRUGating(Module):\n    def __init__(self, dim, scale_residual = False, **kwargs):\n        super().__init__()\n        self.gru = nn.GRUCell(dim, dim)\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n\n    def prepare(self, residual):\n        return residual, residual, dict()\n\n    def forward(self, x, residual, **kwargs):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n\n        gated_output = self.gru(\n            rearrange(x, 'b n d -> (b n) d'),\n            rearrange(residual, 'b n d -> (b n) d')\n        )\n\n        return gated_output.reshape_as(x)", "test_code": "", "difficulty": "medium", "category": "rnn", "source": "synthetic", "metadata": {"original_prompt": "Create a gru gating neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Write a PyTorch code for a HyperConnection class that implements a HyperConnection architecture as described in the paper \"Dynamic Only Connection for Efficient Video Super-Resolution\".", "code": "class HyperConnection(Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        layer_index,\n        num_residual_streams,\n        num_input_views = 1,\n        tanh = True,\n        **kwargs\n    ):\n        \"\"\"\n        https://arxiv.org/abs/2409.19606\n        Appendix J - Algorithm 2, Dynamic only\n        \"\"\"\n        super().__init__()\n\n        self.act = nn.Tanh() if tanh else nn.Identity()\n\n        self.norm = nn.LayerNorm(dim, bias = False)\n\n        self.num_residual_streams = num_residual_streams\n        self.layer_index = layer_index\n\n        self.static_beta = nn.Parameter(torch.ones(num_residual_streams))\n\n        init_alpha0 = torch.zeros((num_residual_streams, num_input_views))\n        init_alpha0[layer_index % num_residual_streams, :] = 1.\n\n        self.static_alpha = nn.Parameter(cat([init_alpha0, torch.eye(num_residual_streams)], dim = 1))\n\n        self.dynamic_alpha_fn = nn.Parameter(torch.zeros(dim, num_residual_streams + num_input_views))\n        self.dynamic_alpha_scale = nn.Parameter(torch.ones(()) * 1e-2)\n\n        self.num_input_views = num_input_views\n\n        self.dynamic_beta_fn = nn.Parameter(torch.zeros(dim))\n        self.dynamic_beta_scale = nn.Parameter(torch.ones(()) * 1e-2)\n\n    def prepare(self, residuals):\n\n        residuals = rearrange(residuals, '(b s) n d -> b n s d', s = self.num_residual_streams)\n\n        normed = self.norm(residuals)\n\n        wc_weight = self.act(normed @ self.dynamic_alpha_fn)\n        dynamic_alpha = wc_weight * self.dynamic_alpha_scale\n        alpha = dynamic_alpha + self.static_alpha\n\n        dc_weight = self.act(normed @ self.dynamic_beta_fn)\n        dynamic_beta = dc_weight * self.dynamic_beta_scale\n        beta = dynamic_beta + self.static_beta\n\n        # width connection\n\n        mix_h = einsum('... s t, ... s d -> ... t d', alpha, residuals)\n\n        views = self.num_input_views\n\n        if views == 1:\n            branch_input, residuals = mix_h[..., 0, :], mix_h[..., 1:, :]\n        else:\n            branch_input, residuals = mix_h[..., :views, :], mix_h[..., views:, :]\n            branch_input = rearrange(branch_input, '... v d -> v ... d')\n\n        return branch_input, residuals, dict(beta = beta)\n\n    def forward(self, x, residuals, *, beta):\n        residuals = einsum('b n d, b n s -> b n s d', x, beta) + residuals\n        return rearrange(residuals, 'b n s d -> (b s) n d')", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "Create a hyper connection neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch model that can dynamically learn the number of layers in a neural network. The model should take an input tensor, x, and a sequence of hidden states, hiddens, and output a sequence of hidden states. The number of layers in the model should be learned dynamically based on the input tensor and the sequence of hidden states.", "code": "class DynamicLIMe(Module):\n    def __init__(\n        self,\n        dim,\n        num_layers,\n        num_views = 1,\n        norm = True,\n        use_softmax = True\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.multiple_views = num_views > 1\n\n        self.to_weights = Sequential(\n            RMSNorm(dim) if norm else None,\n            nn.Linear(dim, num_views * num_layers),\n            Rearrange('... (views layers) -> views ... layers', views = num_views),\n            nn.Softmax(dim = -1) if use_softmax else nn.ReLU()\n        )\n\n    def forward(\n        self,\n        x,\n        hiddens\n    ):\n\n        if not is_tensor(hiddens):\n            hiddens = stack(hiddens)\n\n        assert hiddens.shape[0] == self.num_layers, f'expected hiddens to have {self.num_layers} layers but received {tuple(hiddens.shape)} instead (first dimension must be layers)'\n\n        weights = self.to_weights(x)\n\n        out = einsum('l b n d, v b n l -> v b n d', hiddens, weights)\n\n        if self.multiple_views:\n            return out\n\n        return rearrange(out, '1 ... -> ...')", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "Create a dynamic li me neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch module that performs a shift operation on the input tensor. The shift operation is defined by a list of shifts. Each shift is a tuple of shift parameters. The function applied to the shifted segments of the input tensor is also defined by the `fn` parameter. The `mask` parameter is optional and it is used to ignore some parts of the input tensor.", "code": "class ShiftTokens(Module):\n    def __init__(self, shifts, fn):\n        super().__init__()\n        self.fn = fn\n        self.shifts = tuple(shifts)\n\n    def forward(self, x, **kwargs):\n        mask = kwargs.get('mask', None)\n        shifts = self.shifts\n        segments = len(shifts)\n        feats_per_shift = x.shape[-1] // segments\n        splitted = x.split(feats_per_shift, dim = -1)\n        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n        segments_to_shift = [shift(*args, mask = mask) for args in zip(segments_to_shift, shifts)]\n        x = cat((*segments_to_shift, *rest), dim = -1)\n        return self.fn(x, **kwargs)", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "Create a shift tokens neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "The provided PyTorch code defines a custom PyTorch module that performs a certain operation on input data. The operation is a fold operation on the sequence, but the fold dimension is specified by the user. The function takes an additional module `fn` as input, which is the operation to be performed on the sequence.", "code": "class FoldAxially(Module):\n    def __init__(\n        self,\n        axial_dim,\n        fn: Module\n    ):\n        super().__init__()\n        self.fn = fn\n        self.axial_dim = axial_dim # will fold the sequence as rearrange(\"b (n axial_dim) ... -> (b axial_dim) n ...\")\n\n    def forward(\n        self,\n        x,\n        *args,\n        **kwargs\n    ):\n        if self.axial_dim == 1:\n            return self.fn(x, *args, **kwargs)\n\n        seq_len, axial_dim = x.shape[1], self.axial_dim\n\n        next_multiple = math.ceil(seq_len / axial_dim) * axial_dim\n        x = pad_at_dim(x, (0, next_multiple - seq_len), dim = 1)\n\n        x = rearrange(x, 'b (n axial_dim) ... -> (b axial_dim) n ...', axial_dim = axial_dim)\n\n        out = self.fn(x, *args, **kwargs)\n\n        (out, *rest_out), tree_spec = tree_flatten(out)\n\n        out = rearrange(out, '(b axial_dim) n ... -> b (n axial_dim) ...', axial_dim = axial_dim)\n\n        out = out[:, :seq_len]\n        out = tree_unflatten((out, *rest_out), tree_spec)\n\n        return out", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "Create a fold axially neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Write a PyTorch module named `LayerScale` that scales the output of a given function (`fn`) by a gamma factor. If `unit_offset` is `True`, the gamma factor is offset by 1.", "code": "class LayerScale(Module):\n    def __init__(\n        self,\n        fn: Module,\n        dim,\n        init_value = 0.,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.unit_offset = unit_offset\n\n        self.fn = fn\n        self.gamma = nn.Parameter(torch.zeros(dim))\n        nn.init.constant_(self.gamma, init_value - float(unit_offset))\n\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n\n        gamma = self.gamma + float(self.unit_offset)\n\n        if isinstance(out, Tensor):\n            return out * gamma\n\n        out, *rest = out\n        return out * gamma, *rest", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "synthetic", "metadata": {"original_prompt": "Create a layer scale neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch module that implements an adaptive layer scale.", "code": "class AdaptiveLayerScale(Module):\n    def __init__(\n        self,\n        fn: Module,\n        dim,\n        dim_condition = None,\n        init_bias_value = -2.\n    ):\n        super().__init__()\n        self.fn = fn\n\n        dim_condition = default(dim_condition, dim)\n        self.to_gamma = nn.Linear(dim_condition, dim)\n\n        nn.init.zeros_(self.to_gamma.weight)\n        nn.init.constant_(self.to_gamma.bias, init_bias_value)\n\n    def forward(self, x, *, condition, **kwargs):\n        if condition.ndim == 2:\n            condition = rearrange(condition, 'b d -> b 1 d')\n\n        out = self.fn(x, **kwargs)\n        gamma = self.to_gamma(condition).sigmoid()\n\n        if isinstance(out, Tensor):\n            return out * gamma\n\n        out, *rest = out\n        return out * gamma, *rest", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "synthetic", "metadata": {"original_prompt": "Create a adaptive layer scale neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "class ConcatCombine(Module):", "code": "class ConcatCombine(Module):\n    def __init__(self, dim, prev_layer_ind):\n        super().__init__()\n        self.prev_layer_ind = prev_layer_ind\n        self.combine = LinearNoBias(dim * 2, dim)\n\n    def forward(self, x, prev_layers: list[Tensor]):\n        skip = prev_layers[self.prev_layer_ind]\n        concatted_skip = cat((skip, x), dim = -1)\n        return self.combine(concatted_skip)", "test_code": "", "difficulty": "medium", "category": "other", "source": "synthetic", "metadata": {"original_prompt": "Create a concat combine neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a model architecture in PyTorch for a feedforward neural network with the following characteristics:", "code": "class FeedForward(Module):\n    def __init__(\n        self,\n        dim,\n        dim_out = None,\n        mult = 4,\n        glu = False,\n        glu_mult_bias = False,\n        swish = False,\n        relu_squared = False,\n        solu = False,\n        custom_activation = None,\n        post_act_ln = False,\n        dropout = 0.,\n        sublayer_dropout = 0.,\n        no_bias = False,\n        zero_init_output = False,\n    ):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n\n        assert at_most_one_of(relu_squared, solu)\n\n        if exists(custom_activation):\n            activation = deepcopy(custom_activation)\n        elif relu_squared:\n            activation = ReluSquared()\n        elif solu:\n            activation = SoLU(inner_dim)\n        elif swish:\n            activation = nn.SiLU()\n        else:\n            activation = nn.GELU()\n\n        if glu:\n            proj_in = GLU(dim, inner_dim, activation, mult_bias = glu_mult_bias)\n        else:\n            proj_in = nn.Sequential(\n                nn.Linear(dim, inner_dim, bias = not no_bias),\n                activation\n            )\n\n        proj_out = nn.Linear(inner_dim, dim_out, bias = not no_bias)\n\n        self.ff = Sequential(\n            proj_in,\n            LayerNorm(inner_dim) if post_act_ln else None,\n            nn.Dropout(dropout),\n            proj_out,\n            nn.Dropout(sublayer_dropout) if sublayer_dropout > 0. else None\n        )\n\n        # init last linear layer to 0\n\n        if zero_init_output:\n            init_zero_(proj_out)\n\n    def muon_parameters(self):\n        weights = []\n\n        for m in self.modules():\n            if not isinstance(m, nn.Linear):\n                continue\n\n            weights.append(m.weight)\n\n        return weights\n\n    def forward(\n        self,\n        x,\n        deep_embed = None\n    ):\n        out = self.ff(x)\n\n        if exists(deep_embed):\n            out = out * deep_embed\n\n        return out", "test_code": "", "difficulty": "medium", "category": "mlp", "source": "synthetic", "metadata": {"original_prompt": "Create a feed forward neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "The code is an implementation of an attention pooling mechanism. The attention pooling mechanism is a technique used in natural language processing (NLP) to reduce the dimensionality of the output of a model. The idea is to create a pool of attention scores for each input token, which are then used to weight the input tokens in some way. The pooling operation can be done in various ways, such as taking the average, max, or min of the attention scores, or it can", "code": "class AttentionPool(Module):\n    def __init__(\n        self,\n        dim,\n        num_pooled_tokens = 1,\n        dim_context = None,\n        add_residual = False,\n        depth = 1,\n        heads = 8,\n        dim_head = 64,\n        use_transformer_blocks = None,\n        squeeze_output = None,\n        attn_kwargs: dict = dict()\n    ):\n        super().__init__()\n        dim_context = default(dim_context, dim)\n\n        squeeze_output = default(squeeze_output, False)\n        assert not (squeeze_output and num_pooled_tokens > 1)\n\n        use_transformer_blocks = default(use_transformer_blocks, depth > 1)\n        assert use_transformer_blocks or depth == 1\n\n        self.queries = nn.Parameter(torch.randn(num_pooled_tokens, dim) * 1e-2)\n\n        if use_transformer_blocks:\n            assert not add_residual, 'residual already in effect when doing a full cross attention based transformer for pooling'\n            attn_kwargs = {f'attn_{k}': v for k, v in attn_kwargs.items()}\n\n            self.pooler = CrossAttender(dim = dim, cross_attn_dim_context = dim_context, depth = depth, heads = heads, attn_dim_head = dim_head, )\n        else:\n            self.pooler = Attention(dim = dim, dim_context = dim_context, heads = heads, dim_head = dim_head, **attn_kwargs)\n\n        self.add_residual = add_residual\n        self.squeeze_output = squeeze_output\n\n    def forward(self, context, mask = None):\n        batch = context.shape[0]\n\n        queries = repeat(self.queries, 'n d -> b n d', b = batch)\n\n        pooled = self.pooler(queries, context, context_mask = mask)\n\n        if self.add_residual:\n            pooled = pooled + queries\n\n        if self.squeeze_output:\n            pooled = rearrange(pooled, 'b 1 d -> b d')\n\n        return pooled", "test_code": "", "difficulty": "medium", "category": "attention", "source": "synthetic", "metadata": {"original_prompt": "Create a attention pool neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
{"prompt": "Build a PyTorch model for a Transformer-based language model, given the parameters and classes in the code.", "code": "class XTransformer(Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        tie_token_emb = False,\n        ignore_index = -100,\n        pad_value = 0,\n        cross_attn_tokens_dropout = 0.,\n        **kwargs\n    ):\n        super().__init__()\n        enc_kwargs, kwargs = groupby_prefix_and_trim('enc_', kwargs)\n        dec_kwargs, kwargs = groupby_prefix_and_trim('dec_', kwargs)\n\n        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs, 'dimension of either encoder or decoder must be set with `dim` keyword'\n        enc_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], enc_kwargs)\n        enc_transformer_kwargs['emb_dropout'] = enc_kwargs.pop('emb_dropout', 0)\n        enc_transformer_kwargs['num_memory_tokens'] = enc_kwargs.pop('num_memory_tokens', None)\n        enc_transformer_kwargs['scaled_sinu_pos_emb'] = enc_kwargs.pop('scaled_sinu_pos_emb', False)\n        enc_transformer_kwargs['use_abs_pos_emb'] = enc_kwargs.pop('use_abs_pos_emb', True)\n\n        dec_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], dec_kwargs)\n        dec_transformer_kwargs['emb_dropout'] = dec_kwargs.pop('emb_dropout', 0)\n        dec_transformer_kwargs['scaled_sinu_pos_emb'] = dec_kwargs.pop('scaled_sinu_pos_emb', False)\n        dec_transformer_kwargs['use_abs_pos_emb'] = dec_kwargs.pop('use_abs_pos_emb', True)\n\n        self.cross_attn_tokens_dropout = cross_attn_tokens_dropout  # how many tokens from the encoder to dropout when cross attending from decoder - seen in a couple papers, including Perceiver AR - this will also be very effective regularization when cross attending to very long memories\n\n        self.encoder = TransformerWrapper(\n            **enc_transformer_kwargs,\n            return_only_embed = True,\n            attn_layers = Encoder(dim = dim, **enc_kwargs)\n        )\n\n        self.decoder = TransformerWrapper(\n            **dec_transformer_kwargs,\n            attn_layers = Decoder(dim = dim, cross_attend = True, **dec_kwargs)\n        )\n\n        if tie_token_emb:\n            self.decoder.token_emb = self.encoder.token_emb\n\n        self.decoder = AutoregressiveWrapper(self.decoder, ignore_index=ignore_index, pad_value=pad_value)\n\n    @torch.no_grad()\n    def generate(self, seq_in, seq_out_start, seq_len, mask = None, attn_mask = None, **kwargs):\n        encodings = self.encoder(seq_in, mask = mask, attn_mask = attn_mask, return_embeddings = True)\n        return self.decoder.generate(seq_out_start, seq_len, context = encodings, context_mask = mask, **kwargs)\n\n    def forward(self, src, tgt, mask = None, attn_mask = None, src_prepend_embeds = None):\n\n        enc = self.encoder(src, mask = mask, attn_mask = attn_mask, prepend_embeds = src_prepend_embeds, return_embeddings = True)\n\n        if exists(src_prepend_embeds) and exists(mask):\n            mask = pad_at_dim(mask, (src_prepend_embeds.shape[-2], 0), dim = -1, value = True)\n\n        if self.training and self.cross_attn_tokens_dropout > 0:\n            enc, mask = dropout_seq(enc, mask, self.cross_attn_tokens_dropout)\n\n        out = self.decoder(tgt, context = enc, context_mask = mask)\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "synthetic", "metadata": {"original_prompt": "Create a x transformer neural network module", "original_source": "github:lucidrains/x-transformers", "validation": {"code_type": "nn_module", "parses": true, "runs": true, "reward": 0.30000000000000004, "breakdown": {"type": "nn_module", "parses": 0.1, "instantiates": 0.2, "forward": 0, "backward": 0, "stable": 0}, "error": null, "instantiates": true, "forward_works": false, "backward_works": false, "trains_stable": false, "callable": false, "returns_value": false, "tests_pass": true}}}
