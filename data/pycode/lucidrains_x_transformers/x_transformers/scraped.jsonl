{"prompt": "Create a so lu neural network module", "code": "class SoLU(Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.norm = LayerNorm(dim)\n\n    def forward(self, x):\n        activated = x.softmax(dim = -1) * x\n        return self.norm(activated)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "SoLU", "line": 293}}
{"prompt": "Create a token embedding neural network module", "code": "class TokenEmbedding(Module):\n    def __init__(self, dim, num_tokens, l2norm_embed = False):\n        super().__init__()\n        self.l2norm_embed = l2norm_embed\n        self.emb = nn.Embedding(num_tokens, dim)\n\n    def forward(self, x):\n        token_emb = self.emb(x.long())\n        return l2norm(token_emb) if self.l2norm_embed else token_emb\n\n    def init_(self):\n        if self.l2norm_embed:\n            nn.init.normal_(self.emb.weight, std=1e-5)\n            return\n        nn.init.kaiming_normal_(self.emb.weight)", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "TokenEmbedding", "line": 304}}
{"prompt": "Create a absolute positional embedding neural network module", "code": "class AbsolutePositionalEmbedding(Module):\n    def __init__(self, dim, max_seq_len, l2norm_embed = False):\n        super().__init__()\n        self.scale = dim ** -0.5 if not l2norm_embed else 1.\n        self.max_seq_len = max_seq_len\n        self.l2norm_embed = l2norm_embed\n        self.emb = nn.Embedding(max_seq_len, dim)\n\n    def forward(\n        self,\n        x,\n        pos = None,\n        seq_start_pos = None,\n        offset = 0\n    ):\n        seq_len, device = x.shape[1], x.device\n        assert seq_len <= self.max_seq_len, f'you are passing in a sequence length of {seq_len} but your absolute positional embedding has a max sequence length of {self.max_seq_len}'\n\n        if not exists(pos):\n            pos = arange(seq_len, device = device) + offset\n\n        if exists(seq_start_pos):\n            pos = (pos - seq_start_pos[..., None]).clamp(min = 0)\n\n        pos_emb = self.emb(pos)\n        pos_emb = pos_emb * self.scale\n        return l2norm(pos_emb) if self.l2norm_embed else pos_emb", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AbsolutePositionalEmbedding", "line": 322}}
{"prompt": "Create a scaled sinusoidal embedding neural network module", "code": "class ScaledSinusoidalEmbedding(Module):\n    def __init__(self, dim, theta = 10000):\n        super().__init__()\n        assert divisible_by(dim, 2)\n        self.scale = nn.Parameter(torch.ones(1) * dim ** -0.5)\n\n        half_dim = dim // 2\n        freq_seq = arange(half_dim).float() / half_dim\n        inv_freq = theta ** -freq_seq\n        self.register_buffer('inv_freq', inv_freq, persistent = False)\n\n    def forward(\n        self,\n        x,\n        pos = None,\n        seq_start_pos = None,\n        offset = 0\n    ):\n        seq_len, device = x.shape[1], x.device\n\n        if not exists(pos):\n            pos = arange(seq_len, device = device) + offset\n\n        if exists(seq_start_pos):\n            pos = pos - seq_start_pos[..., None]\n\n        emb = einsum('i, j -> i j', pos, self.inv_freq)\n        emb = cat((emb.sin(), emb.cos()), dim = -1)\n        return emb * self.scale", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ScaledSinusoidalEmbedding", "line": 350}}
{"prompt": "Create a relative position bias neural network module", "code": "class RelativePositionBias(Module):\n    def __init__(self, scale, causal = False, num_buckets = 32, max_distance = 128, heads = 8):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, causal = True, num_buckets = 32, max_distance = 128):\n        ret = 0\n        n = -relative_position\n        if not causal:\n            num_buckets //= 2\n            ret += (n < 0).long() * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = max_exact + (\n            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n        ).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, i, j):\n        device = self.device\n        q_pos = arange(j - i, j, dtype = torch.long, device = device)\n        k_pos = arange(j, dtype = torch.long, device = device)\n        rel_pos = einx.subtract('j, i -> i j', k_pos, q_pos)\n        rp_bucket = self._relative_position_bucket(rel_pos, causal = self.causal, num_buckets = self.num_buckets, max_distance = self.max_distance)\n        values = self.relative_attention_bias(rp_bucket)\n        bias = rearrange(values, 'i j h -> h i j')\n        return bias * self.scale", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "RelativePositionBias", "line": 380}}
{"prompt": "Appendix B of https://arxiv.org/abs/2405.18719", "code": "class CoPE(Module):\n    \"\"\"\n    Appendix B of https://arxiv.org/abs/2405.18719\n    \"\"\"\n    def __init__ (\n        self,\n        dim,\n        heads,\n        max_pos,\n        soft_onehot = False,\n        talking_heads = False,\n        soft_onehot_temp = 5e-2\n    ):\n        super () . __init__ ()\n        self.max_pos = max_pos\n        self.pos_emb = nn.Parameter(torch.zeros(max_pos, dim))\n\n        self.talking_heads = nn.Conv2d(heads, heads, 1, bias = False) if talking_heads else None\n        self.soft_onehot = soft_onehot\n        self.soft_onehot_temp = soft_onehot_temp\n\n        if not soft_onehot:\n            return\n\n        self.register_buffer('positions', arange(max_pos))\n\n    def forward(self, query, attn_logits):\n\n        if exists(self.talking_heads):\n            i, j = attn_logits.shape[-2:]\n            causal_mask = attn_logits.new_ones(i, j).triu_(j - i + 1).bool()\n\n            attn_logits = self.talking_heads(attn_logits)\n\n            attn_logits = attn_logits.masked_fill(causal_mask, -torch.finfo(attn_logits.dtype).max)\n\n        # compute positions\n\n        gates = attn_logits.sigmoid()\n\n        pos = gates.flip(-1).cumsum(dim = -1).flip(-1)\n        pos = pos.clamp(max = self.max_pos - 1)\n\n        logits_int = einsum('b h n d, p d -> b h n p', query, self.pos_emb)\n\n        if self.soft_onehot:\n            diff_pos = einx.subtract('i, j -> i j', pos, self.positions).abs()\n            soft_onehot_pos = F.softmax(-diff_pos / self.soft_onehot_temp, dim = -1)\n            cope_pos_emb = einsum('b h i j p, b h i p -> b h i j', soft_onehot_pos, logits_int)\n        else:\n            # interpolate from integer positions\n            pos_ceil = pos.ceil().long()\n            pos_floor = pos.floor().long()\n            logits_ceil = logits_int.gather(-1, pos_ceil)\n            logits_floor = logits_int.gather(-1, pos_floor)\n\n            w = pos - pos_floor\n            cope_pos_emb = logits_ceil * w + logits_floor * (1 - w)\n\n        return cope_pos_emb", "test_code": "", "difficulty": "medium", "category": "cnn", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "CoPE", "line": 425}}
{"prompt": "Create a dynamic position bias neural network module", "code": "class DynamicPositionBias(Module):\n    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n        super().__init__()\n        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n        self.log_distance = log_distance\n\n        self.mlp = ModuleList([])\n\n        self.mlp.append(Sequential(\n            nn.Linear(1, dim),\n            LayerNorm(dim) if norm else None,\n            nn.SiLU()\n        ))\n\n        for _ in range(depth - 1):\n            self.mlp.append(Sequential(\n                nn.Linear(dim, dim),\n                nn.LayerNorm(dim) if norm else None,\n                nn.SiLU()\n            ))\n\n        self.mlp.append(nn.Linear(dim, heads))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, i, j):\n        n, device = j, self.device\n\n        # get the (n x n) matrix of distances\n        seq_arange = arange(j - i, j, device = device)\n        context_arange = arange(j, device = device)\n        indices = einx.subtract('i, j -> i j', seq_arange, context_arange)\n        indices += (j - 1)\n\n        # input to continuous positions MLP\n        pos = arange(-j + 1, j, device = device).float()\n        pos = rearrange(pos, '... -> ... 1')\n\n        if self.log_distance:\n            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n\n        for layer in self.mlp:\n            pos = layer(pos)\n\n        # get position biases        \n        bias = pos[indices]\n        bias = rearrange(bias, 'i j h -> h i j')\n        return bias", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "DynamicPositionBias", "line": 486}}
{"prompt": "Create a alibi positional bias neural network module", "code": "class AlibiPositionalBias(Module):\n    def __init__(\n        self,\n        heads,\n        total_heads = None,\n        slopes: list[int] | None = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.heads = heads\n        self.total_heads = default(total_heads, heads)\n\n        slopes = Tensor(default(slopes, self._get_slopes(heads)))\n        slopes = rearrange(slopes, 'h -> h 1 1')\n\n        self.register_buffer('slopes', slopes, persistent = False)\n        self.register_buffer('bias', None, persistent = False)\n    \n    @property\n    def device(self):\n        return next(self.buffers()).device\n\n    @staticmethod\n    def _get_slopes(heads):\n        def get_slopes_power_of_2(n):\n            start = (2**(-2**-(math.log2(n)-3)))\n            ratio = start\n            return [start*ratio**i for i in range(n)]\n\n        if math.log2(heads).is_integer():\n            return get_slopes_power_of_2(heads)\n\n        closest_power_of_2 = 2 ** math.floor(math.log2(heads))\n        return get_slopes_power_of_2(closest_power_of_2) + get_slopes_power_of_2(2 * closest_power_of_2)[0::2][:heads-closest_power_of_2]\n\n    def forward_custom_pos(\n        self,\n        pos_i: Tensor,\n        pos_j: Tensor | None = None\n    ):\n        h, device = self.total_heads, self.device\n\n        pos_j = default(pos_j, pos_i)\n        bias = -einx.subtract('... j, ... i -> ... i j', pos_j, pos_i).abs()\n\n        if bias.ndim == 3:\n            bias = rearrange(bias, 'b i j -> b 1 i j')\n\n        bias = bias * self.slopes\n        num_heads_unalibied = h - bias.shape[-3]\n        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim = -3)\n\n        return bias\n\n    def forward(self, i, j):\n        h, device = self.total_heads, self.device\n\n        if exists(self.bias) and self.bias.shape[-1] >= j and self.bias.shape[-2] >= i:\n            return self.bias[..., -i:, -j:]\n\n        seq_arange = arange(j - i, j, device = device)\n        context_arange = arange(j, device = device)\n        bias = -einx.subtract('j, i -> 1 i j', context_arange, seq_arange).abs()\n\n        bias = bias * self.slopes\n        num_heads_unalibied = h - bias.shape[-3]\n        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim = -3)\n\n        self.register_buffer('bias', bias, persistent = False)\n        return self.bias", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AlibiPositionalBias", "line": 537}}
{"prompt": "https://openreview.net/forum?id=q2Lnyegkr8", "code": "class DataDependentAlibi(Module):\n    \"\"\" https://openreview.net/forum?id=q2Lnyegkr8 \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        heads,\n        causal = True,\n        bias_init = 5.,\n        post_log_scale = 1.,\n    ):\n        super().__init__()\n\n        self.causal = causal\n\n        linear = nn.Linear(dim, heads * (1 if causal else 2))\n\n        self.to_forget_gates = nn.Sequential(\n            linear,\n            Rearrange('b n h -> b h n'),\n            nn.LogSigmoid()\n        )\n\n        nn.init.constant_(linear.bias, bias_init)\n        self.post_log_scale = post_log_scale\n\n    def forward(self, x):\n        bidirectional = not self.causal\n\n        forget_gates = self.to_forget_gates(x) * self.post_log_scale\n\n        forget_gates = forget_gates.cumsum(dim = -1)\n\n        if bidirectional:\n            forget_gates, forget_gates_reversed = forget_gates.chunk(2, dim = 1)\n\n        forget_gates = einx.subtract('b h i, b h j -> b h i j', forget_gates, forget_gates)\n\n        if bidirectional:\n            forget_gates_reversed = einx.subtract('b h j, b h i -> b h i j', forget_gates_reversed, forget_gates_reversed)\n            forget_gates = forget_gates.tril() + forget_gates_reversed.triu()\n\n        return forget_gates", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "DataDependentAlibi", "line": 608}}
{"prompt": "same as data dependent alibi from forgetting transformer, but the forgetting gates are also derived by a queries and keys with a small head dimension", "code": "class PerRowDataDependentAlibi(Module):\n    \"\"\" same as data dependent alibi from forgetting transformer, but the forgetting gates are also derived by a queries and keys with a small head dimension \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        heads,\n        causal = True,\n        dim_head = 8,\n        post_log_scale = 1.\n    ):\n        super().__init__()\n        assert causal, 'bidirectional not supported yet'\n\n        self.scale = dim_head ** -0.5\n\n        linear = nn.Linear(dim, heads * dim_head * 2, bias = False)\n\n        self.to_forget_gates = nn.Sequential(\n            linear,\n            Rearrange('b n (qk h d) -> qk b h n d', qk = 2, d = dim_head)\n        )\n\n        self.post_log_scale = post_log_scale\n\n    def forward(self, x):\n        q, k = self.to_forget_gates(x)\n        forget_gates = einsum('... i d, ... j d -> ... i j', q, k) * self.scale\n\n        forget_gates = F.logsigmoid(forget_gates) * self.post_log_scale\n\n        # mask out upper triangle + diagonal\n\n        n = x.shape[-2]\n        causal_mask = torch.ones((n, n), dtype = torch.bool, device = x.device).triu()\n\n        forget_gates = forget_gates.masked_fill(causal_mask, 0.)\n\n        # reverse cumsum\n\n        forget_gates = forget_gates.flip(dims = (-1,))\n        forget_gates = forget_gates.cumsum(dim = -1)\n        forget_gates = forget_gates.flip(dims = (-1,))\n\n        return forget_gates", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "PerRowDataDependentAlibi", "line": 652}}
{"prompt": "Create a rotary embedding neural network module", "code": "class RotaryEmbedding(Module):\n    def __init__(\n        self,\n        dim,\n        use_xpos = False,\n        scale_base = 512,\n        interpolation_factor = 1.,\n        base = 10000,\n        base_rescale_factor = 1.\n    ):\n        super().__init__()\n        # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n        # has some connection to NTK literature\n        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n        base *= base_rescale_factor ** (dim / (dim - 2))\n\n        inv_freq = 1. / (base ** (arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n        assert interpolation_factor >= 1.\n        self.interpolation_factor = interpolation_factor\n\n        if not use_xpos:\n            self.register_buffer('scale', None)\n            return\n\n        scale = (arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n\n        self.scale_base = scale_base\n        self.register_buffer('scale', scale)\n\n    def forward_from_seq_len(self, seq_len):\n        device = self.inv_freq.device\n\n        t = arange(seq_len, device = device)\n        return self.forward(t)\n\n    @autocast('cuda', enabled = False)\n    def forward(self, t, offset = 0):\n        max_pos = t.max() + 1\n\n        if t.ndim == 1:\n            t = rearrange(t, 'n -> 1 n')\n\n        freqs = torch.einsum('b i , j -> b i j', t.type_as(self.inv_freq), self.inv_freq) / self.interpolation_factor\n        freqs = stack((freqs, freqs), dim = -1)\n        freqs = rearrange(freqs, '... d r -> ... (d r)')\n\n        if not exists(self.scale):\n            return freqs, 1.\n\n        power = (t - (max_pos // 2)) / self.scale_base\n        scale = self.scale ** rearrange(power, '... n -> ... n 1')\n        scale = stack((scale, scale), dim = -1)\n        scale = rearrange(scale, '... d r -> ... (d r)')\n\n        return freqs, scale", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "RotaryEmbedding", "line": 698}}
{"prompt": "Create a scale neural network module", "code": "class Scale(Module):\n    def __init__(self, value, fn):\n        super().__init__()\n        self.value = value\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n        scale_fn = lambda t: t * self.value\n\n        if not isinstance(out, tuple):\n            return scale_fn(out)\n\n        return (scale_fn(out[0]), *out[1:])", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "Scale", "line": 784}}
{"prompt": "Create a layer norm neural network module", "code": "class LayerNorm(Module):\n    def __init__(\n        self,\n        dim,\n        unit_offset = False\n    ):\n        \"\"\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \"\"\"\n        super().__init__()\n        self.unit_offset = unit_offset\n\n        self.ln = nn.LayerNorm(dim, elementwise_affine = False)\n        self.gamma = nn.Parameter(torch.ones(dim))\n        nn.init.constant_(self.gamma, 1. - float(unit_offset))\n\n    def forward(self, x):\n        normed = self.ln(x)\n        gamma = self.gamma + float(self.unit_offset)\n        return normed * gamma", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "LayerNorm", "line": 799}}
{"prompt": "Create a adaptive layer norm neural network module", "code": "class AdaptiveLayerNorm(Module):\n    def __init__(\n        self,\n        dim,\n        dim_condition = None\n    ):\n        super().__init__()\n        dim_condition = default(dim_condition, dim)\n\n        self.ln = nn.LayerNorm(dim, elementwise_affine = False)\n        self.to_gamma = LinearNoBias(dim_condition, dim)\n        nn.init.zeros_(self.to_gamma.weight)\n\n    def forward(self, x, *, condition):\n        if condition.ndim == 2:\n            condition = rearrange(condition, 'b d -> b 1 d')\n\n        normed = self.ln(x)\n        gamma = self.to_gamma(condition)\n        return normed * (gamma + 1.)", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AdaptiveLayerNorm", "line": 820}}
{"prompt": "Create a scale norm neural network module", "code": "class ScaleNorm(Module):\n    def __init__(\n        self,\n        dim,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.unit_offset = unit_offset\n        self.scale = dim ** 0.5\n\n        self.g = nn.Parameter(torch.zeros(1))\n        nn.init.constant_(self.g, 1. - float(unit_offset))\n\n    def forward(self, x):\n        gamma = self.g + float(self.unit_offset)\n        return F.normalize(x, dim = -1) * self.scale * gamma", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ScaleNorm", "line": 841}}
{"prompt": "Create a rms norm neural network module", "code": "class RMSNorm(Module):\n    def __init__(\n        self,\n        dim,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.unit_offset = unit_offset\n        self.scale = dim ** 0.5\n\n        self.g = nn.Parameter(torch.zeros(dim))\n        nn.init.constant_(self.g, 1. - float(unit_offset))\n\n    def forward(self, x):\n        gamma = self.g + float(self.unit_offset)\n        return F.normalize(x, dim = -1) * self.scale * gamma", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "RMSNorm", "line": 858}}
{"prompt": "Create a adaptive rms norm neural network module", "code": "class AdaptiveRMSNorm(Module):\n    def __init__(\n        self,\n        dim,\n        dim_condition = None\n    ):\n        super().__init__()\n        self.scale = dim ** 0.5\n        dim_condition = default(dim_condition, dim)\n\n        self.to_gamma = LinearNoBias(dim_condition, dim)\n        nn.init.zeros_(self.to_gamma.weight)\n\n    def forward(self, x, *, condition):\n        if condition.ndim == 2:\n            condition = rearrange(condition, 'b d -> b 1 d')\n\n        normed = F.normalize(x, dim = -1)\n        gamma = self.to_gamma(condition)\n        return normed * self.scale * (gamma + 1.)", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AdaptiveRMSNorm", "line": 875}}
{"prompt": "Create a simple rms norm neural network module", "code": "class SimpleRMSNorm(Module):\n    def __init__(\n        self,\n        dim,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim ** 0.5\n\n    def forward(self, x):\n        return F.normalize(x, dim = -1) * self.scale", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "SimpleRMSNorm", "line": 896}}
{"prompt": "Create a multihead rms norm neural network module", "code": "class MultiheadRMSNorm(Module):\n    def __init__(self, dim, heads):\n        super().__init__()\n        self.rmsnorm = SimpleRMSNorm(dim)\n        self.gamma = nn.Parameter(torch.zeros(heads, 1, dim))\n\n    def forward(self, x):\n        return self.rmsnorm(x) * (self.gamma + 1.)", "test_code": "", "difficulty": "medium", "category": "attention", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "MultiheadRMSNorm", "line": 908}}
{"prompt": "https://arxiv.org/abs/2503.10622", "code": "class DynamicTanh(Module):\n    \"\"\" https://arxiv.org/abs/2503.10622 \"\"\"\n    def __init__(\n        self,\n        dim,\n        init_alpha = 1.,\n        gamma = 1.,\n        beta = 0.,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.pre_tanh_scale = nn.Parameter(tensor(init_alpha))\n\n        self.gamma = nn.Parameter(torch.ones(dim))\n        self.beta = nn.Parameter(torch.zeros(dim))\n\n        self.pre_tanh_scale_offset = init_alpha if unit_offset else 0.\n        self.gamma_offset = float(unit_offset)\n\n        nn.init.constant_(self.pre_tanh_scale, 0 if unit_offset else init_alpha)\n        nn.init.constant_(self.gamma, 1. - float(unit_offset))\n\n    def forward(self, x):\n        pre_tanh_scale = self.pre_tanh_scale + self.pre_tanh_scale_offset\n        gamma = self.gamma + self.gamma_offset\n        return (x * pre_tanh_scale).tanh() * gamma + self.beta", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "DynamicTanh", "line": 917}}
{"prompt": "Create a residual neural network module", "code": "class Residual(Module):\n    def __init__(self, dim, scale_residual = False, scale_residual_constant = 1., **kwargs):\n        super().__init__()\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n        self.scale_residual_constant = scale_residual_constant\n\n    def prepare(self, residual):\n        return residual, residual, dict()\n\n    def forward(self, x, residual, **kwargs):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n\n        if self.scale_residual_constant != 1:\n            residual = residual * self.scale_residual_constant\n\n        return x + residual", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "Residual", "line": 946}}
{"prompt": "Create a gru gating neural network module", "code": "class GRUGating(Module):\n    def __init__(self, dim, scale_residual = False, **kwargs):\n        super().__init__()\n        self.gru = nn.GRUCell(dim, dim)\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n\n    def prepare(self, residual):\n        return residual, residual, dict()\n\n    def forward(self, x, residual, **kwargs):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n\n        gated_output = self.gru(\n            rearrange(x, 'b n d -> (b n) d'),\n            rearrange(residual, 'b n d -> (b n) d')\n        )\n\n        return gated_output.reshape_as(x)", "test_code": "", "difficulty": "medium", "category": "rnn", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "GRUGating", "line": 964}}
{"prompt": "Create a hyper connection neural network module", "code": "class HyperConnection(Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        layer_index,\n        num_residual_streams,\n        num_input_views = 1,\n        tanh = True,\n        **kwargs\n    ):\n        \"\"\"\n        https://arxiv.org/abs/2409.19606\n        Appendix J - Algorithm 2, Dynamic only\n        \"\"\"\n        super().__init__()\n\n        self.act = nn.Tanh() if tanh else nn.Identity()\n\n        self.norm = nn.LayerNorm(dim, bias = False)\n\n        self.num_residual_streams = num_residual_streams\n        self.layer_index = layer_index\n\n        self.static_beta = nn.Parameter(torch.ones(num_residual_streams))\n\n        init_alpha0 = torch.zeros((num_residual_streams, num_input_views))\n        init_alpha0[layer_index % num_residual_streams, :] = 1.\n\n        self.static_alpha = nn.Parameter(cat([init_alpha0, torch.eye(num_residual_streams)], dim = 1))\n\n        self.dynamic_alpha_fn = nn.Parameter(torch.zeros(dim, num_residual_streams + num_input_views))\n        self.dynamic_alpha_scale = nn.Parameter(torch.ones(()) * 1e-2)\n\n        self.num_input_views = num_input_views\n\n        self.dynamic_beta_fn = nn.Parameter(torch.zeros(dim))\n        self.dynamic_beta_scale = nn.Parameter(torch.ones(()) * 1e-2)\n\n    def prepare(self, residuals):\n\n        residuals = rearrange(residuals, '(b s) n d -> b n s d', s = self.num_residual_streams)\n\n        normed = self.norm(residuals)\n\n        wc_weight = self.act(normed @ self.dynamic_alpha_fn)\n        dynamic_alpha = wc_weight * self.dynamic_alpha_scale\n        alpha = dynamic_alpha + self.static_alpha\n\n        dc_weight = self.act(normed @ self.dynamic_beta_fn)\n        dynamic_beta = dc_weight * self.dynamic_beta_scale\n        beta = dynamic_beta + self.static_beta\n\n        # width connection\n\n        mix_h = einsum('... s t, ... s d -> ... t d', alpha, residuals)\n\n        views = self.num_input_views\n\n        if views == 1:\n            branch_input, residuals = mix_h[..., 0, :], mix_h[..., 1:, :]\n        else:\n            branch_input, residuals = mix_h[..., :views, :], mix_h[..., views:, :]\n            branch_input = rearrange(branch_input, '... v d -> v ... d')\n\n        return branch_input, residuals, dict(beta = beta)\n\n    def forward(self, x, residuals, *, beta):\n        residuals = einsum('b n d, b n s -> b n s d', x, beta) + residuals\n        return rearrange(residuals, 'b n s d -> (b s) n d')", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "HyperConnection", "line": 986}}
{"prompt": "Create a dynamic li me neural network module", "code": "class DynamicLIMe(Module):\n    def __init__(\n        self,\n        dim,\n        num_layers,\n        num_views = 1,\n        norm = True,\n        use_softmax = True\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.multiple_views = num_views > 1\n\n        self.to_weights = Sequential(\n            RMSNorm(dim) if norm else None,\n            nn.Linear(dim, num_views * num_layers),\n            Rearrange('... (views layers) -> views ... layers', views = num_views),\n            nn.Softmax(dim = -1) if use_softmax else nn.ReLU()\n        )\n\n    def forward(\n        self,\n        x,\n        hiddens\n    ):\n\n        if not is_tensor(hiddens):\n            hiddens = stack(hiddens)\n\n        assert hiddens.shape[0] == self.num_layers, f'expected hiddens to have {self.num_layers} layers but received {tuple(hiddens.shape)} instead (first dimension must be layers)'\n\n        weights = self.to_weights(x)\n\n        out = einsum('l b n d, v b n l -> v b n d', hiddens, weights)\n\n        if self.multiple_views:\n            return out\n\n        return rearrange(out, '1 ... -> ...')", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "DynamicLIMe", "line": 1059}}
{"prompt": "Create a shift tokens neural network module", "code": "class ShiftTokens(Module):\n    def __init__(self, shifts, fn):\n        super().__init__()\n        self.fn = fn\n        self.shifts = tuple(shifts)\n\n    def forward(self, x, **kwargs):\n        mask = kwargs.get('mask', None)\n        shifts = self.shifts\n        segments = len(shifts)\n        feats_per_shift = x.shape[-1] // segments\n        splitted = x.split(feats_per_shift, dim = -1)\n        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n        segments_to_shift = [shift(*args, mask = mask) for args in zip(segments_to_shift, shifts)]\n        x = cat((*segments_to_shift, *rest), dim = -1)\n        return self.fn(x, **kwargs)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ShiftTokens", "line": 1112}}
{"prompt": "Create a fold axially neural network module", "code": "class FoldAxially(Module):\n    def __init__(\n        self,\n        axial_dim,\n        fn: Module\n    ):\n        super().__init__()\n        self.fn = fn\n        self.axial_dim = axial_dim # will fold the sequence as rearrange(\"b (n axial_dim) ... -> (b axial_dim) n ...\")\n\n    def forward(\n        self,\n        x,\n        *args,\n        **kwargs\n    ):\n        if self.axial_dim == 1:\n            return self.fn(x, *args, **kwargs)\n\n        seq_len, axial_dim = x.shape[1], self.axial_dim\n\n        next_multiple = math.ceil(seq_len / axial_dim) * axial_dim\n        x = pad_at_dim(x, (0, next_multiple - seq_len), dim = 1)\n\n        x = rearrange(x, 'b (n axial_dim) ... -> (b axial_dim) n ...', axial_dim = axial_dim)\n\n        out = self.fn(x, *args, **kwargs)\n\n        (out, *rest_out), tree_spec = tree_flatten(out)\n\n        out = rearrange(out, '(b axial_dim) n ... -> b (n axial_dim) ...', axial_dim = axial_dim)\n\n        out = out[:, :seq_len]\n        out = tree_unflatten((out, *rest_out), tree_spec)\n\n        return out", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "FoldAxially", "line": 1129}}
{"prompt": "Create a layer scale neural network module", "code": "class LayerScale(Module):\n    def __init__(\n        self,\n        fn: Module,\n        dim,\n        init_value = 0.,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.unit_offset = unit_offset\n\n        self.fn = fn\n        self.gamma = nn.Parameter(torch.zeros(dim))\n        nn.init.constant_(self.gamma, init_value - float(unit_offset))\n\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n\n        gamma = self.gamma + float(self.unit_offset)\n\n        if isinstance(out, Tensor):\n            return out * gamma\n\n        out, *rest = out\n        return out * gamma, *rest", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "LayerScale", "line": 1168}}
{"prompt": "Create a adaptive layer scale neural network module", "code": "class AdaptiveLayerScale(Module):\n    def __init__(\n        self,\n        fn: Module,\n        dim,\n        dim_condition = None,\n        init_bias_value = -2.\n    ):\n        super().__init__()\n        self.fn = fn\n\n        dim_condition = default(dim_condition, dim)\n        self.to_gamma = nn.Linear(dim_condition, dim)\n\n        nn.init.zeros_(self.to_gamma.weight)\n        nn.init.constant_(self.to_gamma.bias, init_bias_value)\n\n    def forward(self, x, *, condition, **kwargs):\n        if condition.ndim == 2:\n            condition = rearrange(condition, 'b d -> b 1 d')\n\n        out = self.fn(x, **kwargs)\n        gamma = self.to_gamma(condition).sigmoid()\n\n        if isinstance(out, Tensor):\n            return out * gamma\n\n        out, *rest = out\n        return out * gamma, *rest", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AdaptiveLayerScale", "line": 1194}}
{"prompt": "Create a concat combine neural network module", "code": "class ConcatCombine(Module):\n    def __init__(self, dim, prev_layer_ind):\n        super().__init__()\n        self.prev_layer_ind = prev_layer_ind\n        self.combine = LinearNoBias(dim * 2, dim)\n\n    def forward(self, x, prev_layers: list[Tensor]):\n        skip = prev_layers[self.prev_layer_ind]\n        concatted_skip = cat((skip, x), dim = -1)\n        return self.combine(concatted_skip)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ConcatCombine", "line": 1226}}
{"prompt": "Create a glu neural network module", "code": "class GLU(Module):\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        activation: Callable,\n        mult_bias = False\n    ):\n        super().__init__()\n        self.act = activation\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n        self.mult_bias = nn.Parameter(torch.ones(dim_out)) if mult_bias else 1.\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim = -1)\n        return x * self.act(gate) * self.mult_bias", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "GLU", "line": 1239}}
{"prompt": "Create a feed forward neural network module", "code": "class FeedForward(Module):\n    def __init__(\n        self,\n        dim,\n        dim_out = None,\n        mult = 4,\n        glu = False,\n        glu_mult_bias = False,\n        swish = False,\n        relu_squared = False,\n        solu = False,\n        custom_activation = None,\n        post_act_ln = False,\n        dropout = 0.,\n        sublayer_dropout = 0.,\n        no_bias = False,\n        zero_init_output = False,\n    ):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n\n        assert at_most_one_of(relu_squared, solu)\n\n        if exists(custom_activation):\n            activation = deepcopy(custom_activation)\n        elif relu_squared:\n            activation = ReluSquared()\n        elif solu:\n            activation = SoLU(inner_dim)\n        elif swish:\n            activation = nn.SiLU()\n        else:\n            activation = nn.GELU()\n\n        if glu:\n            proj_in = GLU(dim, inner_dim, activation, mult_bias = glu_mult_bias)\n        else:\n            proj_in = nn.Sequential(\n                nn.Linear(dim, inner_dim, bias = not no_bias),\n                activation\n            )\n\n        proj_out = nn.Linear(inner_dim, dim_out, bias = not no_bias)\n\n        self.ff = Sequential(\n            proj_in,\n            LayerNorm(inner_dim) if post_act_ln else None,\n            nn.Dropout(dropout),\n            proj_out,\n            nn.Dropout(sublayer_dropout) if sublayer_dropout > 0. else None\n        )\n\n        # init last linear layer to 0\n\n        if zero_init_output:\n            init_zero_(proj_out)\n\n    def muon_parameters(self):\n        weights = []\n\n        for m in self.modules():\n            if not isinstance(m, nn.Linear):\n                continue\n\n            weights.append(m.weight)\n\n        return weights\n\n    def forward(\n        self,\n        x,\n        deep_embed = None\n    ):\n        out = self.ff(x)\n\n        if exists(deep_embed):\n            out = out * deep_embed\n\n        return out", "test_code": "", "difficulty": "medium", "category": "mlp", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "FeedForward", "line": 1256}}
{"prompt": "Create a attention neural network module", "code": "class Attention(Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = DEFAULT_DIM_HEAD,\n        dim_context = None,\n        heads = 8,\n        causal = False,\n        flash = False,\n        pre_talking_heads = False,\n        post_talking_heads = False,\n        pre_scale_post_talking_heads = False,\n        head_scale = False,\n        sparse_topk = None,\n        sparse_topk_straight_through = False,\n        num_mem_kv = 0,\n        dropout = 0.,\n        sublayer_dropout = 0.,\n        on_attn = False,\n        gate_value_heads = False,\n        swiglu_values = False,\n        gate_values = False,\n        zero_init_output = False,\n        hard = False,\n        max_attend_past = None,\n        qk_norm = False,\n        qk_norm_groups = 1,\n        qk_norm_scale = 10,\n        qk_norm_dim_scale = False,\n        value_rmsnorm = False,      # used in alphagenome and bytedance's GR3 for further stability\n        l2_distance = False,\n        sigmoid = False,\n        gumbel_softmax = False,\n        gumbel_softmax_temp = 1.,\n        gumbel_softmax_hard = True,\n        selective = False,\n        cog_signed = False,\n        custom_attn_fn: Callable | None = None,\n        hybrid_module: Module | None = None,\n        hybrid_mask_kwarg: str | None = None,\n        hybrid_fold_axial_dim: int | None = None,\n        hybrid_learned_mix = False,\n        one_kv_head = False,\n        kv_heads = None,\n        value_dim_head = None,\n        dim_out = None,\n        add_zero_kv = False,         # same as add_zero_attn in pytorch\n        head_learned_sink = False,\n        rotate_num_heads = None,\n        data_dependent_alibi = False,\n        data_dependent_alibi_per_row = False,\n        data_dependent_alibi_per_row_dim_head = 8,\n        data_dependent_alibi_kwargs: dict = dict(),\n        use_cope = False,\n        cope_max_pos = 16,\n        cope_soft_onehot_pos = False,\n        cope_talking_heads = False,\n        softclamp_logits = False,\n        logit_softclamp_value = 50.,\n        learned_value_residual_mix = False,\n        orthog_projected_values = False,  # https://openreview.net/forum?id=Ard2QzPAUK\n        orthog_projected_values_per_head = False,\n        laser = False,                    # https://arxiv.org/abs/2411.03493v1\n        laser_softclamp_value = 15.,\n        qkv_receive_diff_residuals = False,\n        use_latent_q = False,\n        dim_latent_q = None,\n        use_latent_kv = False,\n        dim_latent_kv = None,\n        latent_rope_subheads = None,\n        onnxable = False,\n        attend_sdp_kwargs: dict = dict(\n            enable_flash = True,\n            enable_math = True,\n            enable_mem_efficient = True\n        )\n    ):\n        super().__init__()\n        dim_kv = default(dim_context, dim)\n\n        self.scale = dim_head ** -0.5\n\n        self.heads = heads\n        self.causal = causal\n        self.max_attend_past = max_attend_past\n\n        assert not (exists(kv_heads) and one_kv_head), 'either attn_one_kv_head is set to True (in which case kv_heads is set to 1), or attn_kv_heads is set, but not both'\n\n        value_dim_head = default(value_dim_head, dim_head)\n        kv_heads = default(kv_heads, heads)\n\n        kv_heads = 1 if one_kv_head else kv_heads\n        assert divisible_by(heads, kv_heads)\n\n        self.kv_heads = kv_heads\n        self.groups = heads // kv_heads\n\n        q_dim = dim_head * heads\n        k_dim = dim_head * kv_heads\n        v_dim = value_dim_head * kv_heads\n        out_dim = value_dim_head * heads\n\n        # determine input dimensions to qkv based on whether intermediate latent q and kv are being used\n        # for eventually supporting multi-latent attention (MLA)\n\n        self.to_latent_q = None\n        self.to_latent_kv = None\n        self.to_rotateable_k = None # for their \"decoupled rope\", subheads of keys that comes directly from base sequence (does not go through latents)\n\n        dim_q_input = dim\n        dim_kv_input = dim_kv\n\n        if use_latent_q:\n            assert exists(dim_latent_q)\n            self.to_latent_q = LinearNoBias(dim, dim_latent_q)\n            dim_q_input = dim_latent_q\n\n        if use_latent_kv:\n            assert exists(dim_latent_kv)\n            self.to_latent_kv = LinearNoBias(dim, dim_latent_kv)\n            dim_kv_input = dim_latent_kv\n\n        if exists(latent_rope_subheads):\n            assert not exists(rotate_num_heads), '`rotate_num_heads` cannot be set when multi-latent attention is being used'\n            rotate_num_heads = latent_rope_subheads\n\n            k_dim = dim_head * (kv_heads - latent_rope_subheads)\n\n            self.to_rotateable_k = LinearNoBias(dim, dim_head * latent_rope_subheads)\n            self.split_rotateable_k_heads = Rearrange('b n (h d) -> b h n d', h = latent_rope_subheads)\n\n        self.use_latent_q = use_latent_q\n        self.use_latent_kv = use_latent_kv\n\n        # query key projection\n\n        self.to_q = LinearNoBias(dim_q_input, q_dim)\n        self.to_k = LinearNoBias(dim_kv_input, k_dim)\n        self.to_v = LinearNoBias(dim_kv_input, v_dim)\n\n        # split and merge of attention heads\n\n        self.split_q_heads = Rearrange('b n (h d) -> b h n d', h = heads)\n        self.split_k_heads = Rearrange('b n (h d) -> b h n d', d = dim_head)\n        self.split_v_heads = Rearrange('b n (h d) -> b h n d', d = value_dim_head)\n\n        self.merge_heads = Rearrange('b h n d -> b n (h d)')\n\n        # whether qkv receives different residual stream combinations from hyper connections or lime\n\n        self.qkv_receive_diff_residuals = qkv_receive_diff_residuals\n\n        # enhancing gradients to attention through exponentiated values\n\n        self.laser = laser\n        self.laser_softclamp_value = laser_softclamp_value\n\n        # add GLU gating for aggregated values, from alphafold2\n\n        self.to_v_gate = None\n        if gate_values:\n            self.to_v_gate = nn.Linear(dim, out_dim)\n            self.to_v_gate_activation = F.silu if swiglu_values else F.sigmoid\n            nn.init.constant_(self.to_v_gate.weight, 0)\n            nn.init.constant_(self.to_v_gate.bias, 10)\n\n        # add per head gating of the output values, from 'Attend to nothing' paper\n\n        self.to_v_head_gate = None\n        if gate_value_heads:\n            self.to_v_head_gate = nn.Linear(dim, heads)\n            nn.init.constant_(self.to_v_head_gate.weight, 0)\n            nn.init.constant_(self.to_v_head_gate.bias, 10)\n\n        # cosine sim attention\n\n        self.qk_norm = qk_norm\n        self.qk_norm_groups = qk_norm_groups\n        self.qk_norm_scale = qk_norm_scale\n\n        # whether to use the rmsnorm (equivalent to cosine sim attention when scale is equal to 1) - https://arxiv.org/abs/2302.05442\n\n        self.qk_norm_dim_scale = qk_norm_dim_scale\n\n        self.qk_norm_q_scale = self.qk_norm_k_scale = 1\n        if qk_norm and qk_norm_dim_scale:\n            self.qk_norm_q_scale = nn.Parameter(torch.ones(heads, 1, dim_head))\n            self.qk_norm_k_scale = nn.Parameter(torch.ones(kv_heads, 1, dim_head))\n\n        assert (not qk_norm) or divisible_by(dim_head, qk_norm_groups), 'dimension per attention head must be divisible by the qk norm groups'\n        assert not (qk_norm and (dim_head // qk_norm_groups) <= 2), 'the group dimension may be too small (2 was too small in my tests, but 4 still works, surprisingly)'\n\n        # value rms norm\n\n        self.value_rmsnorm = MultiheadRMSNorm(dim_head, heads = heads) if value_rmsnorm else None\n\n        # contextual positional encoding\n        # https://arxiv.org/html/2405.18719v2\n\n        cope = None\n\n        if use_cope:\n            assert causal, 'CoPE was designed for causal attention'\n            assert not flash, 'CoPE is not flash attention compatible'\n\n            cope = CoPE(\n                dim = dim_head,\n                heads = heads,\n                max_pos = cope_max_pos,\n                talking_heads = cope_talking_heads,\n                soft_onehot = cope_soft_onehot_pos\n            )\n\n        # data dependent alibi\n        # https://openreview.net/forum?id=q2Lnyegkr8\n\n        self.data_dependent_alibi = None\n\n        if data_dependent_alibi:\n\n            dda_klass = DataDependentAlibi if not data_dependent_alibi_per_row else PerRowDataDependentAlibi\n            dda_kwargs = dict(dim = dim, heads = heads, causal = causal)\n\n            if data_dependent_alibi_per_row:\n                dda_kwargs.update(dim_head = data_dependent_alibi_per_row_dim_head)\n\n            self.data_dependent_alibi = dda_klass(**dda_kwargs, **data_dependent_alibi_kwargs)\n\n        # attend class - includes core attention algorithm + talking heads\n\n        self.attend = Attend(\n            heads = heads,\n            causal = causal,\n            pre_talking_heads = pre_talking_heads,\n            post_talking_heads = post_talking_heads,\n            pre_scale_post_talking_heads = pre_scale_post_talking_heads,\n            dropout = dropout,\n            sparse_topk = sparse_topk,\n            sparse_topk_straight_through = sparse_topk_straight_through,\n            hard = hard,\n            qk_norm = qk_norm,\n            scale = qk_norm_scale if qk_norm else self.scale,\n            l2_distance = l2_distance,\n            sigmoid = sigmoid,\n            gumbel_softmax = gumbel_softmax,\n            gumbel_softmax_temp = gumbel_softmax_temp,\n            gumbel_softmax_hard = gumbel_softmax_hard,\n            selective = selective,\n            cog_signed = cog_signed,\n            custom_attn_fn = custom_attn_fn,\n            add_zero_kv = add_zero_kv,\n            head_learned_sink = head_learned_sink,\n            flash = flash,\n            softclamp_logits = softclamp_logits,\n            logit_softclamp_value = logit_softclamp_value,\n            cope = cope,\n            onnxable = onnxable,\n            sdp_kwargs = attend_sdp_kwargs\n        )\n\n        # head scaling\n\n        self.head_scale = head_scale\n        if head_scale:\n            self.head_scale_params = nn.Parameter(torch.ones(1, heads, 1, 1))\n\n        # explicit topk sparse attention\n\n        self.sparse_topk = sparse_topk\n\n        # add memory key / values\n\n        self.num_mem_kv = num_mem_kv\n        if num_mem_kv > 0:\n            self.mem_k = nn.Parameter(torch.randn(kv_heads, num_mem_kv, dim_head))\n            self.mem_v = nn.Parameter(torch.randn(kv_heads, num_mem_kv, dim_head))\n\n        # maybe learned value residual mixer per token\n\n        self.to_value_residual_mix = nn.Sequential(\n            nn.Linear(dim, heads),\n            nn.Sigmoid(),\n            Rearrange('b n h -> b h n 1')\n         ) if learned_value_residual_mix else always(0.5)\n\n        # attention on attention\n\n        self.attn_on_attn = on_attn\n\n        # return orthogonal projected weighted values on original values\n        # \"belief attention\" - iclr 2026\n\n        self.orthog_projected_values = orthog_projected_values\n        self.orthog_projected_values_per_head = orthog_projected_values_per_head\n\n        out_dim *= max(1, int(orthog_projected_values) + int(orthog_projected_values_per_head))\n\n        # hybrid module, in same vein as hymba https://www.arxiv.org/abs/2411.13676\n\n        hybrid_mix = None\n        hybrid_norms = None\n        hybrid_module = maybe(deepcopy)(hybrid_module)\n\n        if exists(hybrid_module) and exists(hybrid_fold_axial_dim):\n            hybrid_module = FoldAxially(axial_dim = hybrid_fold_axial_dim, fn = hybrid_module)\n            hybrid_mix = LinearNoBias(dim, heads) if hybrid_learned_mix else None\n\n            hybrid_norms = ModuleList([\n                MultiheadRMSNorm(dim_head, heads = heads),\n                MultiheadRMSNorm(dim_head, heads = heads)\n            ])\n\n        self.hybrid_module = hybrid_module\n        self.hybrid_norms = hybrid_norms\n        self.hybrid_mix = hybrid_mix\n        self.hybrid_mask_kwarg = hybrid_mask_kwarg # for bidirectional, can forward `mask` into the hybrid module and let it handle variable lengths\n\n        # output dimension by default same as input, but can be overridden\n\n        dim_out = default(dim_out, dim)\n        self.to_out = nn.Sequential(LinearNoBias(out_dim, dim_out * 2), nn.GLU()) if on_attn else LinearNoBias(out_dim, dim_out)\n\n        # sublayer dropout\n\n        self.sublayer_dropout = nn.Dropout(sublayer_dropout) if sublayer_dropout > 0. else None\n\n        # the number of attention heads to rotate, for decoupled rope in multi-latent attention\n\n        rotate_num_heads = default(rotate_num_heads, heads)\n\n        assert 0 < rotate_num_heads <= heads\n        is_partial_rotate_heads = rotate_num_heads < heads\n        assert not (is_partial_rotate_heads and kv_heads < heads), 'grouped query attention not compatible with partial rotate heads (decoupled rope for multi-latent attention), yet'\n\n        self.rotate_num_heads = rotate_num_heads\n\n        # whether parent can kv cache\n\n        self.can_cache_kv = not selective\n\n        # init output projection 0\n\n        if zero_init_output:\n            init_zero_(self.to_out)\n\n    @torch.no_grad()\n    def qk_clip_(\n        self,\n        pre_softmax_attn: Tensor | Intermediates,\n        tau = 100 # this hyperparameter controls how large the attention logits can be\n    ):\n        \"\"\" proposed by the Moonshot AI team as a solution for Muon training instability \"\"\"\n\n        if not is_tensor(pre_softmax_attn):\n            pre_softmax_attn = pre_softmax_attn.pre_softmax_attn\n\n        attn_logit_maxes = reduce(pre_softmax_attn, 'b h i j -> h', 'max')\n\n        qk_weight_scale = (tau / attn_logit_maxes).clamp(max = 1.).sqrt()\n\n        q_weight = self.to_q.weight\n        k_weight = self.to_k.weight\n\n        qk_dim, heads = q_weight.shape[0], qk_weight_scale.numel()\n\n        qk_weight_scale = repeat(qk_weight_scale, 'h -> (h expand)', expand = qk_dim // heads)\n\n        q_weight.mul_(qk_weight_scale)\n        k_weight.mul_(qk_weight_scale)\n\n    def muon_parameters(self):\n        return chain(self.to_v.parameters(), self.to_out.parameters())\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        context_mask = None,\n        attn_mask = None,\n        rel_pos = None,\n        attn_bias = None,\n        rotary_pos_emb = None,\n        context_rotary_pos_emb = None,\n        pos = None, # for custom alibi positions\n        prev_attn = None,\n        mem = None,\n        mem_mask = None,\n        return_intermediates = False,\n        cache: Intermediates | None = None,\n        value_residual = None,\n        additional_key_values: tuple[Tensor, Tensor] | None = None,\n        additional_key_value_mask = None,\n        kv_input_residual = None,\n    ):\n        b, n, h, kv_h, head_scale, num_mem_kv, device, has_context, qkv_receive_diff_residuals, is_multi_latent_attn = x.shape[0], x.shape[1], self.heads, self.kv_heads, self.head_scale, self.num_mem_kv, x.device, exists(context), self.qkv_receive_diff_residuals, self.use_latent_kv\n\n        # an interesting possibility with hyper connections\n        # having queries, keys, values be routed from different layers\n\n        assert not (qkv_receive_diff_residuals and has_context), 'qkv receiving different sequences can only be used for self attention'\n\n        if qkv_receive_diff_residuals:\n            assert x.ndim == 4 and x.shape[0] == 3\n\n            q_input, k_input, v_input = x\n        else:\n            kv_input = default(context, x)\n            q_input, k_input, v_input = x, kv_input, kv_input\n\n        # done for free transformer\n\n        if exists(kv_input_residual):\n            k_input = k_input + kv_input_residual\n            v_input = v_input + kv_input_residual\n\n        if exists(mem):\n            k_input, mem_packed_shape = pack([mem, k_input], 'b * d')\n            v_input, _ = pack([mem, v_input], 'b * d')\n\n        # multi-latent attention logic\n        # https://arxiv.org/abs/2405.04434 - Deepseek-AI team\n\n        k_sub_heads = None # the rotateable subheads of keys derived from base sequence\n\n        if self.use_latent_q:\n            q_input = self.to_latent_q(q_input)\n\n        if is_multi_latent_attn:\n            assert not qkv_receive_diff_residuals\n            needs_k_sub_heads = exists(self.to_rotateable_k)\n\n            latent_kv_input = self.to_latent_kv(k_input)\n\n            if needs_k_sub_heads:\n                rotateable_k = self.to_rotateable_k(k_input)\n                k_sub_heads = self.split_rotateable_k_heads(rotateable_k)\n\n            if exists(cache):\n                cached_latent_kv, maybe_cached_k_sub_heads = cache.cached_kv\n                latent_kv_input = cat((cached_latent_kv, latent_kv_input), dim = -2)\n\n                if exists(maybe_cached_k_sub_heads):\n                    k_sub_heads = cat((maybe_cached_k_sub_heads, k_sub_heads), dim = -2)\n\n            if return_intermediates:\n                cached_kv = (latent_kv_input, k_sub_heads)\n\n            k_input = v_input = latent_kv_input\n\n        # query, key, value projection\n\n        q = self.to_q(q_input)\n        k = self.to_k(k_input)\n        v = self.to_v(v_input)\n\n        q = self.split_q_heads(q)\n        k = self.split_k_heads(k)\n        v = self.split_v_heads(v)\n\n        # take care of decoupled rope from multi-latent attention\n\n        if exists(k_sub_heads):\n            k = cat((k, k_sub_heads), dim = 1)\n\n        # if previous values passed in for residual, either invoke resformer\n\n        orig_values = v\n\n        # https://arxiv.org/abs/2410.17897v1\n\n        if exists(value_residual):\n            value_residual_mix = self.to_value_residual_mix(q_input)\n            v = value_residual.lerp(v, value_residual_mix)\n\n        # qk normalization\n\n        if self.qk_norm:\n            qk_l2norm = partial(l2norm, groups = self.qk_norm_groups)\n            q, k = map(qk_l2norm, (q, k))\n            scale = self.qk_norm_scale\n\n            q = q * self.qk_norm_q_scale\n            k = k * self.qk_norm_k_scale\n\n        # maybe value rmsnorm\n\n        v = maybe(self.value_rmsnorm)(v)\n\n        # take care of caching\n\n        if not is_multi_latent_attn:\n            if exists(cache):\n                ck, cv = cache.cached_kv\n\n                if exists(mem):\n                    mk, k = unpack(k, mem_packed_shape, 'b h * d')\n                    mv, v = unpack(v, mem_packed_shape, 'b h * d')\n\n                k = cat((ck, k), dim = -2)\n                v = cat((cv, v), dim = -2)\n\n                if exists(mem):\n                    k = cat((mk, k), dim = -2)\n                    v = cat((mv, v), dim = -2)\n\n            if return_intermediates:\n                mem_len = mem.shape[-2] if exists(mem) else 0\n                cached_kv = (k[..., mem_len:, :], v[..., mem_len:, :])\n\n        if exists(rotary_pos_emb):\n            rotate_num_heads = self.rotate_num_heads\n            partial_rotate_heads = rotate_num_heads < h\n\n            freqs, xpos_scale = rotary_pos_emb\n            q_xpos_scale, k_xpos_scale = (xpos_scale, xpos_scale ** -1.) if exists(xpos_scale) else (1., 1.)\n\n            if partial_rotate_heads:\n                q_rest, q = q[:, :-rotate_num_heads], q[:, -rotate_num_heads:]\n                k_rest, k = k[:, :-rotate_num_heads], k[:, -rotate_num_heads:]\n\n            q = apply_rotary_pos_emb(q, freqs, q_xpos_scale)\n\n            if has_context:\n                # override with `context_rotary_pos_emb` if provided\n\n                freqs, xpos_scale = context_rotary_pos_emb\n                _, k_xpos_scale = (xpos_scale, xpos_scale ** -1.) if exists(xpos_scale) else (1., 1.)\n\n            k = apply_rotary_pos_emb(k, freqs, k_xpos_scale)\n\n            if partial_rotate_heads:\n                q = cat((q_rest, q), dim = 1)\n                k = cat((k_rest, k), dim = 1)\n\n        input_mask = context_mask\n\n        if not exists(input_mask) and not has_context:\n            input_mask = mask\n\n            if (exists(input_mask) or exists(mem_mask)) and exists(mem):\n                seq_len, mem_len = n, mem.shape[-2]\n\n                if not exists(mem_mask):\n                    input_mask = pad_at_dim(input_mask, (mem_len, 0), dim = -1, value = True)\n                elif not exists(input_mask):\n                    input_mask = pad_at_dim(mem_mask, (0, seq_len), dim = -1, value = True)\n                else:\n                    input_mask = cat((mem_mask, input_mask), dim = -1)\n\n        # i, j determined for relative positional bias, excluding memory key / values\n\n        i, j = tuple(t.shape[-2] for t in (q, k))\n\n        # maybe append memory key / values\n\n        if num_mem_kv > 0:\n            mem_k, mem_v = tuple(repeat(t, 'h n d -> b h n d', b = b) for t in (self.mem_k, self.mem_v))\n\n            if self.qk_norm:\n                mem_k = l2norm(mem_k)\n                mem_k = mem_k * self.qk_norm_k_scale\n\n            k = cat((mem_k, k), dim = -2)\n            v = cat((mem_v, v), dim = -2)\n\n            if exists(input_mask):\n                input_mask = pad_at_dim(input_mask, (self.num_mem_kv, 0), dim = -1, value = True)\n\n        # maybe append additional key / values\n\n        if exists(additional_key_values):\n            seq_len = k.shape[-2]\n\n            added_k, added_v = additional_key_values\n            added_kv_heads, added_kv_len = added_k.shape[1], added_k.shape[-2]\n\n            # take care of expanding to query heads if mismatch between key / value heads with the ones coming from vlm\n\n            if added_kv_heads != kv_h:\n                assert divisible_by(h, added_kv_heads)\n                k, v, added_k, added_v = tuple(repeat(t, 'b h ... -> b (r h) ...', r = h // t.shape[1]) for t in (k, v, added_k, added_v))\n\n            k = cat((added_k, k), dim = -2)\n            v = cat((added_v, v), dim = -2)\n\n            if (exists(input_mask) or exists(additional_key_value_mask)):\n\n                if not exists(additional_key_value_mask):\n                    input_mask = pad_at_dim(input_mask, (added_kv_len, 0), dim = -1, value = True)\n                elif not exists(input_mask):\n                    input_mask = pad_at_dim(additional_key_value_mask, (0, seq_len), dim = -1, value = True)\n                else:\n                    input_mask = cat((additional_key_value_mask, input_mask), dim = -1)\n\n        # determine masking\n\n        mask_value = max_neg_value(q)\n        masks = []\n        final_attn_mask = None\n\n        if exists(input_mask):\n            input_mask = rearrange(input_mask, 'b j -> b 1 1 j')\n            masks.append(~input_mask)\n\n        if exists(attn_mask):\n            assert 2 <= attn_mask.ndim <= 4, 'attention mask must have greater than 2 dimensions but less than or equal to 4'\n            if attn_mask.ndim == 2:\n                attn_mask = rearrange(attn_mask, 'i j -> 1 1 i j')\n            elif attn_mask.ndim == 3:\n                attn_mask = rearrange(attn_mask, 'h i j -> 1 h i j')\n            masks.append(~attn_mask)\n\n        if exists(self.max_attend_past):\n            range_q = arange(j - i, j, device = device)\n            range_k = arange(j, device = device)\n            dist = einx.subtract('i, j -> 1 1 i j', range_q, range_k)\n            max_attend_past_mask = dist > self.max_attend_past\n            max_attend_past_mask = pad_at_dim(max_attend_past_mask, (num_mem_kv, 0), value = False, dim = -1) # handle memory key / values\n            masks.append(max_attend_past_mask)\n\n        if len(masks) > 0:\n            final_attn_mask = ~or_reduce(masks)\n\n        # prepare relative positional bias, if needed\n\n        if exists(rel_pos):\n            assert not exists(attn_bias)\n\n            if exists(pos):\n                assert isinstance(rel_pos, AlibiPositionalBias), 'only alibi allowed for custom positions at the moment'\n                # allow for custom positions to be passed in\n                attn_bias = rel_pos.forward_custom_pos(pos)\n            else:\n                attn_bias = rel_pos(i, j)\n\n            attn_bias = pad_at_dim(attn_bias, (num_mem_kv, 0)) # handle memory key / values\n\n        # prepare data dependent alibi from forgetting transformers paper, if needed\n\n        if exists(self.data_dependent_alibi):\n            attn_bias = self.data_dependent_alibi(x)\n\n            attn_bias = pad_at_dim(attn_bias, (num_mem_kv, 0))\n\n        if self.laser:\n            v = softclamp(v, self.laser_softclamp_value)\n            v = v.exp()\n\n        # attention is all we need\n\n        out, intermediates = self.attend(\n            q, k, v,\n            mask = final_attn_mask,\n            attn_bias = attn_bias,\n            prev_attn = prev_attn\n        )\n\n        # laser\n\n        if self.laser:\n            out = log(out)\n\n        # store the values for resformer\n\n        intermediates.values = orig_values\n\n        # normformer scaling of heads\n\n        if head_scale:\n            out = out * self.head_scale_params\n\n        # per head gating, from https://arxiv.org/abs/2306.12929\n\n        if exists(self.to_v_head_gate):\n            head_gate = self.to_v_head_gate(x)\n            out = einx.multiply('b n h, b h n d ->b h n d', head_gate.sigmoid(), out)\n\n        # if exists hybrid module, must do a normalization\n\n         # hybrid module\n\n        if exists(self.hybrid_module):\n\n            # hybrid input\n\n            hybrid_forward_kwargs = dict()\n\n            if not self.causal and exists(self.hybrid_mask_kwarg):\n                hybrid_forward_kwargs = {self.hybrid_mask_kwarg: mask}\n\n            # handle maybe hybrid cache\n\n            hybrid_forward_args = ()\n\n            if exists(cache) and exists(cache.hybrid_hidden):\n                hybrid_hiddens = cache.hybrid_hidden\n                hybrid_forward_args = (hybrid_hiddens,)\n\n            # hybrid forward\n\n            hybrid_outputs = self.hybrid_module(x, *hybrid_forward_args, **hybrid_forward_kwargs)\n\n            # handle hybrid out\n\n            (hybrid_out, *rest_hybrid_outs), _ = tree_flatten(hybrid_outputs)\n\n            # handle variable hybrid output and multi rmsnorm before summing to main attention output (also normed)\n\n            if hybrid_out.ndim == 3:\n                hybrid_out = rearrange(hybrid_out, 'b n (h d) -> b h n d', h = h)\n\n            if len(rest_hybrid_outs) > 0:\n                hybrid_hidden = first(rest_hybrid_outs)\n                intermediates.hybrid_hidden = hybrid_hidden\n\n            out_norm, hybrid_out_norm = self.hybrid_norms\n\n            out = out_norm(out)\n            hybrid_out = hybrid_out_norm(hybrid_out)\n\n            if exists(self.hybrid_mix):\n                mix = self.hybrid_mix(x)\n                mix = rearrange(mix, 'b n h -> b h n 1')\n                out = out.lerp(hybrid_out, mix.sigmoid())\n            else:\n                out = 0.5 * (out + hybrid_out)\n\n        # merge heads\n\n        out = self.merge_heads(out)\n\n        # alphafold2 styled gating of the values\n\n        if exists(self.to_v_gate):\n            gates = self.to_v_gate(x)\n            out = out * self.to_v_gate_activation(gates)\n\n        # maybe orthogonal projected weighted values - \"belief\" attention\n\n        if self.orthog_projected_values or self.orthog_projected_values_per_head:\n            orthog_projected = []\n            v_for_proj = repeat(orig_values, 'b h n d -> b n (g h d)', g = self.groups)\n\n            if self.orthog_projected_values:\n                projected = orthog_project(out, v_for_proj)\n                orthog_projected.append(projected)\n\n            if self.orthog_projected_values_per_head:\n                v_for_proj = rearrange(v_for_proj, 'b n (h d) -> b n h d', h = h)\n                out = rearrange(out, 'b n (h d) -> b n h d', h = h)\n                projected = orthog_project(out, v_for_proj)\n                projected = rearrange(projected, 'b n h d -> b n (h d)')\n                orthog_projected.append(projected)\n\n            out = cat(orthog_projected, dim = -1)\n\n        # combine the heads\n\n        out = self.to_out(out)\n\n        # maybe sublayer dropout\n\n        out = maybe(self.sublayer_dropout)(out)\n\n        if exists(mask) and not exists(cache):\n            out = einx.where('b n, b n d, -> b n d', mask, out, 0.)\n\n        if not return_intermediates:\n            return out\n\n        intermediates.cached_kv = cached_kv\n\n        return out, intermediates", "test_code": "", "difficulty": "medium", "category": "attention", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "Attention", "line": 1339}}
{"prompt": "Create a attention layers neural network module", "code": "class AttentionLayers(Module):\n    def __init__(\n        self,\n        dim,\n        depth = None,\n        heads = 8,\n        causal = False,\n        cross_attend = False,\n        only_cross = False,\n        use_scalenorm = False,\n        use_rmsnorm = False,\n        use_dynamic_tanh = False,\n        dynamic_tanh_init_alpha = 1.,\n        use_simple_rmsnorm = False,\n        use_adaptive_layernorm = False,\n        use_adaptive_rmsnorm = False,\n        use_adaptive_layerscale = False, # paired with use_adaptive_layernorm for ada-ln-zero from DiT paper\n        norm_add_unit_offset = True,\n        dim_condition = None,\n        adaptive_condition_mlp = False,\n        adaptive_condition_mlp_expansion = 4,\n        alibi_pos_bias = False,\n        alibi_num_heads = None,\n        rel_pos_bias = False,\n        rel_pos_num_buckets = 32,\n        rel_pos_max_distance = 128,\n        dynamic_pos_bias = False,\n        dynamic_pos_bias_log_distance = False,\n        dynamic_pos_bias_mlp_depth = 2,\n        dynamic_pos_bias_norm = False,\n        rotary_pos_emb = False,\n        rotary_emb_dim = None,\n        rotary_xpos = False,\n        rotary_interpolation_factor = 1.,\n        rotary_xpos_scale_base = 512,\n        rotary_base_rescale_factor = 1.,\n        rotate_num_heads = None,\n        weight_tie_layers = False,\n        custom_layers: tuple[str, ...] | None = None,\n        layers_execute_order: tuple[int, ...] | None = None,\n        sandwich_coef = None,\n        par_ratio = None,\n        residual_attn = False,\n        cross_residual_attn = False,\n        macaron = False,\n        pre_norm = True,\n        pre_norm_has_final_norm = True,\n        gate_residual = False,\n        scale_residual = False,\n        scale_residual_constant = 1.,\n        shift_tokens = 0,\n        sandwich_norm = False,\n        softclamp_output = False,\n        softclamp_output_value = 30.,\n        zero_init_branch_output = False,\n        layer_dropout = 0.,\n        cross_attn_tokens_dropout = 0.,\n        disable_abs_pos_emb = None,\n        use_layerscale = False,\n        layerscale_init_value = 0.,\n        unet_skips = False,\n        integrate_layers = False,\n        layer_integrate_use_softmax = True,\n        num_residual_streams = 1,\n        qkv_receive_diff_residuals = False,\n        reinject_input = False,              # seen first in DEQ paper https://arxiv.org/abs/1909.01377, but later used in a number of papers trying to achieve depthwise generalization https://arxiv.org/abs/2410.03020v1\n        learned_reinject_input_gate = False,\n        add_value_residual = False,          # resformer from Zhou et al - https://arxiv.org/abs/2410.17897v1 - further corroboration by https://arxiv.org/abs/2412.15113 (faster emergence of ICL) - looks like this setting may becoming a necessity for every transformer soon\n        learned_value_residual_mix = True,   # seeing big improvements when the value residual mix value is learned per token - credit goes to @faresobeid for taking the first step with learned scalar mix, then @Blinkdl for taking it a step further with data dependent. here we will use per token learned\n        rel_pos_kwargs: dict = dict(),\n        residual_fn_kwargs: dict = dict(),\n        verbose = True,\n        **kwargs\n    ):\n        super().__init__()\n        rotary_pos_emb = rotary_pos_emb or rotary_xpos\n\n        ff_kwargs, kwargs = groupby_prefix_and_trim('ff_', kwargs)\n        attn_kwargs, kwargs = groupby_prefix_and_trim('attn_', kwargs)\n        cross_attn_kwargs, kwargs = groupby_prefix_and_trim('cross_attn_', kwargs)\n\n        dim_head = attn_kwargs.get('dim_head', DEFAULT_DIM_HEAD)\n        data_dependent_alibi = attn_kwargs.get('data_dependent_alibi', False)\n\n        assert len(kwargs) == 0, f'unrecognized kwargs passed in {kwargs.keys()}'\n\n        self.dim = dim\n        self.causal = causal\n        self.layers = ModuleList([])\n\n        self.attn_heads = heads\n        self.attn_dim_head = dim_head\n\n        # routing related\n        # 1. greater than one residual stream, proposed in Hyper-Connections paper https://arxiv.org/abs/2409.19606\n        # 2. integrating more than one past layer, from LIMe paper https://arxiv.org/abs/2502.09245\n\n        qkv_receive_diff_residuals |= integrate_layers # qkv always receives different views if integrating layers\n\n        # hyper connections\n\n        assert num_residual_streams > 0\n        has_hyper_connections = num_residual_streams > 1\n\n        self.num_residual_streams = num_residual_streams\n        self.stream_emb = nn.Parameter(torch.zeros(num_residual_streams, dim)) if num_residual_streams > 1 else None\n\n        assert not (has_hyper_connections and gate_residual)\n\n        hyper_conn_produce_diff_views = qkv_receive_diff_residuals and not integrate_layers\n\n        # LIMe\n\n        hiddens_counter = 0\n        self.layer_integrators = ModuleList([])\n\n        assert not (qkv_receive_diff_residuals and not (hyper_conn_produce_diff_views or integrate_layers))\n\n        # positions related\n\n        self.disable_abs_pos_emb = default(disable_abs_pos_emb, (rel_pos_bias or rotary_pos_emb))\n\n        rotary_emb_dim = default(rotary_emb_dim, dim_head // 2)\n\n        assert rotary_emb_dim <= dim_head, f'rotary emb dim {rotary_emb_dim} must be less than or equal to attention head dimension {dim_head}'\n\n        if verbose and rotary_emb_dim < 32:\n            logger.warning('when training language model, rotary embedding dimension should be at least 32')\n\n        assert not (rotary_xpos and not causal), 'rotary xpos is not compatible with bidirectional attention'\n        self.rotary_pos_emb = RotaryEmbedding(rotary_emb_dim, use_xpos = rotary_xpos, scale_base = rotary_xpos_scale_base, interpolation_factor = rotary_interpolation_factor, base_rescale_factor = rotary_base_rescale_factor) if rotary_pos_emb else None\n\n        assert at_most_one_of(alibi_pos_bias, rel_pos_bias, data_dependent_alibi), 'you can only choose one of Alibi positional bias, data dependent Alibi (forgetting transformers), dynamic tanh, or T5 relative positional bias'\n        assert rel_pos_num_buckets <= rel_pos_max_distance, 'number of relative position buckets must be less than the relative position max distance'\n\n        # relative positional bias\n\n        flash_attn = attn_kwargs.get('flash', False)\n        assert at_most_one_of(rel_pos_bias, dynamic_pos_bias, alibi_pos_bias), 'you can only choose up to one of t5, alibi, or dynamic positional bias'\n\n        self.rel_pos = None\n\n        if rel_pos_bias:\n            assert not flash_attn, 'flash attention not compatible with t5 relative positional bias'\n            self.rel_pos = RelativePositionBias(scale = dim_head ** 0.5, causal = causal, heads = heads, num_buckets = rel_pos_num_buckets, max_distance = rel_pos_max_distance, **rel_pos_kwargs)\n        elif dynamic_pos_bias:\n            assert not flash_attn, 'flash attention not compatible with dynamic positional bias'\n            self.rel_pos = DynamicPositionBias(dim = dim // 4, heads = heads, log_distance = dynamic_pos_bias_log_distance, depth = dynamic_pos_bias_mlp_depth, norm = dynamic_pos_bias_norm, **rel_pos_kwargs)\n        elif alibi_pos_bias:\n            alibi_num_heads = default(alibi_num_heads, heads)\n            assert alibi_num_heads <= heads, 'number of ALiBi heads must be less than the total number of heads'\n            self.rel_pos = AlibiPositionalBias(heads = alibi_num_heads, total_heads = heads, **rel_pos_kwargs)\n\n        assert not (not pre_norm and sandwich_norm), 'sandwich norm cannot be used when not using prenorm'\n\n        self.pre_norm = pre_norm\n        self.sandwich_norm = sandwich_norm\n\n        self.residual_attn = residual_attn\n        self.cross_residual_attn = cross_residual_attn\n        assert not (flash_attn and (residual_attn or cross_residual_attn)), 'flash attention is not compatible with residual attention'\n\n        self.cross_attend = cross_attend\n\n        # determine norm\n\n        assert at_most_one_of(use_scalenorm, use_rmsnorm, use_dynamic_tanh, use_simple_rmsnorm, use_adaptive_layernorm, use_adaptive_rmsnorm), 'you can only use either scalenorm, rmsnorm, adaptive layernorm, adaptive rmsnorm, or simple rmsnorm'\n\n        norm_need_condition = False\n        dim_condition = default(dim_condition, dim)\n        dim_condition_mult = 1\n\n        if adaptive_condition_mlp:\n            dim_condition_mult = adaptive_condition_mlp_expansion\n\n        if use_scalenorm:\n            norm_class = ScaleNorm\n        elif use_rmsnorm:\n            norm_class = RMSNorm\n        elif use_simple_rmsnorm:\n            norm_class = SimpleRMSNorm\n        elif use_dynamic_tanh:\n            assert pre_norm, 'dynamic tanh norm only tested for pre-norm'\n            norm_class = partial(DynamicTanh, init_alpha = dynamic_tanh_init_alpha)\n        elif use_adaptive_layernorm:\n            norm_need_condition = True\n            norm_class = partial(AdaptiveLayerNorm, dim_condition = dim_condition * dim_condition_mult)\n        elif use_adaptive_rmsnorm:\n            norm_need_condition = True\n            norm_class = partial(AdaptiveRMSNorm, dim_condition = dim_condition * dim_condition_mult)\n        else:\n            norm_class = LayerNorm\n\n        norm_fn = partial(norm_class, dim)\n\n        if not norm_need_condition and norm_add_unit_offset:\n            # researcher Ohad Rubin shares in a blog post by adding an offset to gammas, they can be subjected to weight decay safely\n            norm_fn = partial(norm_fn, unit_offset = True)\n\n        self.norm_need_condition = norm_need_condition\n        self.dim_condition = dim_condition\n\n        # determine default block layer type order\n\n        if cross_attend and not only_cross:\n            default_block = ('a', 'c', 'f')\n        elif cross_attend and only_cross:\n            default_block = ('c', 'f')\n        else:\n            default_block = ('a', 'f')\n\n        if macaron:\n            default_block = ('f',) + default_block\n\n        # determine post branch wrapper\n\n        assert at_most_one_of(use_layerscale, use_adaptive_layerscale)\n\n        post_branch_fn = None\n        post_branch_fn_needs_condition = False\n\n        if use_layerscale:\n            post_branch_fn = partial(LayerScale, dim = dim, init_value = layerscale_init_value)\n        elif use_adaptive_layerscale:\n            post_branch_fn = partial(AdaptiveLayerScale, dim = dim, dim_condition = dim_condition * dim_condition_mult)\n            post_branch_fn_needs_condition = True\n\n        self.post_branch_fn_needs_condition = post_branch_fn_needs_condition\n\n        if exists(post_branch_fn) and not post_branch_fn_needs_condition and norm_add_unit_offset:\n            post_branch_fn = partial(post_branch_fn, unit_offset = True)\n\n        # setup mlp for conditioning\n\n        self.need_condition = norm_need_condition or post_branch_fn_needs_condition\n\n        self.adaptive_mlp = nn.Identity()\n\n        if self.need_condition and adaptive_condition_mlp:\n            self.adaptive_mlp = nn.Sequential(\n                LinearNoBias(dim_condition, dim_condition * dim_condition_mult),\n                nn.SiLU()\n            )\n\n        # zero init\n\n        if zero_init_branch_output:\n            attn_kwargs = {**attn_kwargs, 'zero_init_output':  True}\n            ff_kwargs = {**ff_kwargs, 'zero_init_output':  True}\n\n        # setup weight tying, which is a special case of `layer_execute_order`\n\n        assert not (exists(layers_execute_order) and exists(custom_layers) and exists(depth)), 'depth should not be passed in if using custom layers and custom layer execution order'\n\n        assert not (weight_tie_layers and any([*map(exists, (custom_layers, par_ratio, sandwich_coef))]))\n\n        if weight_tie_layers:\n            assert exists(depth), 'depth must be passed in with `weight_tie_layers` = True'\n            assert not exists(layers_execute_order)\n            layers_execute_order = tuple(range(len(default_block))) * depth\n            depth = 1\n\n        # calculate layer block order\n\n        len_default_block = 1\n\n        if exists(custom_layers):\n            layer_types = custom_layers\n        elif exists(par_ratio):\n            par_depth = depth * len(default_block)\n            assert 1 < par_ratio <= par_depth, 'par ratio out of range'\n            default_block = tuple(filter(not_equals('f'), default_block))\n            par_attn  = par_depth // par_ratio\n            depth_cut = par_depth * 2 // 3  # 2 / 3 attention layer cutoff suggested by PAR paper\n            par_width = (depth_cut + depth_cut // par_attn) // par_attn\n            assert len(default_block) <= par_width, 'default block is too large for par_ratio'\n            par_block = default_block + ('f',) * (par_width - len(default_block))\n            par_head = par_block * par_attn\n            layer_types = par_head + ('f',) * (par_depth - len(par_head))\n        elif exists(sandwich_coef):\n            assert sandwich_coef > 0 and sandwich_coef <= depth, 'sandwich coefficient should be less than the depth'\n            layer_types = ('a',) * sandwich_coef + default_block * (depth - sandwich_coef) + ('f',) * sandwich_coef\n        else:\n            assert exists(depth), '`depth` must be passed in for `Decoder` or `Encoder`'\n            layer_types = default_block * depth\n            len_default_block = len(default_block)\n\n        self.layer_types = layer_types\n        self.layers_execute_order = default(layers_execute_order, tuple(range(len(layer_types))))\n\n        assert all([i < len(self.layer_types) for i in self.layers_execute_order])\n\n        self.num_attn_layers = len(list(filter(equals('a'), layer_types)))\n\n        # set the depth\n\n        depth = default(depth, len(self.layers_execute_order))\n        self.depth = depth\n\n        # stochastic depth\n\n        self.layer_dropouts = cast_tuple(layer_dropout, len(layer_types))\n\n        # structured dropout for cross attending\n\n        self.cross_attn_tokens_dropout = cross_attn_tokens_dropout\n\n        # calculate token shifting\n\n        shift_tokens = cast_tuple(shift_tokens, len(layer_types))\n\n        # optional soft clamping just before the final norm\n        # used in gemma 2\n\n        self.softclamp_output = softclamp_output\n        self.softclamp_output_value = softclamp_output_value\n\n        # whether it has post norm\n\n        self.final_norm = norm_fn() if pre_norm and pre_norm_has_final_norm else nn.Identity()\n\n        # whether unet or not\n\n        self.unet_skips = unet_skips\n        num_skips = self.depth // len_default_block\n\n        assert not (unet_skips and num_skips == 0), 'must have depth of at least 2 for unet skip connections'\n\n        skip_indices = [i * len_default_block for i in range(num_skips)]\n\n        self.skip_combines = ModuleList([])\n\n        # whether there is reinjection of input at every layer\n\n        self.reinject_input = reinject_input\n        self.reinject_input_proj = nn.Linear(dim, dim, bias = False) if reinject_input else None\n        self.learned_reinject_input_gate = nn.Linear(dim, 1, bias = False) if learned_reinject_input_gate else None\n\n        # add the value from the first self attention block to all latter projected self attention values as a residual\n\n        self.add_value_residual = add_value_residual\n\n        is_first_self_attn = True\n        is_first_cross_attn = True\n        learned_value_residual_mix &= add_value_residual\n\n        # iterate and construct layers\n\n        for ind, (layer_type, layer_shift_tokens) in enumerate(zip(self.layer_types, shift_tokens)):\n\n            # `ind` is the index of each module - attention, feedforward, cross attention\n            # but `block_ind` refers to the typical enumeration of a transformer block (attn + ff + [optional] cross attn)\n\n            block_begin = divisible_by(ind, len_default_block)\n            block_ind = ind // len_default_block\n\n            is_last_layer = ind == (len(self.layer_types) - 1)\n\n            # attention, cross attention, feedforward\n\n            layer_qkv_receives_diff_view = layer_type == 'a' and qkv_receive_diff_residuals and not (is_first_self_attn and integrate_layers)\n\n            if layer_type == 'a':\n                self_attn_learned_value_residual = learned_value_residual_mix and not is_first_self_attn\n\n                layer = Attention(dim, heads = heads, causal = causal, qkv_receive_diff_residuals = layer_qkv_receives_diff_view, learned_value_residual_mix = self_attn_learned_value_residual, rotate_num_heads = rotate_num_heads, **attn_kwargs)\n                is_first_self_attn = False\n\n            elif layer_type == 'c':\n                layer = Attention(dim, heads = heads, **{**attn_kwargs, **cross_attn_kwargs})\n                is_first_cross_attn = False\n\n            elif layer_type == 'f':\n                layer = FeedForward(dim, **ff_kwargs)\n                layer = layer if not macaron else Scale(0.5, layer)\n\n            else:\n                raise Exception(f'invalid layer type {layer_type}')\n\n            if layer_shift_tokens > 0:\n                shift_range_upper = layer_shift_tokens + 1\n                shift_range_lower = -layer_shift_tokens if not causal else 0\n                layer = ShiftTokens(range(shift_range_lower, shift_range_upper), layer)\n\n            if exists(post_branch_fn):\n                layer = post_branch_fn(layer)\n\n            layer_integrate = None\n\n            if integrate_layers:\n                num_layer_hiddens = ind + 1\n                layer_integrate_num_view = 3 if layer_qkv_receives_diff_view else 1\n\n                layer_integrate = DynamicLIMe(dim, num_layer_hiddens, num_views = layer_integrate_num_view, use_softmax = layer_integrate_use_softmax)\n\n            if has_hyper_connections:\n                residual_fn = partial(HyperConnection, num_residual_streams = num_residual_streams)\n\n                if layer_type == 'a' and hyper_conn_produce_diff_views:\n                    residual_fn = partial(residual_fn, num_input_views = 3)\n\n            elif gate_residual:\n                residual_fn = GRUGating\n            else:\n                residual_fn = Residual\n\n            residual = residual_fn(dim, layer_index = ind, scale_residual = scale_residual, scale_residual_constant = scale_residual_constant, **residual_fn_kwargs)\n\n            # handle unet skip connection\n\n            skip_combine = None\n            is_latter_half = block_begin and block_ind >= (self.depth / 2)\n\n            if self.unet_skips and is_latter_half:\n                skip_combine = ConcatCombine(dim, skip_indices.pop())\n\n            # all normalizations of the layer\n\n            pre_branch_norm = norm_fn() if pre_norm else None\n            post_branch_norm = norm_fn() if sandwich_norm else None\n            post_main_norm = norm_fn() if not pre_norm else None\n\n            norms = ModuleList([\n                pre_branch_norm,\n                post_branch_norm,\n                post_main_norm\n            ])\n\n            self.skip_combines.append(skip_combine)\n\n            self.layer_integrators.append(layer_integrate)\n\n            self.layers.append(ModuleList([\n                norms,\n                layer,\n                residual\n            ]))\n\n        # determine whether can cache kv\n\n        self.can_cache_kv = all([module.can_cache_kv for module in self.modules() if isinstance(module, Attention)])\n\n    def attn_qk_clip_(\n        self,\n        intermediates: LayerIntermediates,\n        tau = 100.\n    ):\n        # pairs up the attention intermediates with each attention module and does qk clip proposed by kimi team\n\n        layer_and_layer_types = (self.layers, self.layer_types)\n\n        attn_layers = [layer for (_, layer, _), layer_type in zip(self.layers, self.layer_types) if layer_type in ('a', 'c')]\n        attn_intermeds = intermediates.attn_intermediates\n\n        assert len(attn_layers) == len(attn_intermeds)\n\n        for attn_layer, attn_inter in zip(attn_layers, attn_intermeds):\n            attn_layer.qk_clip_(attn_inter, tau = tau)\n\n    def muon_parameters(self):\n        params = []\n\n        for m in self.modules():\n            if not isinstance(m, (Attention, FeedForward)):\n                continue\n\n            params.extend(list(m.muon_parameters()))\n\n        return params\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        context_mask = None,\n        attn_mask = None,\n        self_attn_kv_mask = None,\n        mems = None,\n        mem_masks = None,\n        seq_start_pos: Tensor | None = None,\n        seq_pos_offset: int = 0,\n        cache: LayerIntermediates | None = None,\n        input_not_include_cache = False,\n        cache_age = 1,\n        return_hiddens = False,\n        rotary_pos_emb = None,\n        pos = None,\n        context_pos = None,\n        attn_bias = None,\n        deep_embeds_and_ids: tuple[nn.Parameter, Tensor] | None = None,\n        self_attn_additional_kv: (\n            LayerIntermediates |\n            list[tuple[Tensor, Tensor]]\n            | None\n        ) = None,\n        additional_kv_mask = None,\n        detach_additional_kv = False,\n        route_additional_kv_to_top = True,\n        condition = None,\n        in_attn_cond = None, # https://arxiv.org/abs/2105.04090\n        layers_execute_order: tuple[int, ...] | None = None,\n        self_attn_kv_residuals: Tensor | None = None,\n        cross_attn_kv_residuals: Tensor | None = None\n    ):\n        assert not (self.cross_attend ^ exists(context)), 'context must be passed in if cross_attend is set to True'\n        assert not (exists(condition) ^ self.need_condition), 'condition needs to be passed in if using adaptive layernorm or vice versa'\n\n        # handle condition\n\n        if exists(condition):\n            assert condition.shape[-1] == self.dim_condition, f'expected condition dimension of {self.dim_condition} but received {condition.shape[-1]}'\n\n            assert condition.ndim in {2, 3}\n\n            if condition.ndim == 2:\n                condition = rearrange(condition, 'b d -> b 1 d')\n\n            condition = self.adaptive_mlp(condition)\n\n        # setup maybe layernorm kwarg\n\n        norm_kwargs = dict()\n\n        if self.norm_need_condition:\n            norm_kwargs.update(condition = condition)\n\n        # maybe post branch fn conditioning (DiT paper's ada-ln-zero)\n\n        block_forward_kwargs = dict()\n\n        if self.post_branch_fn_needs_condition:\n            block_forward_kwargs.update(condition = condition)\n\n        # initialize accums\n\n        hiddens = []\n        layer_hiddens = []\n        intermediates = []\n\n        prev_attn = None\n        prev_cross_attn = None\n\n        mems = mems.copy() if exists(mems) else [None] * self.num_attn_layers\n        mem_masks = mem_masks.copy() if exists(mem_masks) else [None] * self.num_attn_layers\n\n        # handle left padded sequences\n\n        if exists(seq_start_pos):\n            seq_arange = arange(x.shape[-2], device = x.device, dtype = torch.long)\n            left_pad_mask = seq_arange >= seq_start_pos[..., None]\n\n            if exists(self_attn_kv_mask):\n                self_attn_kv_mask = self_attn_kv_mask & left_pad_mask\n            else:\n                self_attn_kv_mask = left_pad_mask\n\n        # rotary positions\n\n        cross_attn_rotary_pos_emb = dict()\n\n        if exists(self.rotary_pos_emb):\n            if not exists(rotary_pos_emb):\n                maybe_mem = first(mems, None) # todo - handle edge case where different layers get different memory lengths. don't think this will ever come up but who knows\n                mem_len = maybe_mem.shape[1] if exists(maybe_mem) else 0\n\n                if not exists(pos):\n                    pos = arange(x.shape[1] + mem_len + seq_pos_offset, device = x.device) - mem_len\n\n                rotary_pos_emb = self.rotary_pos_emb(pos)\n\n            # allow for rotary positions for context if provided\n\n            if exists(context_pos):\n                assert self.cross_attend\n                context_rotary_pos_emb = self.rotary_pos_emb(context_pos)\n\n                cross_attn_rotary_pos_emb.update(\n                    rotary_pos_emb = rotary_pos_emb,\n                    context_rotary_pos_emb = context_rotary_pos_emb\n                )\n\n        # assume cached key / values\n\n        prev_cache_length = 0\n\n        attn_cache = []\n\n        if exists(cache):\n            assert self.causal and not exists(attn_mask)\n\n            prev_cache_length = cache.cache_length\n\n            if exists(context):\n                context = context[:, :0]\n\n            if cache_age > 0:\n                x = x[:, -cache_age:] # for spec decoding, may be greater than 1\n\n                if exists(deep_embeds_and_ids):\n                    deep_embeds, token_ids = deep_embeds_and_ids\n                    token_ids = token_ids[:, -cache_age:]\n                    deep_embeds_and_ids = (deep_embeds, token_ids)\n\n            attn_cache = cache.attn_intermediates\n\n        next_cache_length = x.shape[1]\n\n        iter_attn_cache = iter(attn_cache)\n\n        # handle deep embeds if needed\n\n        deep_embeds = []\n\n        if exists(deep_embeds_and_ids):\n            deep_embeds, token_ids = deep_embeds_and_ids\n            deep_embeds_across_depth = deep_embeds[token_ids]\n            deep_embeds = rearrange(deep_embeds_across_depth, 'b n l d -> l b n d')\n\n        deep_embeds_iter = iter(deep_embeds)\n\n        # setup multistreams if needed\n\n        streams = self.num_residual_streams\n        is_multistream = streams > 1\n\n        if is_multistream:\n            x = einx.add('b n d, s d -> (b s) n d', x, self.stream_emb)\n\n        # get layers to be executed\n\n        layer_variables = (\n            self.layer_types,\n            self.skip_combines,\n            self.layers,\n            self.layer_dropouts,\n            self.layer_integrators\n        )\n\n        # able to override the layers execution order on forward, for trying to depth extrapolate\n\n        layers_execute_order = default(layers_execute_order, self.layers_execute_order)\n        layer_variables = tuple(tuple(layer_variable[i] for i in layers_execute_order) for layer_variable in layer_variables)\n\n        # additional self attn key / values - say coming from vlm\n\n        if exists(self_attn_additional_kv) and route_additional_kv_to_top:\n\n            if isinstance(self_attn_additional_kv, LayerIntermediates):\n                self_attn_additional_kv = get_cached_kvs(self_attn_additional_kv)\n\n            if detach_additional_kv:\n                self_attn_additional_kv = detach_all(self_attn_additional_kv)\n\n            num_self_attns = sum([layer_type == 'a' for layer_type in first(layer_variables)])\n\n            self_attn_additional_kv = self_attn_additional_kv[-num_self_attns:]\n            self_attn_additional_kv = [None] * (num_self_attns - len(self_attn_additional_kv)) + self_attn_additional_kv\n\n        iter_self_attn_kv = iter(default(self_attn_additional_kv, ()))\n\n        # derived input for reinjection if needed\n\n        inp_inject = None\n\n        if self.reinject_input:\n            assert not exists(in_attn_cond)\n            inp_inject = self.reinject_input_proj(x)\n\n        elif exists(in_attn_cond):\n            # handle in-attention conditioning, which serves the same purpose of having the network learn the residual\n            inp_inject = in_attn_cond if in_attn_cond.ndim == 3 else rearrange(in_attn_cond, 'b d -> b 1 d')\n\n        if exists(inp_inject) and exists(self.learned_reinject_input_gate):\n            inp_inject_gate = self.learned_reinject_input_gate(x).sigmoid()\n            inp_inject = inp_inject * inp_inject_gate\n\n        # store all hiddens for skips\n\n        skip_hiddens = []\n\n        # for residuals to key value inputs for self and cross attention\n\n        self_attn_kv_residuals_iter = iter((None,))\n        cross_attn_kv_residuals_iter = iter((None,))\n\n        if exists(self_attn_kv_residuals):\n            if self_attn_kv_residuals.ndim == 3:\n                self_attn_kv_residuals = rearrange(self_attn_kv_residuals, '... ->  1 ...')\n\n            self_attn_kv_residuals_iter = iter(self_attn_kv_residuals)\n\n        if exists(cross_attn_kv_residuals):\n            if cross_attn_kv_residuals.ndim == 3:\n                cross_attn_kv_residuals = rearrange(cross_attn_kv_residuals, '... ->  1 ...')\n\n            cross_attn_kv_residuals_iter = iter(cross_attn_kv_residuals)\n\n        # for value residuals\n\n        first_self_attn_inter = None\n        first_cross_attn_inter = None\n\n        # go through the attention and feedforward layers\n\n        for ind, (layer_type, skip_combine, (norm, block, residual_fn), layer_dropout, layer_integrator) in enumerate(zip(*layer_variables)):\n            is_last = ind == (len(self.layers) - 1)\n\n            # handle skip connections\n\n            skip_hiddens.append(x)\n\n            if exists(skip_combine):\n                x = skip_combine(x, skip_hiddens)\n\n            # layer dropout\n\n            if self.training and layer_dropout > 0. and random() < layer_dropout:\n                continue\n\n            if layer_type == 'a':\n                if return_hiddens:\n                    hiddens.append(x)\n\n                layer_mem = mems.pop(0) if mems else None\n                layer_mem_mask = mem_masks.pop(0) if mem_masks else None\n\n            if layer_type == 'c':\n                if self.training and self.cross_attn_tokens_dropout > 0.:\n                    context, context_mask = dropout_seq(context, context_mask, self.cross_attn_tokens_dropout)\n\n            x, inner_residual, residual_kwargs = residual_fn.prepare(x)\n\n            layer_hiddens.append(x)\n\n            if exists(layer_integrator):\n                x = layer_integrator(x, layer_hiddens)\n\n            pre_norm, post_branch_norm, post_main_norm = norm\n\n            if self.need_condition:\n                pre_norm = maybe(partial)(pre_norm, **norm_kwargs)\n                post_branch_norm = maybe(partial)(post_branch_norm, **norm_kwargs)\n                post_main_norm = maybe(partial)(post_main_norm, **norm_kwargs)\n\n            if exists(inp_inject):\n                x = x + inp_inject\n\n            if exists(pre_norm):\n                x = pre_norm(x)\n\n                if layer_type == 'a' and exists(layer_mem):\n                    layer_mem = pre_norm(layer_mem)\n\n            block = partial(block, **block_forward_kwargs)\n\n            # handle maybe value residuals\n\n            maybe_self_attn_value_residual = None\n            maybe_cross_attn_value_residual = None\n\n            if self.add_value_residual:\n                if exists(first_self_attn_inter):\n                    maybe_self_attn_value_residual = first_self_attn_inter.values\n\n                if exists(first_cross_attn_inter):\n                    maybe_cross_attn_value_residual = first_cross_attn_inter.values\n\n            # forward depending on layer type\n\n            if layer_type == 'a':\n                out, inter = block(x, mask = mask, context_mask = self_attn_kv_mask, attn_mask = attn_mask, rel_pos = self.rel_pos, pos = pos, rotary_pos_emb = rotary_pos_emb, additional_key_values = next(iter_self_attn_kv, None), additional_key_value_mask = additional_kv_mask, prev_attn = prev_attn, cache = next(iter_attn_cache, None), mem = layer_mem, mem_mask = layer_mem_mask, attn_bias = attn_bias, kv_input_residual = next(self_attn_kv_residuals_iter, None), value_residual = maybe_self_attn_value_residual, return_intermediates = True)\n            elif layer_type == 'c':\n                out, inter = block(x, context = context, mask = mask, context_mask = context_mask, prev_attn = prev_cross_attn, cache = next(iter_attn_cache, None), kv_input_residual = next(cross_attn_kv_residuals_iter, None), value_residual = maybe_cross_attn_value_residual, **cross_attn_rotary_pos_emb, return_intermediates = True)\n            elif layer_type == 'f':\n                out = block(x, deep_embed = next(deep_embeds_iter, None))\n\n            # store first self or cross attention intermediate for value residual\n\n            if not exists(first_self_attn_inter) and layer_type == 'a':\n                first_self_attn_inter = inter\n\n            if not exists(first_cross_attn_inter) and layer_type == 'c':\n                first_cross_attn_inter = inter\n\n            if exists(post_branch_norm):\n                out = post_branch_norm(out)\n\n            x = residual_fn(out, inner_residual, **residual_kwargs)\n\n            if layer_type in ('a', 'c') and return_hiddens:\n                inter.layer_type = layer_type\n                intermediates.append(inter)\n\n            if layer_type == 'a' and self.residual_attn:\n                prev_attn = inter.pre_softmax_attn\n            elif layer_type == 'c' and self.cross_residual_attn:\n                prev_cross_attn = inter.pre_softmax_attn\n\n            if exists(post_main_norm):\n                x = post_main_norm(x)\n\n        if return_hiddens:\n            layer_hiddens.append(x)\n\n        if self.softclamp_output:\n            x = softclamp(x, self.softclamp_output_value)\n\n        final_norm = self.final_norm\n\n        if self.need_condition:\n            final_norm = maybe(partial)(final_norm, **norm_kwargs)\n\n        # take care of multistreams if needed, use sum for now\n\n        if is_multistream:\n            x = reduce(x, '(b s) n d -> b n d', 'sum', s = streams)\n\n        x = final_norm(x)\n\n        if not return_hiddens:\n            return x\n\n        intermediates = LayerIntermediates(\n            hiddens = hiddens,\n            last_hidden = x,\n            attn_intermediates = intermediates,\n            layer_hiddens = layer_hiddens,\n            cache_length = next_cache_length + prev_cache_length\n        )\n\n        return x, intermediates", "test_code": "", "difficulty": "medium", "category": "attention", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AttentionLayers", "line": 2114}}
{"prompt": "Create a attention pool neural network module", "code": "class AttentionPool(Module):\n    def __init__(\n        self,\n        dim,\n        num_pooled_tokens = 1,\n        dim_context = None,\n        add_residual = False,\n        depth = 1,\n        heads = 8,\n        dim_head = 64,\n        use_transformer_blocks = None,\n        squeeze_output = None,\n        attn_kwargs: dict = dict()\n    ):\n        super().__init__()\n        dim_context = default(dim_context, dim)\n\n        squeeze_output = default(squeeze_output, False)\n        assert not (squeeze_output and num_pooled_tokens > 1)\n\n        use_transformer_blocks = default(use_transformer_blocks, depth > 1)\n        assert use_transformer_blocks or depth == 1\n\n        self.queries = nn.Parameter(torch.randn(num_pooled_tokens, dim) * 1e-2)\n\n        if use_transformer_blocks:\n            assert not add_residual, 'residual already in effect when doing a full cross attention based transformer for pooling'\n            attn_kwargs = {f'attn_{k}': v for k, v in attn_kwargs.items()}\n\n            self.pooler = CrossAttender(dim = dim, cross_attn_dim_context = dim_context, depth = depth, heads = heads, attn_dim_head = dim_head, )\n        else:\n            self.pooler = Attention(dim = dim, dim_context = dim_context, heads = heads, dim_head = dim_head, **attn_kwargs)\n\n        self.add_residual = add_residual\n        self.squeeze_output = squeeze_output\n\n    def forward(self, context, mask = None):\n        batch = context.shape[0]\n\n        queries = repeat(self.queries, 'n d -> b n d', b = batch)\n\n        pooled = self.pooler(queries, context, context_mask = mask)\n\n        if self.add_residual:\n            pooled = pooled + queries\n\n        if self.squeeze_output:\n            pooled = rearrange(pooled, 'b 1 d -> b d')\n\n        return pooled", "test_code": "", "difficulty": "medium", "category": "attention", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AttentionPool", "line": 2991}}
{"prompt": "Create a vi transformer wrapper neural network module", "code": "class ViTransformerWrapper(Module):\n    def __init__(\n        self,\n        *,\n        image_size,\n        patch_size,\n        attn_layers: Encoder,\n        channels = 3,\n        num_classes = None,\n        post_emb_norm = False,\n        num_register_tokens = 0,\n        emb_dropout = 0.\n    ):\n        super().__init__()\n        assert divisible_by(image_size, patch_size), 'image dimensions must be divisible by the patch size'\n        dim = attn_layers.dim\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_size ** 2\n\n        self.patch_size = patch_size\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n\n        has_register_tokens = num_register_tokens > 0\n        self.has_register_tokens = has_register_tokens\n\n        if has_register_tokens:\n            self.register_tokens = nn.Parameter(torch.randn(num_register_tokens, dim))\n\n        self.patch_to_embedding = nn.Sequential(\n            LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim),\n            LayerNorm(dim)\n        )\n\n        self.post_emb_norm = LayerNorm(dim) if post_emb_norm else nn.Identity()\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.attn_layers = attn_layers\n\n        self.mlp_head = nn.Linear(dim, num_classes) if exists(num_classes) else nn.Identity()\n\n    def forward(\n        self,\n        img,\n        return_embeddings = False,\n        return_logits_and_embeddings = False\n    ):\n        b, p = img.shape[0], self.patch_size\n\n        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n        x = self.patch_to_embedding(x)\n        n = x.shape[1]\n\n        x = x + self.pos_embedding[:, :n]\n\n        x = self.post_emb_norm(x)\n        x = self.dropout(x)\n\n        if self.has_register_tokens:\n            r = repeat(self.register_tokens, 'n d -> b n d', b = b)\n            x, ps = pack((x, r), 'b * d')\n\n        embed = self.attn_layers(x)\n\n        if self.has_register_tokens:\n            embed, _ = unpack(embed, ps, 'b * d')\n\n        assert at_most_one_of(return_embeddings, return_logits_and_embeddings)\n\n        if not exists(self.mlp_head) or return_embeddings:\n            return embed\n\n        pooled = embed.mean(dim = -2)\n        logits = self.mlp_head(pooled)\n\n        if not return_logits_and_embeddings:\n            return logits\n\n        return logits, embed", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ViTransformerWrapper", "line": 3042}}
{"prompt": "Create a transformer wrapper neural network module", "code": "class TransformerWrapper(Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        max_seq_len,\n        attn_layers: AttentionLayers,\n        embed_num_tokens: dict[str, int] = dict(),\n        emb_dim = None,\n        max_mem_len = 0,\n        shift_mem_down = 0,\n        emb_dropout = 0.,\n        post_emb_norm = False,\n        num_memory_tokens = None,\n        memory_tokens_interspersed_every = None,\n        tie_embedding = False,\n        logits_dim = None,\n        return_only_embed = False,\n        num_output_heads = 1,\n        use_abs_pos_emb = True,\n        scaled_sinu_pos_emb = False,\n        l2norm_embed = False,\n        recycling = False,            # from Jumper et al. - Alphafold2\n        train_max_recycle_steps = 4,  # saw a benefit for language modeling up to 3 recycling steps, so let's default this to 4\n        emb_frac_gradient = 1.,       # GLM-130B and Cogview successfully used this, set at 0.1\n        attn_z_loss_weight = 1e-4,\n        average_pool_embed = False,\n        use_cls_token = False,\n        num_cls_tokens = 1,\n        attn_pool = False,\n        num_pooled_tokens = 1,\n        attn_pool_depth = 1,\n        dim_pooled_tokens = None,\n        squeeze_out_last_dim = False,\n        token_emb: TokenEmbedding | None = None,\n        mixture_of_softmax = False,\n        mixture_of_softmax_k = 4,\n        sigsoftmax_logits = False,\n        ff_deep_embed = False,\n        to_logits: Module | None = None,\n        add_continuous_pred_head = False\n    ):\n        super().__init__()\n\n        dim = attn_layers.dim\n        depth = attn_layers.depth\n\n        emb_dim = default(emb_dim, dim)\n        self.emb_dim = emb_dim\n        self.num_tokens = num_tokens\n        self.num_cls_tokens = num_cls_tokens\n\n        self.max_seq_len = max_seq_len\n        self.max_mem_len = max_mem_len\n        self.shift_mem_down = shift_mem_down\n\n        self.l2norm_embed = l2norm_embed\n\n        if not exists(token_emb):\n            token_emb = TokenEmbedding(emb_dim, num_tokens, l2norm_embed = l2norm_embed)\n\n        self.token_emb = token_emb\n\n        no_abs_pos_emb = max_seq_len == 0 or not (use_abs_pos_emb and not attn_layers.disable_abs_pos_emb)\n\n        if no_abs_pos_emb:\n            self.pos_emb = always(0)\n        elif scaled_sinu_pos_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(emb_dim)\n        else:\n            self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len, l2norm_embed = l2norm_embed)\n\n        # additional embeddings - say type embedding from BERT\n\n        self.embeds = None\n\n        if len(embed_num_tokens) > 0:\n            self.embeds = ModuleDict({f'{name}_embed': nn.Embedding(num_tokens, emb_dim) for name, num_tokens in embed_num_tokens.items()})\n\n        # deep embed\n\n        # credit goes to Braden Koszarsky for first devising value embeddings in nanogpt-speedrun project\n        # then Bo Peng for coming up with this alternate design in feedforward for RWKV 8\n        # improvements were clearest to me (on my toy setup) with multiplying on output of feedforward, will try with attention at future date\n\n        self.ff_deep_embed = None\n        if ff_deep_embed:\n            self.ff_deep_embed = nn.Parameter(torch.ones(num_tokens, depth, dim))\n\n        # fraction of the gradient that should go to the embedding, https://arxiv.org/abs/2105.13290\n\n        self.emb_frac_gradient = emb_frac_gradient\n\n        self.post_emb_norm = LayerNorm(emb_dim) if post_emb_norm else nn.Identity()\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n        self.attn_layers = attn_layers\n\n        self.init_()\n\n        assert num_output_heads > 0\n\n        assert at_most_one_of(average_pool_embed, use_cls_token)\n\n        # maybe recycling\n\n        self.recycling = recycling\n        self.recycled_proj = LinearNoBias(dim, dim) if recycling else None\n\n        self.train_max_recycle_steps = train_max_recycle_steps\n\n        # either cls token or attn pool, but not both\n\n        assert not (use_cls_token and attn_pool)\n\n        # classic cls token from the bert days\n\n        self.cls_token = None\n\n        if use_cls_token:\n            self.cls_token = nn.Parameter(torch.zeros(num_cls_tokens, dim))\n            nn.init.normal_(self.cls_token, std = 0.02)\n\n        # attn pool\n\n        self.attn_pool = None\n\n        if attn_pool:\n            self.attn_pool = AttentionPool(dim = default(dim_pooled_tokens, dim), dim_context = dim, num_pooled_tokens = num_pooled_tokens, depth = attn_pool_depth, heads = self.attn_layers.attn_heads, dim_head = self.attn_layers.attn_dim_head)\n\n        # whether to average pool the embed (`global average pool`)\n\n        self.average_pool_embed = average_pool_embed\n\n        # output type\n\n        self.output_is_log_prob = mixture_of_softmax\n\n        self.to_mixture = None\n        self.combine_mixture = None\n\n        if mixture_of_softmax:\n            assert num_output_heads == 1\n\n            self.to_mixture = Sequential(\n                LinearNoBias(dim, dim * mixture_of_softmax_k),\n                Rearrange('... (k d) -> ... k d', k = mixture_of_softmax_k)\n            )\n\n            self.combine_mixture = LinearNoBias(dim, mixture_of_softmax_k)\n\n        # sig softmax\n\n        self.sigsoftmax_logits = sigsoftmax_logits\n\n        # output head, usually to logits of num_tokens\n\n        logits_dim = default(logits_dim, num_tokens)\n\n        self.has_multiple_heads = num_output_heads > 1\n\n        if return_only_embed:\n            self.to_logits = None\n        elif tie_embedding:\n            assert isinstance(token_emb, TokenEmbedding), 'can only tie embedding if using `TokenEmbedding`'\n            self.to_logits = lambda t: t @ self.token_emb.emb.weight.t()\n        elif num_output_heads > 1:\n            self.to_logits = ModuleList([LinearNoBias(dim, logits_dim) for _ in range(num_output_heads)])\n        else:\n            self.to_logits = LinearNoBias(dim, logits_dim) if not exists(to_logits) else to_logits\n\n        # add a head that predicts the embedding of the next step\n\n        self.add_continuous_pred_head = add_continuous_pred_head\n\n        if add_continuous_pred_head:\n\n            self.to_next_embed_pred = nn.Sequential(\n                LinearNoBias(dim, dim),\n                nn.SiLU(),\n                LinearNoBias(dim, dim)\n            )\n\n        # memory tokens (like [cls]) from Memory Transformers paper\n\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.num_memory_tokens = num_memory_tokens\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n\n        self.memory_tokens_interspersed_every = memory_tokens_interspersed_every\n\n        # squeeze out last dimension if possible\n\n        self.squeeze_out_last_dim = squeeze_out_last_dim\n\n        # whether can do cached kv decoding\n\n        self.can_cache_kv = self.num_memory_tokens == 0 and not recycling and self.attn_layers.can_cache_kv\n        self.can_cache_kv_outside_max_seq_len = no_abs_pos_emb\n\n    def init_(self):\n        if hasattr(self.token_emb, 'init_'):\n            self.token_emb.init_()\n\n        if self.l2norm_embed:\n            if not isinstance(self.pos_emb, always):\n                nn.init.normal_(self.pos_emb.emb.weight, std = 1e-5)\n\n    def attn_qk_clip_(\n        self,\n        intermediates: LayerIntermediates,\n        tau = 100.\n    ):\n        self.attn_layers.attn_qk_clip_(intermediates, tau = tau)\n\n    def muon_parameters(self):\n        return self.attn_layers.muon_parameters()\n\n    def forward(\n        self,\n        x,\n        return_embeddings = False,\n        return_logits_and_embeddings = False,\n        return_intermediates = False,\n        return_embeddings_and_intermediates = False,\n        return_logit_entropies = False,\n        return_next_embed_pred = False,\n        mask = None,\n        return_mems = False,\n        return_attn = False,\n        mems = None,\n        mem_masks = None,\n        recycle_steps = None,\n        pos = None,\n        prepend_embeds = None,\n        prepend_mask = None,\n        embed_ids: dict[str, Tensor] = dict(),\n        sum_embeds = None,\n        return_attn_z_loss = False,\n        attn_z_loss_weight = 1e-4,\n        seq_start_pos = None,\n        cache: LayerIntermediates | None = None,\n        input_not_include_cache = False,\n        token_emb_kwargs = dict(),\n        to_logits_kwargs = dict(),\n        **kwargs,\n    ):\n\n        # if sequence is None, auto create an empty one if `prepend_embeds` was supplied\n\n        if not exists(x):\n            assert exists(prepend_embeds)\n            x = prepend_embeds.new_empty((prepend_embeds.shape[0], 0), dtype = torch.long)\n\n        # shapes and variables\n\n        b, n, device, token_ids, num_mems, has_memory_tokens, emb_frac_gradient, orig_mask = x.shape[0], x.shape[1], x.device, x, self.num_memory_tokens, self.num_memory_tokens > 0, self.emb_frac_gradient, mask\n\n        return_hiddens = return_mems | return_attn | return_intermediates | return_attn_z_loss | return_embeddings_and_intermediates\n        return_embeddings = return_embeddings | (not exists(self.to_logits)) | return_embeddings_and_intermediates\n\n        # take care of position embedding offsets in the presence of cache and sequence is less than cache length (not full sequence)\n\n        seq_pos_offset = 0\n\n        if exists(cache) and input_not_include_cache:\n            seq_pos_offset = cache.cache_length\n\n        # absolute positional embedding\n\n        external_pos_emb = exists(pos) and pos.dtype != torch.long\n        pos_emb = self.pos_emb(x, pos = pos, seq_start_pos = seq_start_pos, offset = seq_pos_offset) if not external_pos_emb else pos\n        x = self.token_emb(x, **token_emb_kwargs) + pos_emb\n\n        # add additional embeddings\n\n        assert not (exists(self.embeds) ^ (len(embed_ids) > 0)), '`embed_num_tokens` must be defined on `TransformerWrapper`'\n\n        if exists(self.embeds):\n            assert len(embed_ids) == len(self.embeds)\n\n            for name, embed_id in embed_ids.items():\n                embed_key = f'{name}_embed'\n\n                assert embed_key in self.embeds\n                embed = self.embeds[embed_key](embed_id)\n\n                x = x + embed\n\n        # for summing embeddings passed externally - needs this for self-conditioning in non-autoregressive training\n\n        if exists(sum_embeds):\n            x = x + sum_embeds\n\n        # post embedding norm, purportedly leads to greater stabilization\n\n        x = self.post_emb_norm(x)\n\n        # whether to append embeds, as in PaLI, for image embeddings\n\n        if exists(prepend_embeds):\n            prepend_seq, prepend_dim = prepend_embeds.shape[1:]\n            assert prepend_dim == x.shape[-1], 'prepended embeddings need to have same dimensions as text model dimensions'\n\n            x = cat((prepend_embeds, x), dim = -2)\n\n            if exists(prepend_mask) or exists(mask):\n                mask = default(mask, lambda: torch.ones((b, n), device = device, dtype = torch.bool))\n                prepend_mask = default(prepend_mask, lambda: torch.ones((b, prepend_seq), device = device, dtype = torch.bool))\n\n                mask = cat((prepend_mask, mask), dim = -1)\n\n        # whether to reduce the gradient going to the embedding, from cogview paper, corroborated by GLM-130B model\n\n        if emb_frac_gradient < 1:\n            assert emb_frac_gradient > 0\n            x = x * emb_frac_gradient + x.detach() * (1 - emb_frac_gradient)\n\n        # init embed\n\n        init_embed = x\n\n        # embedding dropout\n\n        x = self.emb_dropout(x)\n\n        x = self.project_emb(x)\n\n        # maybe deep embeds\n\n        deep_embed_and_ids = None\n\n        if exists(self.ff_deep_embed):\n            deep_embed_and_ids = (self.ff_deep_embed, token_ids)\n\n        # maybe cls token\n\n        if exists(self.cls_token):\n            cls_tokens = repeat(self.cls_token, '... -> b ...', b = b)\n            x, cls_packed_shape = pack([cls_tokens, x], 'b * d')\n\n            if exists(mask):\n                mask = F.pad(mask, (self.num_cls_tokens, 0), value = True)\n\n        # maybe memory / register tokens\n\n        if has_memory_tokens:\n            mem_seq = x.shape[-2]\n            mem_every = self.memory_tokens_interspersed_every\n\n            if exists(mem_every):\n                assert mem_every > 0\n                assert isinstance(self.attn_layers, Decoder), 'only for decoder'\n                next_seq_len = math.ceil(n / mem_every) * mem_every\n\n                x = pad_at_dim(x, (0, next_seq_len - n), dim = -2, value = 0.)\n                x = rearrange(x, 'b (n m) d -> (b n) m d', m = mem_every)\n\n            mem = repeat(self.memory_tokens, 'n d -> b n d', b = x.shape[0])\n            x, mem_packed_shape = pack((mem, x), 'b * d')\n\n            # auto-handle masking after appending memory tokens\n            if not exists(mem_every) and exists(mask):\n                mask = pad_at_dim(mask, (num_mems, 0), dim = -1, value = True)\n\n            if exists(mem_every):\n                x = rearrange(x, '(b n) m d -> b (n m) d', b = b)\n\n        # handle maybe shifting of memories\n\n        if self.shift_mem_down and exists(mems):\n            mems_l, mems_r = mems[:self.shift_mem_down], mems[self.shift_mem_down:]\n            mems = [*mems_r, *mems_l]\n\n        # attn layers kwargs\n\n        kwargs = dict(\n            **kwargs,\n            pos = pos,\n            seq_pos_offset = seq_pos_offset,\n            seq_start_pos = seq_start_pos,\n            input_not_include_cache = input_not_include_cache\n        )\n\n        # attention layers\n\n        if not self.recycling:\n            assert not exists(recycle_steps) or recycle_steps == 1, 'you did not train with recycling'\n\n            # regular\n\n            attended, intermediates = self.attn_layers(x, mask = mask, mems = mems, mem_masks = mem_masks, cache = cache, deep_embeds_and_ids = deep_embed_and_ids, return_hiddens = True, **kwargs)\n\n        else:\n            # recycling\n\n            recycle_steps = default(recycle_steps, (randrange(self.train_max_recycle_steps) + 1) if self.training else None)\n            assert exists(recycle_steps) and recycle_steps > 0, '`recycle_steps` must be provided on forward if recycling is turned on and not training'\n\n            for i in range(recycle_steps):\n                first_step = i == 0\n                last_step = i == (recycle_steps - 1)\n\n                context = nullcontext if last_step else torch.no_grad\n\n                with context():\n                    maybe_recycled = self.recycled_proj(attended.detach()) if not first_step else 0.\n\n                    attended, intermediates = self.attn_layers(x + maybe_recycled, mask = mask, mems = mems, mem_masks = mem_masks, cache = cache, return_hiddens = True, **kwargs)\n\n        x = attended\n\n        # handle memories post-attention\n\n        if has_memory_tokens:\n            if exists(mem_every):\n                x = rearrange(x, 'b (n m) d -> (b n) m d', m = (mem_every + num_mems))\n\n            mem, x = unpack(x, mem_packed_shape, 'b * d')\n\n            intermediates.memory_tokens = mem\n\n            if exists(mem_every):\n                x = rearrange(x, '(b n) m d -> b (n m) d', b = b)\n\n            x = x[:, :mem_seq]\n\n        # store last layer hiddens, for access in case of cls token or attention pooling\n\n        intermediates.last_layer_hiddens = x\n\n        # store initial embed\n\n        intermediates.initial_embed = init_embed\n\n        # global average pool\n\n        if self.average_pool_embed:\n            x = masked_mean(x, mask = orig_mask, dim = 1)\n\n        # cls token(s)\n\n        if exists(self.cls_token):\n            x, last_layer_hiddens = unpack(x, cls_packed_shape, 'b * d')\n\n            intermediates.last_layer_hiddens = last_layer_hiddens\n\n            if x.shape[1] == 1:\n                x = rearrange(x, 'b 1 d -> b d')  # Remove sequence dimension if num_cls_tokens=1 to keep previous behavior\n\n        # attention pool\n\n        is_encoder = not self.attn_layers.causal\n        return_pooled_tokens = exists(self.attn_pool) and is_encoder\n\n        if (\n            exists(self.attn_pool) and\n            (return_intermediates or is_encoder) # in a new paper, they use attention pooling on decoder - so we'll default to returning pooled tokens if encoder, but for decoder, they must set `return_intermediates`\n        ):\n\n            attn_pooled_tokens = self.attn_pool(x, mask = mask)\n\n            intermediates.attn_pooled_tokens = attn_pooled_tokens\n\n        # handle expansion to mixture if needed (for mixture of softmax)\n\n        combine_mixture = None\n\n        if exists(self.to_mixture):\n            combine_mixture = self.combine_mixture(x).softmax(dim = -1)\n            x = self.to_mixture(x)\n\n        # projecting to logits\n\n        if not return_embeddings:\n            if self.has_multiple_heads:\n                logits = tuple(fn(x, **to_logits_kwargs) for fn in self.to_logits)\n            else:\n                logits = self.to_logits(x, **to_logits_kwargs)\n\n        # maybe sig softmax\n\n        if self.sigsoftmax_logits:\n            logits = logits + logits.sigmoid().log()\n\n        # handle maybe combine mixture\n\n        if exists(combine_mixture):\n            with autocast('cuda', enabled = False):\n                prob = logits.softmax(dim = -1)\n                mos = einsum('... k d, ... k -> ... d', prob, combine_mixture)\n                logits = log(mos)\n\n        # maybe squeeze out last dimension of logits\n\n        if self.squeeze_out_last_dim:\n            logits = tuple((rearrange(t, '... 1 -> ...') if t.shape[-1] == 1 else t) for t in cast_tuple(logits))\n\n            if not self.has_multiple_heads:\n                logits = first(logits)\n\n        # different returns\n\n        if return_logits_and_embeddings:\n            out = (logits, x)\n        elif return_embeddings_and_intermediates:\n            out = (x, intermediates)\n        elif return_embeddings:\n            out = x\n        elif return_pooled_tokens:\n            intermediates.logits = logits\n            out = attn_pooled_tokens\n        else:\n            out = logits\n\n        # maybe next embed pred\n\n        if return_next_embed_pred:\n            assert self.add_continuous_pred_head\n            next_embed_out = self.to_next_embed_pred(x)\n\n            out = (out, (next_embed_out, init_embed))\n\n        # logit entropies\n\n        if return_logit_entropies:\n            intermediates.logit_entropies = calc_entropy(logits)\n            return_intermediates = True\n\n        # aux loss\n\n        if return_attn_z_loss:\n            pre_softmax_attns = [t.pre_softmax_attn for t in  intermediates.attn_intermediates]\n            intermediates.attn_z_loss = calc_z_loss(pre_softmax_attns, weight = attn_z_loss_weight)\n            return_intermediates = True\n\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = [cat(pair, dim = -2) for pair in zip(mems, hiddens)] if exists(mems) else hiddens\n            new_mems = [t[..., -self.max_mem_len:, :].detach() for t in new_mems]\n\n            if not return_intermediates:\n                return out, new_mems\n\n            intermediates.mems = new_mems\n\n        if return_intermediates:\n            return out, intermediates\n\n        if return_attn:\n            attn_maps = [t.post_softmax_attn for t in intermediates.attn_intermediates]\n            return out, attn_maps\n\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "TransformerWrapper", "line": 3123}}
{"prompt": "Create a x transformer neural network module", "code": "class XTransformer(Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        tie_token_emb = False,\n        ignore_index = -100,\n        pad_value = 0,\n        cross_attn_tokens_dropout = 0.,\n        **kwargs\n    ):\n        super().__init__()\n        enc_kwargs, kwargs = groupby_prefix_and_trim('enc_', kwargs)\n        dec_kwargs, kwargs = groupby_prefix_and_trim('dec_', kwargs)\n\n        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs, 'dimension of either encoder or decoder must be set with `dim` keyword'\n        enc_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], enc_kwargs)\n        enc_transformer_kwargs['emb_dropout'] = enc_kwargs.pop('emb_dropout', 0)\n        enc_transformer_kwargs['num_memory_tokens'] = enc_kwargs.pop('num_memory_tokens', None)\n        enc_transformer_kwargs['scaled_sinu_pos_emb'] = enc_kwargs.pop('scaled_sinu_pos_emb', False)\n        enc_transformer_kwargs['use_abs_pos_emb'] = enc_kwargs.pop('use_abs_pos_emb', True)\n\n        dec_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], dec_kwargs)\n        dec_transformer_kwargs['emb_dropout'] = dec_kwargs.pop('emb_dropout', 0)\n        dec_transformer_kwargs['scaled_sinu_pos_emb'] = dec_kwargs.pop('scaled_sinu_pos_emb', False)\n        dec_transformer_kwargs['use_abs_pos_emb'] = dec_kwargs.pop('use_abs_pos_emb', True)\n\n        self.cross_attn_tokens_dropout = cross_attn_tokens_dropout  # how many tokens from the encoder to dropout when cross attending from decoder - seen in a couple papers, including Perceiver AR - this will also be very effective regularization when cross attending to very long memories\n\n        self.encoder = TransformerWrapper(\n            **enc_transformer_kwargs,\n            return_only_embed = True,\n            attn_layers = Encoder(dim = dim, **enc_kwargs)\n        )\n\n        self.decoder = TransformerWrapper(\n            **dec_transformer_kwargs,\n            attn_layers = Decoder(dim = dim, cross_attend = True, **dec_kwargs)\n        )\n\n        if tie_token_emb:\n            self.decoder.token_emb = self.encoder.token_emb\n\n        self.decoder = AutoregressiveWrapper(self.decoder, ignore_index=ignore_index, pad_value=pad_value)\n\n    @torch.no_grad()\n    def generate(self, seq_in, seq_out_start, seq_len, mask = None, attn_mask = None, **kwargs):\n        encodings = self.encoder(seq_in, mask = mask, attn_mask = attn_mask, return_embeddings = True)\n        return self.decoder.generate(seq_out_start, seq_len, context = encodings, context_mask = mask, **kwargs)\n\n    def forward(self, src, tgt, mask = None, attn_mask = None, src_prepend_embeds = None):\n\n        enc = self.encoder(src, mask = mask, attn_mask = attn_mask, prepend_embeds = src_prepend_embeds, return_embeddings = True)\n\n        if exists(src_prepend_embeds) and exists(mask):\n            mask = pad_at_dim(mask, (src_prepend_embeds.shape[-2], 0), dim = -1, value = True)\n\n        if self.training and self.cross_attn_tokens_dropout > 0:\n            enc, mask = dropout_seq(enc, mask, self.cross_attn_tokens_dropout)\n\n        out = self.decoder(tgt, context = enc, context_mask = mask)\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "XTransformer", "line": 3680}}
