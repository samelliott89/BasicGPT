{"prompt": "Create a binary mapper neural network module", "code": "class BinaryMapper(Module):\n    def __init__(\n        self,\n        bits = 1,\n        kl_loss_threshold = NAT # 1 bit\n    ):\n        super().__init__()\n\n        self.bits = bits\n        self.num_codes = 2 ** bits\n\n        power_two = 2 ** arange(bits)\n        codes = (arange(self.num_codes)[:, None].bitwise_and(power_two) != 0).byte().bool()\n\n        self.register_buffer('power_two', power_two, persistent = False)\n        self.register_buffer('codes', codes, persistent = False)\n\n        # aux loss\n\n        self.kl_loss_threshold = kl_loss_threshold\n        self.register_buffer('zero', tensor(0.), persistent = False)\n\n    def forward(\n        self,\n        logits,\n        temperature = 1.,\n        straight_through = None,\n        calc_aux_loss = None\n    ):\n        straight_through = default(straight_through, self.training)\n        calc_aux_loss = default(calc_aux_loss, self.training)\n\n        assert logits.shape[-1] == self.bits, f'logits must have a last dimension of {self.bits}'\n\n        # temperature and prob for sampling\n\n        prob_for_sample = (logits / temperature).sigmoid()\n\n        # sampling\n\n        sampled_bits = (torch.rand_like(logits) <= prob_for_sample).long()\n        indices = (self.power_two * sampled_bits).sum(dim = -1)\n\n        one_hot = F.one_hot(indices, self.num_codes).float()\n\n        # maybe calculate aux loss\n\n        aux_kl_loss = self.zero\n\n        if calc_aux_loss:\n            # calculate negative entropy\n\n            kl_div = self.bits * NAT - binary_entropy(logits)\n            aux_kl_loss = F.relu(kl_div - self.kl_loss_threshold).mean()\n\n        # maybe straight through\n\n        if straight_through:\n            # get the soft G for the gradients and do a straight through\n\n            soft_G = (\n                einsum(F.logsigmoid(logits), self.codes.float(), '... bits, codes bits -> ... codes') +\n                einsum(F.logsigmoid(-logits), (~self.codes).float(), '... bits, codes bits -> ... codes')\n            ).exp()\n\n            # straight through\n\n            one_hot = one_hot + soft_G - soft_G.detach()\n\n        return one_hot, aux_kl_loss", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/free_transformer.py", "name": "BinaryMapper", "line": 59}}
{"prompt": "Create a free transformer neural network module", "code": "class FreeTransformer(Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        dec_head_depth,\n        dec_tail_depth,\n        max_seq_len,\n        enc_depth = 1,\n        dim_latent = None,\n        attn_dim_head = 64,\n        heads = 8,\n        latent_bits = 16,\n        per_token_latents = True,  # they use a latent per token in the sequence, instead of one for entire sequence, iiuc\n        kl_loss_threshold = NAT,\n        binary_mapper_kwargs: dict = dict(),\n        enc_kwargs: dict = dict(),\n        dec_kwargs: dict = dict(),\n        kl_loss_weight = 1.,\n        latent_dropout_prob = 0.,\n        pad_id = -1,\n        **kwargs\n    ):\n        super().__init__()\n        dim_latent = default(dim_latent, dim)\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n\n        self.token_unembed = nn.Linear(dim, num_tokens, bias = False)\n\n        self.query_token_for_latents = nn.Parameter(torch.randn(dim) * 1e-2)\n\n        self.per_token_latents = per_token_latents\n\n        self.encoder = Encoder(\n            dim = dim,\n            depth = enc_depth,\n            attn_dim_head = attn_dim_head,\n            heads = heads,\n            only_cross = True,\n            cross_attend = True,\n            use_rmsnorm = True,\n            rotary_pos_emb = True,\n            pre_norm_has_final_norm = True,\n            **kwargs,\n            **enc_kwargs\n        )\n\n        self.to_latent_bit_logits = nn.Linear(dim, latent_bits, bias = False)\n\n        self.binary_mapper = BinaryMapper(\n            latent_bits,\n            kl_loss_threshold,\n            **binary_mapper_kwargs\n        )\n\n        self.from_latent_to_condition = nn.Linear(self.binary_mapper.num_codes, dim, bias = False)\n\n        self.latent_dropout = nn.Dropout(latent_dropout_prob)\n\n        self.decoder_head = Decoder(\n            dim = dim,\n            depth = dec_head_depth,\n            attn_dim_head = attn_dim_head,\n            heads = heads,\n            rotary_pos_emb = True,\n            use_rmsnorm = True,\n            pre_norm_has_final_norm = False,\n            **kwargs,\n            **dec_kwargs\n        ) if dec_head_depth > 0 else None\n\n        assert dec_tail_depth > 0\n\n        self.decoder_tail = Decoder(\n            dim = dim,\n            depth = dec_tail_depth,\n            attn_dim_head = attn_dim_head,\n            heads = heads,\n            rotary_pos_emb = True,\n            use_rmsnorm = True,\n            pre_norm_has_final_norm = True,\n            **kwargs,\n            **dec_kwargs\n        )\n\n        self.pad_id = pad_id\n\n        self.kl_loss_weight = kl_loss_weight\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def encode_to_latents(\n        self,\n        decoder_head_embeds,\n        mask = None,\n        return_kl_loss = False,\n        per_token_latents = None\n    ):\n        per_token_latents = default(per_token_latents, self.per_token_latents)\n\n        batch, seq_len, device = *decoder_head_embeds.shape[:2], decoder_head_embeds.device\n\n        query_tokens = repeat(self.query_token_for_latents, 'd -> b 1 d', b = batch)\n\n        encoder_kwargs = dict()\n\n        # handle the interesting per query token latents, as in the paper\n\n        if per_token_latents:\n            query_tokens = repeat(query_tokens, 'b 1 d -> b n d', n = seq_len)\n\n            rotary_pos = torch.arange(seq_len, device = device)\n\n            encoder_kwargs.update(\n                pos = rotary_pos,\n                context_pos = rotary_pos\n            )\n\n        pooled = self.encoder(\n            query_tokens,\n            context = decoder_head_embeds,\n            context_mask = mask,\n            **encoder_kwargs\n        )\n\n        bit_logits = self.to_latent_bit_logits(pooled)\n\n        one_hot_latents, kl_loss = self.binary_mapper(bit_logits, calc_aux_loss = return_kl_loss)\n\n        if not return_kl_loss:\n            return one_hot_latents\n\n        return one_hot_latents, kl_loss\n\n    @torch.no_grad()\n    def generate(\n        self,\n        prompts,\n        seq_len,\n        latents = None,\n        filter_logits_fn = top_p,\n        logit_filter_kwargs: dict = dict(thres = 0.9),\n        use_kv_cache = True\n    ):\n        prompts, inverse_pack = pack_with_inverse(prompts, '* n')\n\n        batch = prompts.shape[0]\n\n        # prepend embeds\n\n        condition = None\n        if exists(latents):\n            if not is_tensor(latents):\n                latents = tensor(latents, device = self.device)\n\n            if latents.dtype in (torch.int, torch.long):\n                # if given as indices\n                latents = F.one_hot(latents, self.binary_mapper.num_codes).float()\n\n            if latents.ndim == 1: # repeat latents\n                latents = repeat(latents, 'd -> b 1 d', b = batch)\n            elif latents.ndim == 2:\n                latents = rearrange(latents, 'b d -> b 1 d')\n\n            condition = self.from_latent_to_condition(latents)\n\n        # kv cache\n\n        head_cache = tail_cache = None\n\n        # generated\n\n        prompt_len = prompts.shape[-1]\n\n        generated = prompts\n\n        tokens = self.token_emb(generated)\n\n        for _ in range(max(0, seq_len - prompt_len)):\n\n            # head, which may not exist\n\n            if exists(self.decoder_head):\n                head_embed, next_head_cache = self.decoder_head(tokens, cache = head_cache, return_hiddens = True)\n            else:\n                head_embed, next_head_cache = tokens, None\n\n            # handle one token being given to the decoder tail when doing kv caching - rotary embedding needs to know the seq position offset\n\n            seq_pos_offset = head_cache.cache_length if exists(head_cache) else 0\n\n            # tail\n\n            tail_embed, next_tail_cache = self.decoder_tail(head_embed, cache = tail_cache, seq_pos_offset = seq_pos_offset, self_attn_kv_residuals = condition, return_hiddens = True)\n\n            tail_embed = tail_embed[:, -1]\n\n            logits = self.token_unembed(tail_embed)\n\n            logits = filter_logits_fn(logits, **logit_filter_kwargs)\n\n            sampled = gumbel_sample(logits)\n\n            generated, _ = pack((generated, sampled), 'b *')\n            tokens, _ = pack((tokens, self.token_emb(sampled)), 'b * d')\n\n            if use_kv_cache:\n                head_cache = next_head_cache\n                tail_cache = next_tail_cache\n\n        return inverse_pack(generated)\n\n    def forward(\n        self,\n        seq,\n        seq_for_latents = None,\n        return_all_losses = False\n    ):\n        batch, device = seq.shape[0], seq.device\n\n        seq, labels = seq[:, :-1], seq[:, 1:]\n\n\n        tokens = self.token_emb(seq)\n\n        # decoder head\n\n        if exists(self.decoder_head):\n            tokens = self.decoder_head(tokens)\n\n        # determine whether to use a separate sequence for encoding latents\n\n        if exists(seq_for_latents):\n            tokens_for_latents = self.token_emb(seq_for_latents)\n\n            if exists(self.decoder_head):\n                tokens_for_latents = self.decoder_head(tokens_for_latents)\n\n            encoder_mask = seq_for_latents != self.pad_id\n            per_token_latents = False\n        else:\n\n            tokens_for_latents = tokens\n            encoder_mask = seq != self.pad_id\n            per_token_latents = None\n\n        # get latent Z\n\n        latents, kl_loss = self.encode_to_latents(tokens_for_latents, mask = encoder_mask, per_token_latents = per_token_latents, return_kl_loss = True)\n\n        latents = self.latent_dropout(latents)\n\n        condition = self.from_latent_to_condition(latents)\n\n        # decoder tail\n\n        tokens = self.decoder_tail(tokens, self_attn_kv_residuals = condition)\n\n        # cross entropy loss\n\n        logits = self.token_unembed(tokens)\n\n        ar_loss = F.cross_entropy(\n            rearrange(logits, 'b n l -> b l n'),\n            labels,\n            ignore_index = self.pad_id\n        )\n\n        # return losses\n\n        total_loss = (\n            ar_loss +\n            kl_loss * self.kl_loss_weight\n        )\n\n        if not return_all_losses:\n            return total_loss\n\n        losses = (ar_loss, kl_loss)\n\n        return total_loss, losses", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/free_transformer.py", "name": "FreeTransformer", "line": 132}}
