{"prompt": "Create a x val transformer wrapper neural network module", "code": "class XValTransformerWrapper(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        max_seq_len,\n        numerical_token_id,\n        attn_layers: AttentionLayers,\n        emb_dim = None,\n        logits_dim = None,\n        tie_embedding = False,\n        max_mem_len = 0,\n        num_memory_tokens = None,\n        emb_dropout = 0.,\n        use_abs_pos_emb = True,\n        scaled_sinu_pos_emb = False\n    ):\n        super().__init__()\n        dim = attn_layers.dim\n        emb_dim = default(emb_dim, dim)\n\n        self.emb_dim = emb_dim\n        self.token_emb = TokenEmbedding(emb_dim, num_tokens)\n\n        self.numerical_token_id = numerical_token_id\n\n        self.max_seq_len = max_seq_len\n\n        self.max_mem_len = max_mem_len\n\n        if not (use_abs_pos_emb and not attn_layers.disable_abs_pos_emb):\n            self.pos_emb = always(0)\n        elif scaled_sinu_pos_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(dim)\n        else:\n            self.pos_emb = AbsolutePositionalEmbedding(dim, max_seq_len)\n\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        # memory tokens\n\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.has_memory_tokens = num_memory_tokens > 0\n\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n\n        # attention layers\n\n        self.attn_layers = attn_layers\n\n        # to logits\n\n        logits_dim = default(logits_dim, num_tokens)\n        self.to_logits = nn.Linear(dim, logits_dim) if not tie_embedding else lambda t: t @ self.token_emb.emb.weight.t()\n\n        self.to_numerical_output = nn.Sequential(\n            nn.Linear(dim, 1),\n            Rearrange('... 1 -> ...')\n        )\n\n    def forward(\n        self,\n        x: Tensor,\n        x_num: Tensor,\n        return_embeddings = False,\n        return_intermediates = False,\n        return_mems = False,\n        mask = None,\n        return_attn = False,\n        mems = None,\n        pos = None,\n        prepend_embeds = None,\n        **kwargs\n    ):\n        assert x.shape == x_num.shape\n\n        batch = x.shape[0]\n\n        is_number_mask = x == self.numerical_token_id\n\n        x = self.token_emb(x)\n\n        scale = torch.where(is_number_mask, x_num, 1.)\n        scale = rearrange(scale, '... -> ... 1')\n\n        x = x * scale\n\n        x = x + self.pos_emb(x, pos = pos)\n\n        # memory tokens\n\n        if self.has_memory_tokens:\n            m = repeat(self.memory_tokens, 'm d -> b m d', b = batch)\n            x, mem_ps = pack([m, x], 'b * d')\n\n            if exists(mask):\n                num_mems = m.shape[-2]\n                mask = pad_at_dim(mask, (num_mems, 0), dim = -1, value = True)\n\n        # whether to append embeds, as in PaLI, for image embeddings\n\n        if exists(prepend_embeds):\n            _, prepend_dim = prepend_embeds.shape[1:]\n            assert prepend_dim == x.shape[-1], 'prepended embeddings need to have same dimensions as model dimensions'\n\n            x = torch.cat((prepend_embeds, x), dim = -2)\n\n        x = self.emb_dropout(x)\n\n        # attention layers\n\n        x, intermediates = self.attn_layers(x, mask = mask, mems = mems, return_hiddens = True, **kwargs)\n\n        # splice out memory tokens\n\n        if self.has_memory_tokens:\n            m, x = unpack(x, mem_ps, 'b * d')\n            intermediates.memory_tokens = m\n\n        if not return_embeddings:\n            logits = self.to_logits(x)\n            numerical_pred = self.to_numerical_output(x)\n            out = (logits, numerical_pred)\n        else:\n            out = x\n\n        if return_intermediates:\n            return out, intermediates\n\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = tuple(t[..., -self.max_mem_len:, :].detach() for t in hiddens)\n            return out, new_mems\n\n        if return_attn:\n            attn_maps = tuple(t.post_softmax_attn for t in intermediates.attn_intermediates)\n            return out, attn_maps\n\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/xval.py", "name": "XValTransformerWrapper", "line": 48}}
{"prompt": "Create a x val autoregressive wrapper neural network module", "code": "class XValAutoregressiveWrapper(nn.Module):\n    def __init__(\n        self,\n        net: XValTransformerWrapper,\n        ignore_index = -100,\n        pad_value = 0,\n        numerical_loss_weight = 1.\n    ):\n        super().__init__()\n        self.net = net\n        self.max_seq_len = net.max_seq_len\n        self.numerical_loss_weight = numerical_loss_weight\n        self.ignore_index = ignore_index\n\n    @torch.no_grad()\n    def generate(\n        self,\n        start_tokens: Tensor,\n        start_numbers: Tensor,\n        seq_len,\n        filter_logits_fn: Callable = top_k,\n        filter_kwargs: dict = dict(),\n        temperature = 1.,\n        **kwargs\n    ):\n        device = start_tokens.device\n        was_training = self.net.training\n        num_dims = len(start_tokens.shape)\n\n        assert num_dims >= 2, 'number of dimensions of your start tokens must be greater or equal to 2'\n        assert start_tokens.shape == start_numbers.shape\n\n        b, t, device = *start_tokens.shape, start_tokens.device\n\n        self.net.eval()\n        out = start_tokens\n        num_out = start_numbers\n\n        for _ in range(seq_len):\n            x = out[:, -self.max_seq_len:]\n            x_num = num_out[:, -self.max_seq_len:]\n\n            logits, numerical_pred = self.net(x, x_num, **kwargs)\n\n            last_logits = logits[:, -1]\n            last_num_pred = numerical_pred[:, -1:]\n\n            filtered_logits = filter_logits_fn(last_logits, **filter_kwargs)\n\n            probs = F.softmax(filtered_logits / temperature, dim=-1)\n\n            sample = torch.multinomial(probs, 1)\n\n            out = torch.cat((out, sample), dim = -1)\n            num_out = torch.cat((num_out, last_num_pred), dim = -1)\n\n        out = out[:, t:]\n        num_out = num_out[:, t:]\n\n        is_number = out == self.net.numerical_token_id\n        num_out = torch.where(is_number, num_out, float('nan'))\n\n        self.net.train(was_training)\n        return GenerateReturn(out, num_out, is_number)\n\n    def forward(\n        self,\n        x: Tensor,\n        x_num: Tensor,\n        return_loss_breakdown = False,\n        **kwargs\n    ):\n        inp, target = x[:, :-1], x[:, 1:]\n        x_num_inp, x_num_target = x_num[:, :-1], x_num[:, 1:]\n\n        # ignore index\n\n        target_mask = target != self.ignore_index\n\n        # key padding mask\n\n        mask = kwargs.get('mask', None)\n        if exists(mask):\n            target_mask &= mask\n\n            if mask.shape[1] == x.shape[1]:\n                mask = mask[:, :-1]\n                kwargs['mask'] = mask\n\n        logits, numerical_pred = self.net(inp, x_num_inp, **kwargs)\n\n        logits = rearrange(logits, 'b n c -> b c n')\n\n        cross_entropy_loss = F.cross_entropy(logits, target, reduction = 'none', ignore_index = self.ignore_index)\n\n        # protect against nan in `x_num` input tensor\n\n        target_is_number_mask = target == self.net.numerical_token_id\n        x_num_target = x_num_target.masked_fill(~target_is_number_mask, 0.)\n\n        # numerical mse loss\n\n        numerical_mse_loss = F.mse_loss(numerical_pred, x_num_target, reduction = 'none')\n\n        numerical_mse_loss = numerical_mse_loss * target_mask\n        numerical_mse_loss = numerical_mse_loss.masked_fill(~target_is_number_mask, 0.)\n\n        # combine losses\n\n        loss = cross_entropy_loss + numerical_mse_loss * self.numerical_loss_weight\n\n        loss = loss[target_mask]\n        loss = loss.mean()\n\n        if not return_loss_breakdown:\n            return loss\n\n        return loss, LossBreakdown(cross_entropy_loss, numerical_mse_loss)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/xval.py", "name": "XValAutoregressiveWrapper", "line": 189}}
