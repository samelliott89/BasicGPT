{"prompt": "Create a xl autoregressive wrapper neural network module", "code": "class XLAutoregressiveWrapper(nn.Module):\n    def __init__(\n        self,\n        net,\n        ignore_index = -100,\n        pad_value = 0\n    ):\n        super().__init__()\n        self.pad_value = pad_value\n        self.ignore_index = ignore_index\n\n        self.net = net\n        self.max_seq_len = net.max_seq_len\n\n    @torch.no_grad()\n    @eval_decorator\n    def generate(\n        self,\n        start_tokens,\n        seq_len,\n        eos_token = None,\n        temperature = 1.,\n        filter_logits_fn = top_k,\n        filter_kwargs: dict = dict(),\n        mems = None,\n        **kwargs\n    ):\n        device, max_seq_len = start_tokens.device, self.max_seq_len\n\n        start_tokens, ps = pack([start_tokens], '* n')\n\n        b, t = start_tokens.shape\n\n        *all_leading_tokens, _ = start_tokens.split(max_seq_len, dim = -1)\n\n        # catch the memory up to the current segment\n\n        for leading_tokens in all_leading_tokens:\n            _, mems = self.net(\n                leading_tokens,\n                mems = mems,\n                return_mems = True,\n                **kwargs\n            )\n\n        # now start sampling from the current segment\n\n        curr_pos = len(all_leading_tokens) * max_seq_len\n        curr_mems = mems\n\n        cache = None\n        out = start_tokens\n\n        for _ in range(seq_len):\n            curr_segment_len = out.shape[-1]\n            is_last_segment_tokens = divisible_by(curr_segment_len, max_seq_len)\n\n            x = out[:, curr_pos:]\n\n            logits, cache = self.net(\n                x,\n                mems = curr_mems,\n                cache = cache,\n                return_mems = True,\n                return_intermediates = True,\n                **kwargs\n            )\n\n            mems = cache.mems\n\n            logits = logits[:, -1]\n            filtered_logits = filter_logits_fn(logits, **filter_kwargs)\n            probs = F.softmax(filtered_logits / temperature, dim=-1)\n\n            sample = torch.multinomial(probs, 1)\n\n            if is_last_segment_tokens:\n                curr_pos = curr_segment_len\n                curr_mems = mems\n\n            out = torch.cat((out, sample), dim=-1)\n\n            if exists(eos_token):\n                is_eos_tokens = (out == eos_token)\n\n                if is_eos_tokens.any(dim = -1).all():\n                    # mask out everything after the eos tokens\n                    shifted_is_eos_tokens = F.pad(is_eos_tokens, (1, -1))\n                    mask = shifted_is_eos_tokens.float().cumsum(dim = -1) >= 1\n                    out = out.masked_fill(mask, self.pad_value)\n                    break\n\n        out = out[:, t:]\n\n        out, = unpack(out, ps, '* n')\n\n        return out\n\n    def forward(\n        self,\n        x,\n        mems = None,\n        **kwargs\n    ):\n        ignore_index, max_seq_len = self.ignore_index, self.max_seq_len\n\n        x, labels = x[:, :-1], x[:, 1:]\n\n        seq_len = x.shape[1]\n\n        # prepare chunks\n\n        split_x = x.split(max_seq_len, dim = -1)\n        split_labels = labels.split(max_seq_len, dim = -1)\n        loss_weights = tuple((t.shape[-1] / seq_len) for t in split_x)\n\n        loss_fn = F.cross_entropy if not self.net.output_is_log_prob else F.nll_loss\n\n        # go through each chunk and derive weighted losses\n\n        total_loss = 0.        \n\n        for chunk, chunk_labels, loss_weight in zip(split_x, split_labels, loss_weights):\n\n            logits, mems = self.net(\n                chunk,\n                mems = mems,\n                return_mems = True,\n                **kwargs\n            )\n\n            loss = loss_fn(\n                rearrange(logits, 'b n c -> b c n'),\n                chunk_labels,\n                ignore_index = ignore_index\n            )\n\n            total_loss = total_loss + loss * loss_weight\n\n        return total_loss", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/xl_autoregressive_wrapper.py", "name": "XLAutoregressiveWrapper", "line": 20}}
