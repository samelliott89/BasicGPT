{"prompt": "Create a autoregressive wrapper neural network module", "code": "class AutoregressiveWrapper(Module):\n    def __init__(\n        self,\n        net,\n        ignore_index = -100,\n        pad_value = 0,\n        mask_prob = 0.,\n        add_attn_z_loss = False,\n        next_embed_loss_weight = 0.1\n    ):\n        super().__init__()\n        self.pad_value = pad_value\n        self.ignore_index = ignore_index\n\n        self.net = net\n        self.max_seq_len = net.max_seq_len\n\n        # paper shows masking (MLM) in conjunction with autoregressive decoder-only training leads to big improvements https://arxiv.org/abs/2210.13432\n        assert mask_prob < 1.\n        self.mask_prob = mask_prob\n\n        # whether to add router z-loss\n        self.add_attn_z_loss = add_attn_z_loss\n\n        # whether to add a continuous loss\n        self.add_continuous_pred_head = net.add_continuous_pred_head\n        self.next_embed_loss_weight = next_embed_loss_weight\n\n    @torch.no_grad()\n    @eval_decorator\n    def beam_search(\n        self,\n        prompts,\n        seq_len,\n        beams = 4,\n        return_beams_and_scores = False,\n        eos_token = None,\n        temperature = 1.,\n        stochastic = False,\n        prompt_lens: Tensor | None = None,\n        filter_logits_fn: str | Callable = identity,\n        restrict_to_max_seq_len = True,\n        filter_kwargs: dict = dict(),\n        cache_kv = True,\n        **kwargs\n    ):\n        assert not exists(eos_token), 'eos token not supported yet'\n\n        max_seq_len, greedy, device = self.max_seq_len, temperature == 0., prompts.device\n\n        prompts, packed_shape = pack([prompts], '* n')\n\n        batch, orig_seq_len = prompts.shape\n\n        # handle filter logits fn given as string\n\n        if isinstance(filter_logits_fn, str):\n            assert filter_logits_fn in FILTER_LOGITS_FN, f\"only {join(FILTER_LOGITS_FN.keys())} are available\"\n\n            filter_logits_fn = FILTER_LOGITS_FN[filter_logits_fn]\n\n        # handle variable lengthed prompts (prefixes)\n\n        seq_start_pos = None\n        if exists(prompt_lens):\n            prompts = align_right(prompts, prompt_lens, pad_id = self.pad_value)\n            seq_start_pos = orig_seq_len - prompt_lens\n\n        # output from which sampled tokens appended to\n\n        out = prompts\n\n        # kv caches\n\n        cache = None\n\n        should_cache = cache_kv and self.net.can_cache_kv\n\n        # scores for the beams\n\n        scores = torch.zeros((batch,), device = device)\n\n        batch_arange = torch.arange(batch, device = device)\n\n        # sampling up to seq_len\n\n        for i in range(seq_len):\n            is_first = i == 0\n\n            if restrict_to_max_seq_len:\n                max_len_exceeded = out.shape[-1] > max_seq_len\n\n                assert not (cache_kv and max_len_exceeded and not self.net.can_cache_kv_outside_max_seq_len), 'the network cannot use cached key values when decoding outside the max sequence length. most likely because you are using absolute positional embedding. you can switch to rotary embeddings to resolve this issue'\n\n                x = out[:, -max_seq_len:]\n\n                if exists(cache):\n                    modify_cached_kv(cache, lambda t: t[..., -(max_seq_len - 1):, :])\n\n            logits, new_cache = self.net(\n                x,\n                return_intermediates = True,\n                cache = cache,\n                seq_start_pos = seq_start_pos,\n                **kwargs\n            )\n\n            if should_cache:\n                cache = new_cache\n\n            logits = logits[:, -1]\n\n            # to add to the scores\n\n            log_probs = logits.log_softmax(dim = -1)\n\n            # maybe filter by top_k, top_p (nucleus) for stochastic beam search\n\n            if stochastic and not greedy:\n                logits = filter_logits_fn(logits, **filter_kwargs)\n                logits = (logits / temperature) + gumbel_noise(logits)\n\n            # (gumbel) topk\n\n            samples = logits.topk(beams, dim = -1).indices\n\n            # get the scores for keeping track of beams\n\n            next_scores = log_probs.gather(-1, samples)\n\n            # expand beam times\n\n            scores = repeat(scores, 'b -> b beams', beams = beams)\n            scores = scores + next_scores\n\n            out = repeat(out, 'b ... -> (b beams) ...', beams = beams)\n            samples = rearrange(samples, 'b beams -> (b beams) 1')\n\n            if should_cache and is_first:\n                modify_cached_kv(cache, lambda t: repeat(t, 'b ... -> (b beams) ...', beams = beams))\n\n            # concat sample\n\n            out = torch.cat((out, samples), dim=-1)\n\n            # sort by score and excise\n            # excise out the beams\n\n            scores = rearrange(scores, '(b prev_beams) next_beams -> b (prev_beams next_beams)', b = batch)\n            curr_num_beams = scores.shape[-1]\n\n            if curr_num_beams > beams:\n                scores, sort_indices = scores.sort(dim = -1, descending = True)\n\n                scores = scores[:, :beams]\n                top_beams_indices = sort_indices[:, :beams]\n\n                top_beams_indices = curr_num_beams * batch_arange[:, None] + top_beams_indices\n\n                flattened_beam_indices = rearrange(top_beams_indices, 'b beams -> (b beams)')\n\n                out = out[flattened_beam_indices]\n\n            scores = rearrange(scores, 'b beams -> (b beams)')\n\n            if not exists(eos_token):\n                continue\n\n            is_eos_tokens = (out == eos_token)\n\n            if is_eos_tokens.any(dim = -1).all():\n                break\n\n        if exists(eos_token):\n            # mask out everything after the eos tokens\n            shifted_is_eos_tokens = F.pad(is_eos_tokens, (1, -1))\n            mask = shifted_is_eos_tokens.float().cumsum(dim = -1) >= 1\n            out = out.masked_fill(mask, self.pad_value)\n\n        # select out the top beam\n\n        out = rearrange(out, '(b beams) seq -> b beams seq', b = batch)\n\n        out = out[..., orig_seq_len:]\n\n        out, = unpack(out, packed_shape, '* beams n') # prompt may have no batch dimension\n\n        if not return_beams_and_scores:\n            return out[..., 0, :]\n\n        scores = rearrange(scores, '(b beams) -> beams b', b = batch)\n        out = rearrange(out, 'b beams n -> beams b n')\n\n        return out, scores\n\n    @torch.no_grad()\n    @eval_decorator\n    def generate(\n        self,\n        prompts: list[Tensor] | Tensor,\n        seq_len,\n        eos_token = None,\n        temperature = 1.,\n        prompt_lens: Tensor | None = None,\n        filter_logits_fn: str | Callable = top_k,\n        restrict_to_max_seq_len = True,\n        amateur_model: Module | Tuple[Module] | None = None,\n        filter_kwargs: dict = dict(),\n        contrastive_decode_kwargs: dict | Tuple[dict] = dict(\n            beta = 0.5,\n            alpha = 0.1\n        ),\n        cache_kv = True,\n        **kwargs\n    ):\n        max_seq_len, greedy = self.max_seq_len, temperature == 0.\n\n        # handle prompts given as list of variable lengthed token ids\n\n        if isinstance(prompts, list):\n            assert len(prompts) > 0, 'prompts cannot be empty list'\n            assert not exists(prompt_lens), '`prompt_len` will be auto derived if prompts are passed in as list of Tensors'\n\n            prompt_lens = tensor([t.shape[0] for t in prompts], device = prompts[0].device)\n\n            prompts = pad_sequence(prompts, batch_first = True)\n\n        # pack maybe no batch\n\n        prompts, ps = pack([prompts], '* n')\n\n        b, t, device = *prompts.shape, prompts.device\n\n        # handle filter logits fn given as string\n\n        if isinstance(filter_logits_fn, str):\n            assert filter_logits_fn in FILTER_LOGITS_FN, f\"only {join(FILTER_LOGITS_FN.keys())} are available\"\n\n            filter_logits_fn = FILTER_LOGITS_FN[filter_logits_fn]\n\n        # handle variable lengthed prompts (prefixes)\n\n        seq_start_pos = None\n        if exists(prompt_lens):\n            prompts = align_right(prompts, prompt_lens, pad_id = self.pad_value)\n            seq_start_pos = t - prompt_lens\n\n        # output from which sampled tokens appended to\n\n        out = prompts\n\n        # kv caches\n\n        cache = None\n\n        # if doing contrastive decoding, turn off filter automatically\n\n        if exists(amateur_model):\n            amateur_model = cast_tuple(amateur_model)\n            contrastive_decode_kwargs = cast_tuple(contrastive_decode_kwargs)\n\n            assert len(amateur_model) == len(contrastive_decode_kwargs)\n\n            amateur_caches = [None] * len(amateur_model)\n            filter_logits_fn = identity\n\n            for i, module in enumerate(amateur_model):\n                if isinstance(module, AutoregressiveWrapper):\n                    amateur_model[i] = module.net\n\n                module.eval()\n\n        # sampling up to seq_len\n\n        for _ in range(seq_len):\n\n            if restrict_to_max_seq_len:\n                max_len_exceeded = out.shape[-1] > max_seq_len\n\n                assert not (cache_kv and max_len_exceeded and not self.net.can_cache_kv_outside_max_seq_len), 'the network cannot use cached key values when decoding outside the max sequence length. most likely because you are using absolute positional embedding. you can switch to rotary embeddings to resolve this issue'\n\n                x = out[:, -max_seq_len:]\n\n                if exists(cache):\n                    for inter in cache.attn_intermediates:\n                        if inter.layer_type == 'a':\n                            inter.cached_kv = [t[..., -(max_seq_len - 1):, :] for t in inter.cached_kv]\n\n            logits, new_cache = self.net(\n                x,\n                return_intermediates = True,\n                cache = cache,\n                seq_start_pos = seq_start_pos,\n                **kwargs\n            )\n\n            if cache_kv and self.net.can_cache_kv:\n                cache = new_cache\n\n            logits = logits[:, -1]\n\n            # handle contrastive decoding, Li et al.\n            # https://arxiv.org/abs/2210.15097\n\n            if exists(amateur_model):\n                for i, (amateur, amateur_cache, amateur_contrastive_decode_kwargs) in enumerate(zip(amateur_model, amateur_caches, contrastive_decode_kwargs)):\n                    amateur_logits, next_amateur_cache = amateur(\n                        x,\n                        return_intermediates = True,\n                        cache = amateur_cache,\n                        seq_start_pos = seq_start_pos,\n                        **kwargs\n                    )\n\n                    amateur_logits = amateur_logits[:, -1]\n\n                    assert amateur_logits.shape == logits.shape, 'logits dimension are not the same between amateur and expert model'\n                    logits = contrastive_decode_fn(logits, amateur_logits, **amateur_contrastive_decode_kwargs)\n\n                    if cache_kv and amateur.can_cache_kv:\n                        amateur_caches[i] = next_amateur_cache\n\n            # filter by top_k, top_p (nucleus), top_a, or custom\n\n            if greedy:\n                sample = logits.argmax(dim = -1, keepdim = True)\n            else:\n                filtered_logits = filter_logits_fn(logits, **filter_kwargs)\n                probs = F.softmax(filtered_logits / temperature, dim=-1)\n                sample = torch.multinomial(probs, 1)\n\n            # concat sample\n\n            out = torch.cat((out, sample), dim=-1)\n\n            if not exists(eos_token):\n                continue\n\n            is_eos_tokens = (out == eos_token)\n\n            if is_eos_tokens.any(dim = -1).all():\n                break\n\n        if exists(eos_token):\n            # mask out everything after the eos tokens\n            shifted_is_eos_tokens = F.pad(is_eos_tokens, (1, -1))\n            mask = shifted_is_eos_tokens.float().cumsum(dim = -1) >= 1\n            out = out.masked_fill(mask, self.pad_value)\n\n        out = out[:, t:]\n\n        out, = unpack(out, ps, '* n')\n\n        return out\n\n    def forward(\n        self,\n        x,\n        return_outputs = False,\n        prepend_embeds = None,\n        **kwargs\n    ):\n        seq, ignore_index, add_attn_z_loss, add_next_embed_loss = x.shape[1], self.ignore_index, self.add_attn_z_loss, self.add_continuous_pred_head\n\n        inp, target = x, x[:, 1:]\n        inp = torch.where(inp == ignore_index, self.pad_value, inp)\n\n        if self.mask_prob > 0.:\n            rand = torch.randn(inp.shape, device = x.device)\n            rand[:, 0] = -torch.finfo(rand.dtype).max # first token should not be masked out\n            num_mask = min(int(seq * self.mask_prob), seq - 1)\n            indices = rand.topk(num_mask, dim = -1).indices\n            mask = ~torch.zeros_like(inp).scatter(1, indices, 1.).bool()\n            kwargs.update(self_attn_kv_mask = mask)\n\n        out, cache = self.net(\n            inp,\n            return_intermediates = True,\n            return_attn_z_loss = add_attn_z_loss,\n            return_next_embed_pred = add_next_embed_loss,\n            prepend_embeds = prepend_embeds,\n            **kwargs\n        )\n\n        # destruct differently if doing continuous pred\n\n        if add_next_embed_loss:\n            logits, (next_embed_pred, init_embeds) = out\n        else:\n            logits = out\n\n        # if there are prepended embeds, excise it out\n\n        if exists(prepend_embeds):\n            prepend_len = prepend_embeds.shape[1]\n            logits = logits[:, prepend_len:]\n\n        # take all tokens but the last\n\n        logits = logits[:, :-1]\n\n        # loss function\n\n        loss_fn = F.cross_entropy if not self.net.output_is_log_prob else F.nll_loss\n\n        # cross entropy loss\n\n        loss = loss_fn(\n            rearrange(logits, 'b n c -> b c n'),\n            target,\n            ignore_index = ignore_index\n        )\n\n        if add_attn_z_loss:\n            loss = loss + cache.attn_z_loss\n\n        if add_next_embed_loss:\n            mask = target != ignore_index\n            embed_pred = next_embed_pred[:, :-1]\n            cont_targets = init_embeds[:, 1:].detach()\n\n            cont_loss = F.l1_loss(embed_pred, cont_targets, reduction = 'none')\n            cont_loss = cont_loss[mask].mean()\n\n            loss = loss + cont_loss * self.next_embed_loss_weight\n\n        if not return_outputs:\n            return loss\n\n        return loss, (logits, cache)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/autoregressive_wrapper.py", "name": "AutoregressiveWrapper", "line": 156}}
