{"prompt": "Create a attend neural network module", "code": "class Attend(Module):\n    def __init__(\n        self,\n        *,\n        dropout = 0.,\n        causal = False,\n        heads = None,\n        pre_talking_heads = False,\n        post_talking_heads = False,\n        pre_scale_post_talking_heads = False,\n        sparse_topk = None,\n        sparse_topk_straight_through = False, # https://arxiv.org/abs/2505.22074\n        scale = None,\n        qk_norm = False,\n        l2_distance = False,\n        sigmoid = False,\n        gumbel_softmax = False,\n        gumbel_softmax_temp = 1.,\n        gumbel_softmax_hard = True,\n        cog_signed = False,\n        custom_attn_fn: Callable | None = None,\n        flash = False,\n        softclamp_logits = False,\n        logit_softclamp_value = 50.,\n        add_zero_kv = False,\n        head_learned_sink = False,\n        selective = False,\n        hard = False,\n        cope = None,\n        onnxable = False,\n        sdp_kwargs: dict = dict(\n            enable_flash = True,\n            enable_math = True,\n            enable_mem_efficient = True\n        )\n    ):\n        super().__init__()\n        self.scale = scale\n\n        # causal related\n\n        self.causal = causal\n        self.create_causal_mask = onnx_create_causal_mask if onnxable else create_causal_mask\n\n        # attention type\n\n        is_sparse_topk_attn = exists(sparse_topk)\n\n        assert not (flash and sigmoid), 'sigmoid attention not available for flash'\n        assert not (flash and hard), 'hard attention not available for flash'\n        assert not (flash and is_sparse_topk_attn), 'topk attention not available for flash'\n\n        assert at_most_one_of(sigmoid, hard, l2_distance, gumbel_softmax, is_sparse_topk_attn)\n\n        if exists(custom_attn_fn):\n            self.attn_fn = custom_attn_fn\n        elif sigmoid:\n            self.attn_fn = F.sigmoid\n        elif hard:\n            self.attn_fn = one_hot_straight_through\n        elif is_sparse_topk_attn:\n            self.attn_fn = partial(sparse_topk_attn, sparse_topk = sparse_topk, straight_through = sparse_topk_straight_through)\n        elif gumbel_softmax:\n            self.attn_fn = partial(F.gumbel_softmax, dim = -1, tau = gumbel_softmax_temp, hard = gumbel_softmax_hard)\n        else:\n            softmax_fn = partial(F.softmax, dim = -1)\n            self.attn_fn = partial(softmax_fn, dtype = torch.float32) if not qk_norm else softmax_fn\n\n        # dropouts\n\n        self.dropout = dropout\n        self.attn_dropout = nn.Dropout(dropout)\n\n        # talking heads\n\n        assert not (flash and (pre_talking_heads or post_talking_heads or pre_scale_post_talking_heads)), 'talking heads not compatible with flash attention'\n\n        self.pre_softmax_talking_heads = nn.Conv2d(heads, heads, 1, bias = False) if pre_talking_heads else None\n        self.post_softmax_talking_heads = nn.Conv2d(heads, heads, 1, bias = False) if post_talking_heads else None\n        self.pre_scale_post_talking_heads = nn.Conv2d(heads, heads, 1, bias = False) if pre_scale_post_talking_heads else None\n\n        if exists(self.pre_softmax_talking_heads):\n            nn.init.dirac_(self.pre_softmax_talking_heads.weight)\n\n        if exists(self.post_softmax_talking_heads):\n            nn.init.dirac_(self.post_softmax_talking_heads.weight)\n\n        if exists(self.pre_scale_post_talking_heads):\n            # an improvisation where heads are combined pre-softmax attention, then used to scale post-softmax attention\n            nn.init.dirac_(self.pre_scale_post_talking_heads.weight)\n\n        # selective attention\n\n        assert not (flash and selective), 'selective attention cannot work on flash attention'\n        assert not (selective and not causal), 'selective attention is designed for autoregressive'\n        self.selective = selective\n\n        # cog attention - negative weights for expressiveness\n        # https://openreview.net/forum?id=ezRrwwbxd0\n\n        assert not (flash and cog_signed), 'cog attention not available for flash'\n        self.cog_signed = cog_signed\n\n        # l2 distance attention\n\n        self.l2_distance = l2_distance\n\n        # add a key / value token composed of zeros\n        # in case this helps controlling outliers, proposed by https://www.evanmiller.org/attention-is-off-by-one.html\n\n        self.add_zero_kv = add_zero_kv\n\n        # learned sink concatted pre-softmax, working solution from gpt-oss\n\n        assert not (head_learned_sink and flash), f'not supported for flash attention yet'\n\n        self.head_learned_sink = head_learned_sink\n        self.head_attn_sink = Parameter(torch.zeros(heads)) if head_learned_sink else None\n\n        # soft clamp attention logit value\n\n        if softclamp_logits:\n            assert not flash, 'flash attention not compatible with logit softclamp value yet'\n            assert logit_softclamp_value > 0.\n\n        self.softclamp_logits = softclamp_logits\n        self.logit_softclamp_value = logit_softclamp_value\n\n        # contextual positional encoding\n\n        self.cope = cope\n\n        # flash attention\n\n        self.flash = flash\n\n        torch_version = version.parse(torch.__version__)\n        assert not (flash and torch_version < version.parse('2.0.0')), 'in order to use flash attention, you must be using pytorch 2.0 or above'\n\n        # torch 2.3 uses new backend and context manager\n\n        if self.flash:\n            if torch_version >= version.parse('2.3'):\n                from torch.nn.attention import SDPBackend\n\n                str_to_backend = dict(\n                    enable_flash = SDPBackend.FLASH_ATTENTION,\n                    enable_mem_efficient = SDPBackend.EFFICIENT_ATTENTION,\n                    enable_math = SDPBackend.MATH,\n                    enable_cudnn = SDPBackend.CUDNN_ATTENTION\n                )\n\n                sdpa_backends = [str_to_backend[enable_str] for enable_str, enable in sdp_kwargs.items() if enable]\n\n                self.sdp_context_manager = partial(torch.nn.attention.sdpa_kernel, sdpa_backends)\n            else:\n                self.sdp_context_manager = partial(torch.backends.cuda.sdp_kernel, **sdp_kwargs)\n\n    def flash_attn(\n        self,\n        q, k, v,\n        mask = None,\n        attn_bias = None\n    ):\n        batch, heads, q_len, _, k_len, is_cuda, device = *q.shape, k.shape[-2], q.is_cuda, q.device\n\n        # Recommended for multi-query single-key-value attention by Tri Dao\n        # kv shape torch.Size([1, 512, 64]) -> torch.Size([1, 8, 512, 64])\n\n        if k.ndim == 3:\n            k = repeat(k, 'b ... -> b h ...', h = q.shape[1])\n\n        if v.ndim == 3:\n            v = repeat(v, 'b ... -> b h ...', h = q.shape[1])\n\n        # handle maybe l2 distance\n\n        if self.l2_distance:\n            k_norm_sq = k.norm(dim = -1, keepdim = True) ** 2\n            k = F.pad(k, (0, 1), value = -1.)\n            k = cat((k, k_norm_sq), dim = -1)\n\n            q_norm_sq = q.norm(dim = -1, keepdim = True) ** 2\n            q = cat((2 * q, q_norm_sq), dim = -1)\n            q = F.pad(q, (0, 1), value = -1.)\n\n        # handle scale - by default they scale by dim_head ** -0.5, but need to take care if using cosine sim attention\n\n        if exists(self.scale):\n            default_scale = q.shape[-1] ** -0.5\n            q = q * (self.scale / default_scale)\n\n        # Check if mask exists and expand to compatible shape\n        # The mask is B L, so it would have to be expanded to B H N L\n\n        causal = self.causal\n\n        # in the case of kv caching with one token (q_len == 1), just turn off causal masking\n        # in speculative decoding, this may go up to 5-6, so right aligned causal mask will be needed there\n\n        if q_len == 1 and causal:\n            causal = False\n\n        # expand key padding mask\n\n        if exists(mask):\n            assert mask.ndim == 4\n            mask = mask.expand(batch, heads, q_len, k_len)\n\n        # handle kv cache - this should be bypassable in updated flash attention 2\n\n        if k_len > q_len and causal:\n            causal_mask = self.create_causal_mask(q_len, k_len, device = device)\n            if not exists(mask):\n                mask = ~causal_mask\n            else:\n                mask = mask & ~causal_mask\n            causal = False\n\n        # manually handle causal mask, if another mask was given\n\n        if exists(mask) and causal:\n            causal_mask = self.create_causal_mask(q_len, k_len, device = device)\n            mask = mask & ~causal_mask\n            causal = False\n\n        # protect against an entire row being masked out\n\n        row_is_entirely_masked = None\n\n        if exists(mask):\n            row_is_entirely_masked = ~mask.any(dim = -1)\n\n        # handle alibi positional bias\n        # convert from bool to float\n\n        if exists(attn_bias):\n            attn_bias = attn_bias.expand(batch, heads, -1, -1)\n\n            # if mask given, the mask would already contain the causal mask from above logic\n            # otherwise, if no mask given but still causal, mask out alibi positional bias to a large negative number\n\n            mask_value = -torch.finfo(q.dtype).max\n\n            if exists(mask):\n                attn_bias = attn_bias.masked_fill(~mask, mask_value // 2)\n            elif causal:\n                causal_mask = self.create_causal_mask(q_len, k_len, device = device)\n                attn_bias = attn_bias.masked_fill(causal_mask, mask_value // 2)\n                causal = False\n\n            # scaled_dot_product_attention handles attn_mask either as bool or additive bias\n            # make it an additive bias here\n\n            mask = attn_bias\n\n        # pytorch 2.0 flash attn: q, k, v, mask, dropout, causal, softmax_scale\n\n        with self.sdp_context_manager():\n            out = F.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask = mask,\n                dropout_p = self.dropout if self.training else 0., \n                is_causal = causal\n            )\n\n        # for a row that is entirely masked out, should zero out the output of that row token\n\n        if exists(row_is_entirely_masked) and row_is_entirely_masked.any():\n            out = out.masked_fill(row_is_entirely_masked[..., None], 0.)\n\n        return out, Intermediates()\n\n    def forward(\n        self,\n        q, k, v,\n        mask = None,\n        attn_bias = None,\n        prev_attn = None\n    ):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        n, heads, kv_heads, device = q.shape[-2], q.shape[1], k.shape[1], q.device\n\n        scale = default(self.scale, q.shape[-1] ** -0.5)\n\n        causal = self.causal\n\n        # handle key padding mask\n\n        if exists(mask) and mask.ndim == 2:\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n\n        # handle kv cached decoding\n\n        if n == 1 and causal:\n            causal = False\n\n        # handle grouped multi-query attention\n\n        if kv_heads == 1:\n            k, v = tuple(rearrange(t, 'b 1 n d -> b n d') for t in (k, v))\n        elif kv_heads < heads:\n            k, v = tuple(repeat(t, 'b kvh n d -> b (r kvh) n d', r = heads // kv_heads) for t in (k, v))\n\n        # handle zero kv, as means for allowing network to attend to nothing\n\n        if self.add_zero_kv:\n            k, v = tuple(F.pad(t, (0, 0, 1, 0), value = 0.) for t in (k, v))\n\n            if exists(mask):\n                mask = F.pad(mask, (1, 0), value = True)\n\n            if exists(attn_bias):\n                attn_bias = F.pad(attn_bias, (1, 0), value = 0.)\n\n        if self.flash:\n            assert not exists(prev_attn), 'residual attention not compatible with flash attention'\n            return self.flash_attn(q, k, v, mask = mask, attn_bias = attn_bias)\n\n        kv_einsum_eq = 'b j d' if k.ndim == 3 else 'b h j d'\n\n        if not self.l2_distance:\n            sim = einsum(f'b h i d, {kv_einsum_eq} -> b h i j', q, k)\n        else:\n            sim = -qk_l2_dist_squared(q, k)\n\n        sim = sim * scale\n\n        if exists(prev_attn):\n            sim = sim + prev_attn\n\n        qk_similarities = sim.clone()\n\n        if exists(self.pre_scale_post_talking_heads):\n            pre_to_post_scale = self.pre_scale_post_talking_heads(sim)\n\n        if exists(self.pre_softmax_talking_heads):\n            sim = sim + self.pre_softmax_talking_heads(sim)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        if self.softclamp_logits:\n            sim = softclamp(sim, self.logit_softclamp_value)\n\n        # pre-masking - handle cog by storing sign\n\n        if self.cog_signed:\n            sim_sign = sim.sign()\n            sim = sim.abs()\n\n        # masking\n\n        i, j, dtype = *sim.shape[-2:], sim.dtype\n\n        mask_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            sim = sim.masked_fill(~mask, mask_value)\n\n        if causal:\n            causal_mask = self.create_causal_mask(i, j, device = device)\n            sim = sim.masked_fill(causal_mask, mask_value)\n\n        row_is_entirely_masked = None\n\n        if exists(mask):\n            row_is_entirely_masked = ~mask.any(dim = -1)\n\n        if exists(self.cope):\n            sim = sim + self.cope(q, sim)\n\n        if self.selective:\n            sim = selective_attn(sim)\n\n        if self.head_learned_sink:\n            # add learned attention sink\n            attn_sink = repeat(self.head_attn_sink, 'h -> b h i 1', b = sim.shape[0], i = sim.shape[2])\n\n            if self.cog_signed:\n                attn_sink, attn_sink_sign = attn_sink.abs(), attn_sink.sign()\n                sim_sign = cat((attn_sink_sign, sim_sign), dim = -1)\n\n            sim = cat((attn_sink, sim), dim = -1)\n\n        pre_softmax_attn = sim\n\n        attn = self.attn_fn(sim)\n\n        attn = attn.type(dtype)\n\n        # add back the sign\n\n        if self.cog_signed:\n            attn = attn * sim_sign\n\n        post_softmax_attn = attn\n\n        if self.head_learned_sink:\n            # remove attention sink\n            attn = attn[..., 1:]\n\n        attn = self.attn_dropout(attn)\n\n        if exists(self.post_softmax_talking_heads):\n            attn = self.post_softmax_talking_heads(attn)\n\n        if exists(self.pre_scale_post_talking_heads):\n            attn = attn * pre_to_post_scale\n\n        out = einsum(f'b h i j, {kv_einsum_eq} -> b h i d', attn, v)\n\n        intermediates = Intermediates(\n            qk_similarities = qk_similarities,\n            pre_softmax_attn = pre_softmax_attn,\n            post_softmax_attn = post_softmax_attn\n        )\n\n        if exists(row_is_entirely_masked) and row_is_entirely_masked.any():\n            out = out.masked_fill(row_is_entirely_masked[..., None], 0.)\n\n        return out, intermediates", "test_code": "", "difficulty": "medium", "category": "cnn", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/attend.py", "name": "Attend", "line": 167}}
{"prompt": "Create a autoregressive wrapper neural network module", "code": "class AutoregressiveWrapper(Module):\n    def __init__(\n        self,\n        net,\n        ignore_index = -100,\n        pad_value = 0,\n        mask_prob = 0.,\n        add_attn_z_loss = False,\n        next_embed_loss_weight = 0.1\n    ):\n        super().__init__()\n        self.pad_value = pad_value\n        self.ignore_index = ignore_index\n\n        self.net = net\n        self.max_seq_len = net.max_seq_len\n\n        # paper shows masking (MLM) in conjunction with autoregressive decoder-only training leads to big improvements https://arxiv.org/abs/2210.13432\n        assert mask_prob < 1.\n        self.mask_prob = mask_prob\n\n        # whether to add router z-loss\n        self.add_attn_z_loss = add_attn_z_loss\n\n        # whether to add a continuous loss\n        self.add_continuous_pred_head = net.add_continuous_pred_head\n        self.next_embed_loss_weight = next_embed_loss_weight\n\n    @torch.no_grad()\n    @eval_decorator\n    def beam_search(\n        self,\n        prompts,\n        seq_len,\n        beams = 4,\n        return_beams_and_scores = False,\n        eos_token = None,\n        temperature = 1.,\n        stochastic = False,\n        prompt_lens: Tensor | None = None,\n        filter_logits_fn: str | Callable = identity,\n        restrict_to_max_seq_len = True,\n        filter_kwargs: dict = dict(),\n        cache_kv = True,\n        **kwargs\n    ):\n        assert not exists(eos_token), 'eos token not supported yet'\n\n        max_seq_len, greedy, device = self.max_seq_len, temperature == 0., prompts.device\n\n        prompts, packed_shape = pack([prompts], '* n')\n\n        batch, orig_seq_len = prompts.shape\n\n        # handle filter logits fn given as string\n\n        if isinstance(filter_logits_fn, str):\n            assert filter_logits_fn in FILTER_LOGITS_FN, f\"only {join(FILTER_LOGITS_FN.keys())} are available\"\n\n            filter_logits_fn = FILTER_LOGITS_FN[filter_logits_fn]\n\n        # handle variable lengthed prompts (prefixes)\n\n        seq_start_pos = None\n        if exists(prompt_lens):\n            prompts = align_right(prompts, prompt_lens, pad_id = self.pad_value)\n            seq_start_pos = orig_seq_len - prompt_lens\n\n        # output from which sampled tokens appended to\n\n        out = prompts\n\n        # kv caches\n\n        cache = None\n\n        should_cache = cache_kv and self.net.can_cache_kv\n\n        # scores for the beams\n\n        scores = torch.zeros((batch,), device = device)\n\n        batch_arange = torch.arange(batch, device = device)\n\n        # sampling up to seq_len\n\n        for i in range(seq_len):\n            is_first = i == 0\n\n            if restrict_to_max_seq_len:\n                max_len_exceeded = out.shape[-1] > max_seq_len\n\n                assert not (cache_kv and max_len_exceeded and not self.net.can_cache_kv_outside_max_seq_len), 'the network cannot use cached key values when decoding outside the max sequence length. most likely because you are using absolute positional embedding. you can switch to rotary embeddings to resolve this issue'\n\n                x = out[:, -max_seq_len:]\n\n                if exists(cache):\n                    modify_cached_kv(cache, lambda t: t[..., -(max_seq_len - 1):, :])\n\n            logits, new_cache = self.net(\n                x,\n                return_intermediates = True,\n                cache = cache,\n                seq_start_pos = seq_start_pos,\n                **kwargs\n            )\n\n            if should_cache:\n                cache = new_cache\n\n            logits = logits[:, -1]\n\n            # to add to the scores\n\n            log_probs = logits.log_softmax(dim = -1)\n\n            # maybe filter by top_k, top_p (nucleus) for stochastic beam search\n\n            if stochastic and not greedy:\n                logits = filter_logits_fn(logits, **filter_kwargs)\n                logits = (logits / temperature) + gumbel_noise(logits)\n\n            # (gumbel) topk\n\n            samples = logits.topk(beams, dim = -1).indices\n\n            # get the scores for keeping track of beams\n\n            next_scores = log_probs.gather(-1, samples)\n\n            # expand beam times\n\n            scores = repeat(scores, 'b -> b beams', beams = beams)\n            scores = scores + next_scores\n\n            out = repeat(out, 'b ... -> (b beams) ...', beams = beams)\n            samples = rearrange(samples, 'b beams -> (b beams) 1')\n\n            if should_cache and is_first:\n                modify_cached_kv(cache, lambda t: repeat(t, 'b ... -> (b beams) ...', beams = beams))\n\n            # concat sample\n\n            out = torch.cat((out, samples), dim=-1)\n\n            # sort by score and excise\n            # excise out the beams\n\n            scores = rearrange(scores, '(b prev_beams) next_beams -> b (prev_beams next_beams)', b = batch)\n            curr_num_beams = scores.shape[-1]\n\n            if curr_num_beams > beams:\n                scores, sort_indices = scores.sort(dim = -1, descending = True)\n\n                scores = scores[:, :beams]\n                top_beams_indices = sort_indices[:, :beams]\n\n                top_beams_indices = curr_num_beams * batch_arange[:, None] + top_beams_indices\n\n                flattened_beam_indices = rearrange(top_beams_indices, 'b beams -> (b beams)')\n\n                out = out[flattened_beam_indices]\n\n            scores = rearrange(scores, 'b beams -> (b beams)')\n\n            if not exists(eos_token):\n                continue\n\n            is_eos_tokens = (out == eos_token)\n\n            if is_eos_tokens.any(dim = -1).all():\n                break\n\n        if exists(eos_token):\n            # mask out everything after the eos tokens\n            shifted_is_eos_tokens = F.pad(is_eos_tokens, (1, -1))\n            mask = shifted_is_eos_tokens.float().cumsum(dim = -1) >= 1\n            out = out.masked_fill(mask, self.pad_value)\n\n        # select out the top beam\n\n        out = rearrange(out, '(b beams) seq -> b beams seq', b = batch)\n\n        out = out[..., orig_seq_len:]\n\n        out, = unpack(out, packed_shape, '* beams n') # prompt may have no batch dimension\n\n        if not return_beams_and_scores:\n            return out[..., 0, :]\n\n        scores = rearrange(scores, '(b beams) -> beams b', b = batch)\n        out = rearrange(out, 'b beams n -> beams b n')\n\n        return out, scores\n\n    @torch.no_grad()\n    @eval_decorator\n    def generate(\n        self,\n        prompts: list[Tensor] | Tensor,\n        seq_len,\n        eos_token = None,\n        temperature = 1.,\n        prompt_lens: Tensor | None = None,\n        filter_logits_fn: str | Callable = top_k,\n        restrict_to_max_seq_len = True,\n        amateur_model: Module | Tuple[Module] | None = None,\n        filter_kwargs: dict = dict(),\n        contrastive_decode_kwargs: dict | Tuple[dict] = dict(\n            beta = 0.5,\n            alpha = 0.1\n        ),\n        cache_kv = True,\n        **kwargs\n    ):\n        max_seq_len, greedy = self.max_seq_len, temperature == 0.\n\n        # handle prompts given as list of variable lengthed token ids\n\n        if isinstance(prompts, list):\n            assert len(prompts) > 0, 'prompts cannot be empty list'\n            assert not exists(prompt_lens), '`prompt_len` will be auto derived if prompts are passed in as list of Tensors'\n\n            prompt_lens = tensor([t.shape[0] for t in prompts], device = prompts[0].device)\n\n            prompts = pad_sequence(prompts, batch_first = True)\n\n        # pack maybe no batch\n\n        prompts, ps = pack([prompts], '* n')\n\n        b, t, device = *prompts.shape, prompts.device\n\n        # handle filter logits fn given as string\n\n        if isinstance(filter_logits_fn, str):\n            assert filter_logits_fn in FILTER_LOGITS_FN, f\"only {join(FILTER_LOGITS_FN.keys())} are available\"\n\n            filter_logits_fn = FILTER_LOGITS_FN[filter_logits_fn]\n\n        # handle variable lengthed prompts (prefixes)\n\n        seq_start_pos = None\n        if exists(prompt_lens):\n            prompts = align_right(prompts, prompt_lens, pad_id = self.pad_value)\n            seq_start_pos = t - prompt_lens\n\n        # output from which sampled tokens appended to\n\n        out = prompts\n\n        # kv caches\n\n        cache = None\n\n        # if doing contrastive decoding, turn off filter automatically\n\n        if exists(amateur_model):\n            amateur_model = cast_tuple(amateur_model)\n            contrastive_decode_kwargs = cast_tuple(contrastive_decode_kwargs)\n\n            assert len(amateur_model) == len(contrastive_decode_kwargs)\n\n            amateur_caches = [None] * len(amateur_model)\n            filter_logits_fn = identity\n\n            for i, module in enumerate(amateur_model):\n                if isinstance(module, AutoregressiveWrapper):\n                    amateur_model[i] = module.net\n\n                module.eval()\n\n        # sampling up to seq_len\n\n        for _ in range(seq_len):\n\n            if restrict_to_max_seq_len:\n                max_len_exceeded = out.shape[-1] > max_seq_len\n\n                assert not (cache_kv and max_len_exceeded and not self.net.can_cache_kv_outside_max_seq_len), 'the network cannot use cached key values when decoding outside the max sequence length. most likely because you are using absolute positional embedding. you can switch to rotary embeddings to resolve this issue'\n\n                x = out[:, -max_seq_len:]\n\n                if exists(cache):\n                    for inter in cache.attn_intermediates:\n                        if inter.layer_type == 'a':\n                            inter.cached_kv = [t[..., -(max_seq_len - 1):, :] for t in inter.cached_kv]\n\n            logits, new_cache = self.net(\n                x,\n                return_intermediates = True,\n                cache = cache,\n                seq_start_pos = seq_start_pos,\n                **kwargs\n            )\n\n            if cache_kv and self.net.can_cache_kv:\n                cache = new_cache\n\n            logits = logits[:, -1]\n\n            # handle contrastive decoding, Li et al.\n            # https://arxiv.org/abs/2210.15097\n\n            if exists(amateur_model):\n                for i, (amateur, amateur_cache, amateur_contrastive_decode_kwargs) in enumerate(zip(amateur_model, amateur_caches, contrastive_decode_kwargs)):\n                    amateur_logits, next_amateur_cache = amateur(\n                        x,\n                        return_intermediates = True,\n                        cache = amateur_cache,\n                        seq_start_pos = seq_start_pos,\n                        **kwargs\n                    )\n\n                    amateur_logits = amateur_logits[:, -1]\n\n                    assert amateur_logits.shape == logits.shape, 'logits dimension are not the same between amateur and expert model'\n                    logits = contrastive_decode_fn(logits, amateur_logits, **amateur_contrastive_decode_kwargs)\n\n                    if cache_kv and amateur.can_cache_kv:\n                        amateur_caches[i] = next_amateur_cache\n\n            # filter by top_k, top_p (nucleus), top_a, or custom\n\n            if greedy:\n                sample = logits.argmax(dim = -1, keepdim = True)\n            else:\n                filtered_logits = filter_logits_fn(logits, **filter_kwargs)\n                probs = F.softmax(filtered_logits / temperature, dim=-1)\n                sample = torch.multinomial(probs, 1)\n\n            # concat sample\n\n            out = torch.cat((out, sample), dim=-1)\n\n            if not exists(eos_token):\n                continue\n\n            is_eos_tokens = (out == eos_token)\n\n            if is_eos_tokens.any(dim = -1).all():\n                break\n\n        if exists(eos_token):\n            # mask out everything after the eos tokens\n            shifted_is_eos_tokens = F.pad(is_eos_tokens, (1, -1))\n            mask = shifted_is_eos_tokens.float().cumsum(dim = -1) >= 1\n            out = out.masked_fill(mask, self.pad_value)\n\n        out = out[:, t:]\n\n        out, = unpack(out, ps, '* n')\n\n        return out\n\n    def forward(\n        self,\n        x,\n        return_outputs = False,\n        prepend_embeds = None,\n        **kwargs\n    ):\n        seq, ignore_index, add_attn_z_loss, add_next_embed_loss = x.shape[1], self.ignore_index, self.add_attn_z_loss, self.add_continuous_pred_head\n\n        inp, target = x, x[:, 1:]\n        inp = torch.where(inp == ignore_index, self.pad_value, inp)\n\n        if self.mask_prob > 0.:\n            rand = torch.randn(inp.shape, device = x.device)\n            rand[:, 0] = -torch.finfo(rand.dtype).max # first token should not be masked out\n            num_mask = min(int(seq * self.mask_prob), seq - 1)\n            indices = rand.topk(num_mask, dim = -1).indices\n            mask = ~torch.zeros_like(inp).scatter(1, indices, 1.).bool()\n            kwargs.update(self_attn_kv_mask = mask)\n\n        out, cache = self.net(\n            inp,\n            return_intermediates = True,\n            return_attn_z_loss = add_attn_z_loss,\n            return_next_embed_pred = add_next_embed_loss,\n            prepend_embeds = prepend_embeds,\n            **kwargs\n        )\n\n        # destruct differently if doing continuous pred\n\n        if add_next_embed_loss:\n            logits, (next_embed_pred, init_embeds) = out\n        else:\n            logits = out\n\n        # if there are prepended embeds, excise it out\n\n        if exists(prepend_embeds):\n            prepend_len = prepend_embeds.shape[1]\n            logits = logits[:, prepend_len:]\n\n        # take all tokens but the last\n\n        logits = logits[:, :-1]\n\n        # loss function\n\n        loss_fn = F.cross_entropy if not self.net.output_is_log_prob else F.nll_loss\n\n        # cross entropy loss\n\n        loss = loss_fn(\n            rearrange(logits, 'b n c -> b c n'),\n            target,\n            ignore_index = ignore_index\n        )\n\n        if add_attn_z_loss:\n            loss = loss + cache.attn_z_loss\n\n        if add_next_embed_loss:\n            mask = target != ignore_index\n            embed_pred = next_embed_pred[:, :-1]\n            cont_targets = init_embeds[:, 1:].detach()\n\n            cont_loss = F.l1_loss(embed_pred, cont_targets, reduction = 'none')\n            cont_loss = cont_loss[mask].mean()\n\n            loss = loss + cont_loss * self.next_embed_loss_weight\n\n        if not return_outputs:\n            return loss\n\n        return loss, (logits, cache)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/autoregressive_wrapper.py", "name": "AutoregressiveWrapper", "line": 156}}
{"prompt": "Figure 13. in https://arxiv.org/abs/2410.23506", "code": "class BeliefStateWrapper(Module):\n    \"\"\"\n    Figure 13. in https://arxiv.org/abs/2410.23506\n    \"\"\"\n\n    def __init__(\n        self,\n        forward_decoder: TransformerWrapper,\n        backward_decoder: TransformerWrapper | None = None,\n        train_frac_forward_backward_pairs: float = 1.,\n        text_head: Module | None = None,\n        backward_ar_loss_weight: float = 1., # can weigh the training of the backwards decoder differently, perhaps fwd/bwd have a shared backbone etc etc\n        pred_distance = False,\n        pred_distance_loss_weight: float = 1.,\n        cond_on_distance = False,\n        cond_on_distance_prob = 0.5,\n        max_pred_distance = None\n    ):\n        super().__init__()\n        backward_decoder = default(backward_decoder, forward_decoder) # if backward decoder not set, use the same transformer, assume it knows how to switch gears based on suffix token\n\n        assert forward_decoder.emb_dim == backward_decoder.emb_dim, 'forward and backwards model must have the same embedding dimension'\n        assert forward_decoder.num_tokens == backward_decoder.num_tokens, 'forward and backwards model must have the same number of tokens'\n\n        dim = forward_decoder.emb_dim\n        num_tokens = forward_decoder.num_tokens\n        max_seq_len = forward_decoder.max_seq_len\n\n        self.num_tokens = num_tokens\n\n        # the suffix token\n\n        self.suffix_token = nn.Parameter(torch.zeros(dim))\n        nn.init.normal_(self.suffix_token, std = 0.02)\n\n        # the text prediction head, which predicts for the combinations of prefix and suffix the next and previous token for forwards and backward sequences\n\n        if not exists(text_head):\n            text_head = nn.Sequential(\n                nn.Linear(dim * 2, dim),\n                nn.LeakyReLU(),\n                nn.Linear(dim, num_tokens * 2),\n            )\n\n        self.text_head = text_head\n\n        # predicting terminal state (when suffix and prefix predict the same token)\n\n        self.max_pred_distance = default(max_pred_distance, max_seq_len)\n\n        self.to_distance_logits = nn.Sequential(\n            nn.Linear(dim * 2, dim),\n            nn.LeakyReLU(),\n            nn.Linear(dim, self.max_pred_distance),\n        ) if pred_distance else None\n\n        self.pred_distance_loss_weight = pred_distance_loss_weight\n\n        # conditioning on distance\n\n        assert 0. < cond_on_distance_prob < 1.\n\n        self.cond_on_distance = cond_on_distance\n        self.cond_on_distance_prob = cond_on_distance_prob\n\n        if cond_on_distance:\n            self.to_distance_cond = nn.Sequential(\n                Rearrange('... -> ... 1'),\n                nn.Linear(1, dim),\n                nn.LeakyReLU(),\n                nn.Linear(dim, dim * 2),\n            )\n\n        # the two decoders, one which is causal forward, the other causal backwards\n\n        self.forward_decoder = forward_decoder\n        self.backward_decoder = backward_decoder\n\n        # what fraction of forward backward pairs to train on\n        # for further memory efficiency\n\n        assert 0 < train_frac_forward_backward_pairs <= 1.\n        self.train_frac_fb_pairs = train_frac_forward_backward_pairs\n        self.needs_subsample_fb_pairs = train_frac_forward_backward_pairs < 1.\n\n        # loss weighting\n\n        self.backward_ar_loss_weight = backward_ar_loss_weight\n        self.needs_loss_weight = backward_ar_loss_weight != 1.\n\n        self.register_buffer('loss_weights', tensor([1., self.backward_ar_loss_weight]))\n\n        # sampling\n\n        self.max_seq_len = self.forward_decoder.max_seq_len\n\n    @torch.no_grad()\n    @eval_decorator\n    def generate_with_suffix_cond(\n        self,\n        prompts,\n        seq_len,\n        temperature = 1.25,\n        cache_kv = False,\n        suffix: Tensor | None = None, # the goal conditioning\n        filter_logits_fn = min_p,\n        filter_kwargs = dict(\n            min_p = 0.1\n        ),\n        decode_backwards = False,\n        **kwargs\n    ):\n        max_seq_len, greedy, device = self.max_seq_len, temperature == 0., prompts.device\n\n        prompts, batch_ps = pack([prompts], '* d')\n\n        batch, orig_seq_len = prompts.shape\n\n        # allow for decoding backwards, to make sure it is working\n\n        main_decoder = self.forward_decoder\n\n        if decode_backwards:\n            prompts = prompts.flip(1)\n            main_decoder = self.backward_decoder\n\n        out = prompts\n\n        # kv caches\n\n        cache = None\n\n        # get the encoded suffix token once\n\n        suffix_sos_tokens = rearrange(self.suffix_token, 'd -> 1 1 d')\n\n        suffix_sos_tokens = repeat(suffix_sos_tokens, '1 1 d -> b 1 d', b = batch)\n\n        if not decode_backwards:\n            if exists(suffix):\n                if suffix.ndim == 1:\n                    suffix = repeat(suffix, 'n -> b n', b = batch)\n\n                suffix = suffix.flip(1) # reverse autoregressive\n\n            suffix_embed = self.backward_decoder(\n                suffix,\n                prepend_embeds = suffix_sos_tokens,\n                return_embeddings = True\n            )\n\n            # pick out the last embedding for fill in the middle\n\n            suffix_embed = suffix_embed[:, -1:]\n\n        else:\n            # just grab a random token for now for prefix\n\n            prefix_embed = torch.randint(0, self.num_tokens, (batch, 1), device = device)\n\n            prefix_embed = self.forward_decoder(prefix_embed, return_embeddings = True)\n\n        # sampling up to seq_len\n\n        for _ in range(seq_len):\n\n            embeds, new_cache = main_decoder(\n                out,\n                prepend_embeds = suffix_sos_tokens if decode_backwards else None,\n                return_intermediates = True,\n                return_embeddings = True,\n                cache = cache,\n                **kwargs\n            )\n\n            last_embeds = embeds[:, -1:]\n\n            if not decode_backwards:\n                embeds = cat((last_embeds, suffix_embed), dim = -1)\n            else:\n                embeds = cat((prefix_embed, last_embeds), dim = -1)\n\n            if cache_kv and self.forward_decoder.can_cache_kv:\n                cache = new_cache\n\n            forward_logits, backward_logits = self.text_head(embeds).chunk(2, dim = -1)\n\n            logits = forward_logits if not decode_backwards else backward_logits\n\n            logits = logits[:, -1]\n\n            if greedy:\n                sample = logits.argmax(dim = -1, keepdim = True)\n            else:\n                filtered_logits = filter_logits_fn(logits, **filter_kwargs)\n                probs = F.softmax(filtered_logits / temperature, dim = -1)\n                sample = torch.multinomial(probs, 1)\n\n            # concat sample\n\n            out = torch.cat((out, sample), dim = -1)\n\n        out = out[:, orig_seq_len:]\n\n        out, = unpack(out, batch_ps, '* n')\n\n        return out\n\n    def forward(\n        self,\n        seq,\n        lens: Tensor | None = None, # Int['b']\n        loss_weight_by_fb_indices: callable | None = None\n    ):\n        batch, seq_len, device = *seq.shape, seq.device\n\n        # handle variable length sequences\n\n        seq_for_labels = seq\n\n        if exists(lens):\n            mask = einx.less('j, i -> i j', arange(seq_len, device = device), lens)\n            seq_for_labels = torch.where(mask, seq, -1)\n\n        # forward autoregressive\n\n        forward_embeds = self.forward_decoder(seq, return_embeddings = True)\n\n        # backward autoregressive\n\n        backward_seq = flip(seq, lens = lens)\n\n        suffix_tokens = repeat(self.suffix_token, 'd -> b 1 d', b = batch)\n\n        backward_embeds = self.backward_decoder(\n            backward_seq,\n            prepend_embeds = suffix_tokens,\n            return_embeddings = True\n        )\n\n        backward_embeds = flip(backward_embeds, lens = lens)\n\n        # trick to reduce memory on backwards pass\n\n        forward_embeds, backward_embeds = detach_multiple(forward_embeds, backward_embeds)\n\n        # belief state objective\n\n        seq_arange = arange(seq_len, device = device)\n\n        fb_pairs = cartesian_prod(seq_arange, seq_arange + 1) # plus one for suffix token\n\n        # filter down to valid pairs, as in figure 11\n        # f - forward, b - backward, i - indices\n\n        fi, bi = fb_pairs.unbind(dim = -1)\n\n        valid_mask = (bi - fi) >= 2\n\n        fb_pairs = fb_pairs[valid_mask]\n\n        # maybe subsample fb pairs\n\n        if self.needs_subsample_fb_pairs:\n            num_pairs = fb_pairs.shape[0]\n\n            num_subsampled = max(int(num_pairs * self.train_frac_fb_pairs), 1)\n\n            rand_subsampled_indices = torch.randperm(num_pairs, device = device)[:num_subsampled]\n\n            fb_pairs = fb_pairs[rand_subsampled_indices]\n\n        # get labels for both\n\n        fi, bi = fb_pairs.unbind(dim = -1)\n\n        labels_fi, labels_bi = (fi + 1), (bi - 1)\n\n        forward_labels, backward_labels = seq_for_labels[:, labels_fi], seq_for_labels[:, labels_bi]\n\n        labels = cat((forward_labels, backward_labels), dim = -1)\n\n        # get the forward and backward embedding pairs and feed them through the text head for both forward and backward predictions\n\n        fb_embeds = cat((\n            forward_embeds[:, fi],\n            backward_embeds[:, bi]\n        ), dim = -1)\n\n        logits = self.text_head(fb_embeds)\n\n        # cross entropy loss\n\n        loss = F.cross_entropy(\n            rearrange(logits, 'b n (fb l) -> b l (fb n)', fb = 2),\n            labels,\n            reduction = 'none' if self.needs_loss_weight else 'mean',\n            ignore_index = -1\n        )\n\n        # maybe condition on distance\n\n        cond_on_distance = self.cond_on_distance and (random() < self.cond_on_distance_prob)\n\n        if cond_on_distance:\n            distance = (bi - fi).float()\n            distance_cond = self.to_distance_cond(distance)\n\n            fb_embeds = fb_embeds * distance_cond\n\n        # maybe predict distance\n\n        if exists(self.to_distance_logits) and not cond_on_distance:\n            distance_logits = self.to_distance_logits(fb_embeds)\n\n            distance_labels = (bi - fi).clamp(max = self.max_pred_distance - 1)\n            distance_labels = repeat(distance_labels, 'n -> b n', b = batch)\n\n            pred_dist_loss = F.cross_entropy(\n                rearrange(distance_logits, 'b n l -> b l n'),\n                distance_labels\n            )\n\n            loss = (\n                loss +\n                pred_dist_loss * self.pred_distance_loss_weight\n            )\n\n        # maybe loss weighting\n\n        needs_loss_weight = default(self.needs_loss_weight, exists(loss_weight_by_fb_indices))\n\n        if needs_loss_weight:\n            loss = rearrange(loss, 'b (fb n) -> b fb n', fb = 2)\n\n            if self.needs_loss_weight:\n                loss = einx.multiply('b fb n, fb', loss, self.loss_weights)\n\n            # allow researcher to pass in a function that acts on the the forward backward indices Int['n fb']\n            # the reason this may be needed is because the earlier tokens will have more eligible pairs for training, and perhaps this could be normalized\n\n            if exists(loss_weight_by_fb_indices):\n                loss_weight = loss_weight_by_fb_indices(fb_pairs)\n\n                if loss_weight.ndim == 1:\n                    loss = einx.multiply('b fb n, n', loss, loss_weight)\n                elif loss_weight.ndim == 2:\n                    loss = einx.multiply('b fb n, n fb', loss, loss_weight)\n                else:\n                    raise ValueError('invalid loss weight dims')\n\n            loss = loss.mean()\n\n        return loss", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/belief_state_wrapper.py", "name": "BeliefStateWrapper", "line": 79}}
{"prompt": "Create a continuous transformer wrapper neural network module", "code": "class ContinuousTransformerWrapper(Module):\n    def __init__(\n        self,\n        *,\n        max_seq_len,\n        attn_layers: AttentionLayers,\n        dim_in = None,\n        dim_out = None,\n        emb_dim = None,\n        max_mem_len = 0,\n        num_memory_tokens = None,\n        post_emb_norm = False,\n        emb_dropout = 0.,\n        use_abs_pos_emb = True,\n        scaled_sinu_pos_emb = False,\n        average_pool_embed = False,\n        probabilistic = False,\n    ):\n        super().__init__()\n        dim = attn_layers.dim\n\n        self.max_seq_len = max_seq_len\n\n        self.max_mem_len = max_mem_len\n        \n        no_abs_pos_emb = max_seq_len == 0 or not (use_abs_pos_emb and not attn_layers.disable_abs_pos_emb)\n\n        if no_abs_pos_emb:\n            self.pos_emb = always(0)\n        elif scaled_sinu_pos_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(dim)\n        else:\n            self.pos_emb = AbsolutePositionalEmbedding(dim, max_seq_len)\n\n        self.post_emb_norm = LayerNorm(dim) if post_emb_norm else nn.Identity()\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        # memory tokens\n\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.has_memory_tokens = num_memory_tokens > 0\n\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n\n        # attention layers\n\n        self.attn_layers = attn_layers\n\n        # average pool\n\n        self.average_pool_embed = average_pool_embed\n\n        # project in and out\n\n        self.project_in = nn.Linear(dim_in, dim, bias = False) if exists(dim_in) else nn.Identity()\n\n        # output is multipled by 2 for outputting mean and log variance\n\n        self.probabilistic = probabilistic\n\n        self.project_out = nn.Linear(dim, dim_out * (2 if probabilistic else 1), bias = False) if exists(dim_out) else nn.Identity()\n\n        # can cache kv\n\n        self.can_cache_kv = all([module.can_cache_kv for module in self.modules() if isinstance(module, Attention)])\n\n    def forward(\n        self,\n        x,\n        return_embeddings = False,\n        return_intermediates = False,\n        return_mems = False,\n        mask = None,\n        lens = None,\n        return_attn = False,\n        mems = None,\n        mem_masks = None,\n        pos = None,\n        sum_embeds = None,\n        prepend_embeds = None,\n        prepend_mask = None,\n        cache: LayerIntermediates | None = None,\n        input_not_include_cache = False,\n        seq_start_pos = None,\n        **kwargs\n    ):\n        batch, seq, orig_mask, device = *x.shape[:2], mask, x.device\n\n        # maybe seq lengths passed in\n\n        if exists(lens):\n            assert not exists(mask), 'either `mask` or `lens` passed in, but not both'\n            seq_arange = arange(seq, device = device)\n\n            mask = einx.less('j, i -> i j', seq_arange, lens)\n\n        # take care of position embedding offsets in the presence of cache and sequence is less than cache length (not full sequence)\n\n        seq_pos_offset = 0\n\n        if exists(cache) and input_not_include_cache:\n            seq_pos_offset = cache.cache_length\n\n        # project in + positional embedding\n\n        x = self.project_in(x)\n        x = x + self.pos_emb(x, pos = pos, seq_start_pos = seq_start_pos, offset = seq_pos_offset)\n\n        if exists(sum_embeds):\n            x = x + sum_embeds\n\n        x = self.post_emb_norm(x)\n\n        # memory tokens\n\n        if self.has_memory_tokens:\n            m = repeat(self.memory_tokens, 'm d -> b m d', b = batch)\n            x, mem_ps = pack([m, x], 'b * d')\n\n            if exists(mask):\n                num_mems = m.shape[-2]\n                mask = pad_at_dim(mask, (num_mems, 0), dim = -1, value = True)\n\n        # whether to append embeds, as in PaLI, for image embeddings\n\n        if exists(prepend_embeds):\n            prepend_seq, prepend_dim = prepend_embeds.shape[1:]\n\n            assert prepend_dim == x.shape[-1], 'prepended embeddings need to have same dimensions as model dimensions'\n\n            x = cat((prepend_embeds, x), dim = -2)\n\n            if exists(prepend_mask) or exists(mask):\n                mask = default(mask, lambda: torch.ones((batch, seq), device = device, dtype = torch.bool))\n                prepend_mask = default(prepend_mask, lambda: torch.ones((batch, prepend_seq), device = device, dtype = torch.bool))\n\n                mask = cat((prepend_mask, mask), dim = -1)\n\n        x = self.emb_dropout(x)\n\n        # attention layers\n\n        x, intermediates = self.attn_layers(x, mask = mask, mems = mems, mem_masks = mem_masks, cache = cache, input_not_include_cache = input_not_include_cache, seq_pos_offset = seq_pos_offset, return_hiddens = True, **kwargs)\n\n        # splice out memory tokens\n\n        if self.has_memory_tokens:\n            m, x = unpack(x, mem_ps, 'b * d')\n            intermediates.memory_tokens = m\n\n        if self.average_pool_embed:\n            x = masked_mean(x, mask = orig_mask)\n\n        # maybe linear project out\n\n        out = self.project_out(x) if not return_embeddings else x\n\n        if not return_embeddings and self.probabilistic:\n            mean, log_var = rearrange(out, '... (d mean_log_var) -> mean_log_var ... d', mean_log_var = 2)\n            variance = log_var.exp()\n            out = stack((mean, variance))\n\n        if return_intermediates:\n            return out, intermediates\n\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = tuple(t[..., -self.max_mem_len:, :].detach() for t in hiddens)\n            return out, new_mems\n\n        if return_attn:\n            attn_maps = tuple(t.post_softmax_attn for t in intermediates.attn_intermediates)\n            return out, attn_maps\n\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/continuous.py", "name": "ContinuousTransformerWrapper", "line": 62}}
{"prompt": "Create a continuous autoregressive wrapper neural network module", "code": "class ContinuousAutoregressiveWrapper(Module):\n    def __init__(\n        self,\n        net: ContinuousTransformerWrapper,\n        loss_fn: Module | None = None,\n        use_l1_loss = False,\n        equal_loss_weight_batch = False,  # setting this to True, if the mask is passed in and sequences are variable in length, each sequence will be weighted the same (as opposed to each token)\n    ):\n        super().__init__()\n        self.net = net\n        self.max_seq_len = net.max_seq_len\n\n        probabilistic = net.probabilistic\n        self.probabilistic = probabilistic\n\n        # default loss function\n\n        if not exists(loss_fn):\n            if probabilistic:\n                loss_fn = GaussianNLL()\n            elif use_l1_loss:\n                loss_fn = nn.L1Loss(reduction = 'none')\n            else:\n                loss_fn = nn.MSELoss(reduction = 'none')\n\n        self.loss_fn = loss_fn\n        self.equal_loss_weight_batch = equal_loss_weight_batch\n\n    @torch.no_grad()\n    def generate(\n        self,\n        start_tokens,\n        seq_len,\n        temperature = 1.,\n        cache_kv = True,\n        **kwargs\n    ):\n        should_cache_kv = cache_kv and self.net.can_cache_kv\n        device = start_tokens.device\n\n        was_training = self.net.training\n        num_dims = start_tokens.ndim\n\n        assert num_dims >= 2, 'number of dimensions of your start tokens must be greater or equal to 2'\n        no_batch = num_dims == 2\n\n        if no_batch:\n            start_tokens = rearrange(start_tokens, 'n d -> 1 n d')\n\n        b, t, _, device = *start_tokens.shape, start_tokens.device\n\n        self.net.eval()\n        out = start_tokens\n\n        cache = None\n\n        for _ in range(seq_len):\n            x = out[:, -self.max_seq_len:]\n\n            net_out, new_cache = self.net(x, cache = cache, return_intermediates = True, **kwargs)\n\n            last_output = net_out[..., -1:, :]\n\n            if self.probabilistic:\n                mean, var = last_output\n                last_output = sample_from_mean_variance(mean, var, temperature = temperature)\n\n            out = cat((out, last_output), dim = -2)\n\n            if should_cache_kv:\n                cache = new_cache\n\n        out = out[:, t:]\n\n        if no_batch:\n            out = rearrange(out, '1 n d -> n d')\n\n        self.net.train(was_training)\n        return out\n\n    def forward_rollout(\n        self,\n        x,\n        rollout_steps = 2,\n        **kwargs\n    ):\n        assert rollout_steps > 1\n\n        steps = rollout_steps\n\n        device = x.device\n\n        # assert inputs\n\n        assert 'prepend_embeds' not in kwargs\n\n        # lens\n\n        lens = kwargs.pop('lens', None)\n\n        if exists(lens):\n            assert 'mask' not in kwargs, 'either `mask` or `lens` passed in, but not both'\n            seq_len, device = inp.shape[1], inp.device\n            seq_arange = arange(seq_len, device = device)\n            mask = einx.less('j, i -> i j', seq_arange, lens)\n            kwargs['mask'] = mask\n\n        if not exists(lens):\n            batch, seq_len = x.shape[:2]\n            lens = torch.full((batch,), seq_len, device = device)\n\n        # handle mask manually\n\n        mask = kwargs.pop('mask', None)\n\n        # pick a random range for each batch sample and aligh the sequence to the right for rollout loss\n\n        valid_tokens_for_rollout = (lens - steps).clamp(min = 0)\n        valid_sample = valid_tokens_for_rollout > 0\n\n        x = x[valid_sample] # remove invalid sequence (lens less than rollout steps)\n\n        if exists(mask):\n            mask = mask[valid_sample]\n\n        batch = x.shape[0]\n        seq_start_pos = (torch.rand((batch,), device = device) * valid_tokens_for_rollout).floor().long()\n\n        batch_arange = torch.arange(batch, device = device)\n        batch_arange = rearrange(batch_arange, 'b -> b 1')\n\n        # crop out sequence to use\n\n        seq_end_pos = seq_start_pos + steps\n        max_end_pos = seq_end_pos.amax().item()\n        x = x[:, :max_end_pos]\n\n        x = align_right(x, seq_end_pos)\n\n        # get the input\n\n        inp, targets = x[:, :-steps], x[:, -steps:]\n\n        # maybe rollout\n\n        cache = None\n        preds = []\n\n        for _ in range(steps):\n\n            out, cache = self.net(\n                inp,\n                seq_start_pos = seq_start_pos,\n                return_intermediates = True,\n                **kwargs\n            )\n\n            last_pred = out[..., -1:, :]\n\n            if self.probabilistic:\n                mean, var = last_pred\n                inp = sample_from_mean_variance(mean, var)\n            else:\n                inp = last_pred\n\n            preds.append(last_pred)\n\n        # stack for predictions\n\n        preds = cat(preds, dim = 1)\n\n        # loss\n\n        loss = self.loss_fn(preds, targets)\n\n        return loss.mean()\n\n    def forward(\n        self,\n        x,\n        rollout_steps = 1, # they used 2 rollout steps in a successful world model paper https://ai.meta.com/vjepa/\n        **kwargs\n    ):\n        if rollout_steps > 1:\n            return self.forward_rollout(x, rollout_steps = rollout_steps, **kwargs)\n\n        inp, target = x[:, :-1], x[:, 1:]\n\n        assert 'prepend_embeds' not in kwargs\n\n        # lens\n\n        lens = kwargs.pop('lens', None)\n\n        if exists(lens):\n            assert 'mask' not in kwargs, 'either `mask` or `lens` passed in, but not both'\n            seq_len, device = inp.shape[1], inp.device\n            seq_arange = torch.arange(seq_len, device = device)\n            mask = einx.less('j, i -> i j', seq_arange, lens)\n\n            kwargs['mask'] = mask\n\n        # mask\n\n        mask = kwargs.get('mask', None)\n\n        if exists(mask) and mask.shape[1] == x.shape[1]:\n            mask = mask[:, :-1]\n            kwargs['mask'] = mask\n\n        out = self.net(inp, **kwargs)\n\n        loss = self.loss_fn(out, target)\n\n        if exists(mask):\n            assert loss.ndim > 1, 'loss should not be reduced if mask is passed in'\n\n            if self.equal_loss_weight_batch:\n                loss = masked_mean(loss, mask)\n            else:\n                loss = loss[mask]\n\n        return loss.mean()", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/continuous.py", "name": "ContinuousAutoregressiveWrapper", "line": 239}}
{"prompt": "Create a dpo neural network module", "code": "class DPO(Module):\n    def __init__(\n        self,\n        model: TransformerWrapper,\n        *,\n        beta = 0.1,\n        pad_id = None\n    ):\n        super().__init__()\n        self.policy_model = model\n\n        self.ref_model = deepcopy(model)\n        freeze_all_layers_(self.ref_model)\n\n        self.beta = beta\n        self.pad_id = pad_id\n\n    def parameters(self):\n        return self.policy_model.parameters()\n\n    def forward(\n        self,\n        preferred_seq,\n        unpreferred_seq,\n        *,\n        prompt_mask,\n        preferred_seq_mask = None,\n        unpreferred_seq_mask = None,\n    ):\n        assert preferred_seq.ndim == 2\n        assert preferred_seq.shape == unpreferred_seq.shape\n\n        if exists(self.pad_id):\n            if not exists(preferred_seq_mask):\n                preferred_seq_mask = preferred_seq != self.pad_id\n\n            if not exists(unpreferred_seq_mask):\n                unpreferred_seq_mask = unpreferred_seq != self.pad_id\n\n        \"\"\"\n        Following Appendix B in https://arxiv.org/abs/2305.18290\n        \"\"\"\n\n        with torch.no_grad():\n            self.ref_model.eval()\n            ref_preferred_logprob = log_prob_from_model_and_seq(self.ref_model, preferred_seq)\n            ref_unpreferred_logprob = log_prob_from_model_and_seq(self.ref_model, unpreferred_seq)\n\n        policy_preferred_logprob = log_prob_from_model_and_seq(self.policy_model, preferred_seq)\n        policy_unpreferred_logprob = log_prob_from_model_and_seq(self.policy_model, unpreferred_seq)\n\n        # masked mean of log probs\n\n        preferred_seq_mask = maybe_and_mask(~prompt_mask, preferred_seq_mask)\n        unpreferred_seq_mask = maybe_and_mask(~prompt_mask, unpreferred_seq_mask)\n\n        ref_preferred_logprob, policy_preferred_logprob = map(lambda t: masked_mean(t, preferred_seq_mask), (ref_preferred_logprob, policy_preferred_logprob))\n        ref_unpreferred_logprob, policy_unpreferred_logprob = map(lambda t: masked_mean(t, unpreferred_seq_mask), (ref_unpreferred_logprob, policy_unpreferred_logprob))\n\n        # main dpo formula\n\n        policy_logratios = policy_preferred_logprob - policy_unpreferred_logprob\n        ref_logratios = ref_preferred_logprob - ref_unpreferred_logprob\n\n        losses = -F.logsigmoid(self.beta * (policy_logratios - ref_logratios))\n\n        return losses.mean()", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/dpo.py", "name": "DPO", "line": 51}}
{"prompt": "Create a entropy based tokenizer neural network module", "code": "class EntropyBasedTokenizer(Module):\n    def __init__(\n        self,\n        decoder: Module,\n        entropy_threshold: float,\n        max_token_size: int | None = None\n    ):\n        super().__init__()\n        self.decoder = decoder\n        self.entropy_threshold = entropy_threshold\n\n        self.max_token_size = max_token_size\n\n    @torch.no_grad()\n    def forward(\n        self,\n        seq,            # Float['b n'] | Float['n']\n        lens = None,    # Int['b']\n        return_segmented_seq = False,\n        decoder_forward_kwargs: dict = dict()\n    ):\n        no_batch_dim = seq.ndim == 1\n        seq, maybe_batch_ps = pack((seq,), '* n')\n\n        self.decoder.eval()\n\n        is_var_length = exists(lens)\n        batch, seq_len, device, max_token_size = *seq.shape, seq.device, self.max_token_size\n\n        arange = torch.arange(seq_len, device = device)\n\n        # forward through a small trained decoder and get the entropies of the logits\n\n        logits = self.decoder(seq, **decoder_forward_kwargs)\n\n        entropies = calc_entropy_from_logits(logits)\n\n        # get length mask for boundaries\n\n        mask = tensor(True, device = device)\n\n        if is_var_length:\n            mask = einx.less('n, b -> b n', arange, lens)\n\n        # the mask for tokens that were of a sufficient surprise level\n\n        over_thres_mask = (entropies >= self.entropy_threshold) & mask\n\n        # needed for selecting out indices at entropy threshold mask\n\n        arange_plus_one = arange + 1\n        arange_plus_one = repeat(arange_plus_one, 'n -> b n', b = batch)\n\n        # get a tensor of Int['b num_tokens'] with the token lengths, zero padded\n\n        boundaries = over_thres_mask.clone()\n\n        # set the boundary of the last token\n\n        # if `lens` not given, assume always last token\n        # but if `lens` were given, then properly set the index\n\n        if not is_var_length:\n            boundaries[..., -1] = True\n        else:\n            scatter_indices = rearrange(lens - 1, 'b -> b 1')\n            boundaries.scatter_(-1, scatter_indices, True)\n\n        # handle max token size - technique has the flaw that repeating subsequences are grouped into one large token\n\n        if exists(max_token_size):\n            token_ids = boundaries.cumsum(dim = -1)\n            token_ids = F.pad(token_ids, (1, -1), value = 0)\n\n            max_num_tokens = boundaries.sum(dim = -1).amax().item()\n            token_ids_seq = torch.arange(max_num_tokens, device = device)\n\n            token_mask = einx.equal('j, b i -> b j i', token_ids_seq, token_ids)\n\n            token_sub_seq_arange = token_mask.cumsum(dim = -1)\n\n            sub_seq_boundaries = (token_sub_seq_arange % max_token_size == 0)\n            sub_seq_boundaries = (sub_seq_boundaries & token_mask).any(dim = 1)\n\n            boundaries = boundaries | sub_seq_boundaries\n\n            if exists(mask):\n                boundaries = boundaries & mask\n\n        # number of tokens\n\n        num_tokens = boundaries.sum(dim = -1)\n\n        # get number of tokens as well as derived indices\n\n        indices = arange_plus_one[boundaries].split(num_tokens.tolist())\n\n        # get the token lengths\n\n        token_lengths = []\n\n        for one_indices in indices:\n            padded_indices = F.pad(one_indices, (1, 0), value = 0.)\n            one_token_lengths = padded_indices[1:] - padded_indices[:-1]\n\n            token_lengths.append(one_token_lengths)\n\n        token_lengths = pad_sequence(token_lengths, batch_first = True)\n\n        # early return\n\n        if not return_segmented_seq:\n            token_lengths, = unpack(token_lengths, maybe_batch_ps, '* num_tokens')\n\n            return token_lengths\n\n        # segment the sequence based on the token lengths\n\n        lens = default(lens, (None,))\n        segmented_seq = []\n\n        for one_seq, one_len, one_token_length in zip_longest(seq, lens, token_lengths):\n\n            if exists(one_len):\n                one_seq = one_seq[:one_len]\n\n            one_token_length = one_token_length[one_token_length > 0]\n\n            splitted_seq = one_seq.split(one_token_length.tolist())\n            segmented_seq.append(splitted_seq)\n\n        if no_batch_dim:\n            segmented_seq = segmented_seq[0]\n\n        return segmented_seq", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/entropy_based_tokenizer.py", "name": "EntropyBasedTokenizer", "line": 33}}
{"prompt": "Create a binary mapper neural network module", "code": "class BinaryMapper(Module):\n    def __init__(\n        self,\n        bits = 1,\n        kl_loss_threshold = NAT # 1 bit\n    ):\n        super().__init__()\n\n        self.bits = bits\n        self.num_codes = 2 ** bits\n\n        power_two = 2 ** arange(bits)\n        codes = (arange(self.num_codes)[:, None].bitwise_and(power_two) != 0).byte().bool()\n\n        self.register_buffer('power_two', power_two, persistent = False)\n        self.register_buffer('codes', codes, persistent = False)\n\n        # aux loss\n\n        self.kl_loss_threshold = kl_loss_threshold\n        self.register_buffer('zero', tensor(0.), persistent = False)\n\n    def forward(\n        self,\n        logits,\n        temperature = 1.,\n        straight_through = None,\n        calc_aux_loss = None\n    ):\n        straight_through = default(straight_through, self.training)\n        calc_aux_loss = default(calc_aux_loss, self.training)\n\n        assert logits.shape[-1] == self.bits, f'logits must have a last dimension of {self.bits}'\n\n        # temperature and prob for sampling\n\n        prob_for_sample = (logits / temperature).sigmoid()\n\n        # sampling\n\n        sampled_bits = (torch.rand_like(logits) <= prob_for_sample).long()\n        indices = (self.power_two * sampled_bits).sum(dim = -1)\n\n        one_hot = F.one_hot(indices, self.num_codes).float()\n\n        # maybe calculate aux loss\n\n        aux_kl_loss = self.zero\n\n        if calc_aux_loss:\n            # calculate negative entropy\n\n            kl_div = self.bits * NAT - binary_entropy(logits)\n            aux_kl_loss = F.relu(kl_div - self.kl_loss_threshold).mean()\n\n        # maybe straight through\n\n        if straight_through:\n            # get the soft G for the gradients and do a straight through\n\n            soft_G = (\n                einsum(F.logsigmoid(logits), self.codes.float(), '... bits, codes bits -> ... codes') +\n                einsum(F.logsigmoid(-logits), (~self.codes).float(), '... bits, codes bits -> ... codes')\n            ).exp()\n\n            # straight through\n\n            one_hot = one_hot + soft_G - soft_G.detach()\n\n        return one_hot, aux_kl_loss", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/free_transformer.py", "name": "BinaryMapper", "line": 59}}
{"prompt": "Create a free transformer neural network module", "code": "class FreeTransformer(Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        dec_head_depth,\n        dec_tail_depth,\n        max_seq_len,\n        enc_depth = 1,\n        dim_latent = None,\n        attn_dim_head = 64,\n        heads = 8,\n        latent_bits = 16,\n        per_token_latents = True,  # they use a latent per token in the sequence, instead of one for entire sequence, iiuc\n        kl_loss_threshold = NAT,\n        binary_mapper_kwargs: dict = dict(),\n        enc_kwargs: dict = dict(),\n        dec_kwargs: dict = dict(),\n        kl_loss_weight = 1.,\n        latent_dropout_prob = 0.,\n        pad_id = -1,\n        **kwargs\n    ):\n        super().__init__()\n        dim_latent = default(dim_latent, dim)\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n\n        self.token_unembed = nn.Linear(dim, num_tokens, bias = False)\n\n        self.query_token_for_latents = nn.Parameter(torch.randn(dim) * 1e-2)\n\n        self.per_token_latents = per_token_latents\n\n        self.encoder = Encoder(\n            dim = dim,\n            depth = enc_depth,\n            attn_dim_head = attn_dim_head,\n            heads = heads,\n            only_cross = True,\n            cross_attend = True,\n            use_rmsnorm = True,\n            rotary_pos_emb = True,\n            pre_norm_has_final_norm = True,\n            **kwargs,\n            **enc_kwargs\n        )\n\n        self.to_latent_bit_logits = nn.Linear(dim, latent_bits, bias = False)\n\n        self.binary_mapper = BinaryMapper(\n            latent_bits,\n            kl_loss_threshold,\n            **binary_mapper_kwargs\n        )\n\n        self.from_latent_to_condition = nn.Linear(self.binary_mapper.num_codes, dim, bias = False)\n\n        self.latent_dropout = nn.Dropout(latent_dropout_prob)\n\n        self.decoder_head = Decoder(\n            dim = dim,\n            depth = dec_head_depth,\n            attn_dim_head = attn_dim_head,\n            heads = heads,\n            rotary_pos_emb = True,\n            use_rmsnorm = True,\n            pre_norm_has_final_norm = False,\n            **kwargs,\n            **dec_kwargs\n        ) if dec_head_depth > 0 else None\n\n        assert dec_tail_depth > 0\n\n        self.decoder_tail = Decoder(\n            dim = dim,\n            depth = dec_tail_depth,\n            attn_dim_head = attn_dim_head,\n            heads = heads,\n            rotary_pos_emb = True,\n            use_rmsnorm = True,\n            pre_norm_has_final_norm = True,\n            **kwargs,\n            **dec_kwargs\n        )\n\n        self.pad_id = pad_id\n\n        self.kl_loss_weight = kl_loss_weight\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def encode_to_latents(\n        self,\n        decoder_head_embeds,\n        mask = None,\n        return_kl_loss = False,\n        per_token_latents = None\n    ):\n        per_token_latents = default(per_token_latents, self.per_token_latents)\n\n        batch, seq_len, device = *decoder_head_embeds.shape[:2], decoder_head_embeds.device\n\n        query_tokens = repeat(self.query_token_for_latents, 'd -> b 1 d', b = batch)\n\n        encoder_kwargs = dict()\n\n        # handle the interesting per query token latents, as in the paper\n\n        if per_token_latents:\n            query_tokens = repeat(query_tokens, 'b 1 d -> b n d', n = seq_len)\n\n            rotary_pos = torch.arange(seq_len, device = device)\n\n            encoder_kwargs.update(\n                pos = rotary_pos,\n                context_pos = rotary_pos\n            )\n\n        pooled = self.encoder(\n            query_tokens,\n            context = decoder_head_embeds,\n            context_mask = mask,\n            **encoder_kwargs\n        )\n\n        bit_logits = self.to_latent_bit_logits(pooled)\n\n        one_hot_latents, kl_loss = self.binary_mapper(bit_logits, calc_aux_loss = return_kl_loss)\n\n        if not return_kl_loss:\n            return one_hot_latents\n\n        return one_hot_latents, kl_loss\n\n    @torch.no_grad()\n    def generate(\n        self,\n        prompts,\n        seq_len,\n        latents = None,\n        filter_logits_fn = top_p,\n        logit_filter_kwargs: dict = dict(thres = 0.9),\n        use_kv_cache = True\n    ):\n        prompts, inverse_pack = pack_with_inverse(prompts, '* n')\n\n        batch = prompts.shape[0]\n\n        # prepend embeds\n\n        condition = None\n        if exists(latents):\n            if not is_tensor(latents):\n                latents = tensor(latents, device = self.device)\n\n            if latents.dtype in (torch.int, torch.long):\n                # if given as indices\n                latents = F.one_hot(latents, self.binary_mapper.num_codes).float()\n\n            if latents.ndim == 1: # repeat latents\n                latents = repeat(latents, 'd -> b 1 d', b = batch)\n            elif latents.ndim == 2:\n                latents = rearrange(latents, 'b d -> b 1 d')\n\n            condition = self.from_latent_to_condition(latents)\n\n        # kv cache\n\n        head_cache = tail_cache = None\n\n        # generated\n\n        prompt_len = prompts.shape[-1]\n\n        generated = prompts\n\n        tokens = self.token_emb(generated)\n\n        for _ in range(max(0, seq_len - prompt_len)):\n\n            # head, which may not exist\n\n            if exists(self.decoder_head):\n                head_embed, next_head_cache = self.decoder_head(tokens, cache = head_cache, return_hiddens = True)\n            else:\n                head_embed, next_head_cache = tokens, None\n\n            # handle one token being given to the decoder tail when doing kv caching - rotary embedding needs to know the seq position offset\n\n            seq_pos_offset = head_cache.cache_length if exists(head_cache) else 0\n\n            # tail\n\n            tail_embed, next_tail_cache = self.decoder_tail(head_embed, cache = tail_cache, seq_pos_offset = seq_pos_offset, self_attn_kv_residuals = condition, return_hiddens = True)\n\n            tail_embed = tail_embed[:, -1]\n\n            logits = self.token_unembed(tail_embed)\n\n            logits = filter_logits_fn(logits, **logit_filter_kwargs)\n\n            sampled = gumbel_sample(logits)\n\n            generated, _ = pack((generated, sampled), 'b *')\n            tokens, _ = pack((tokens, self.token_emb(sampled)), 'b * d')\n\n            if use_kv_cache:\n                head_cache = next_head_cache\n                tail_cache = next_tail_cache\n\n        return inverse_pack(generated)\n\n    def forward(\n        self,\n        seq,\n        seq_for_latents = None,\n        return_all_losses = False\n    ):\n        batch, device = seq.shape[0], seq.device\n\n        seq, labels = seq[:, :-1], seq[:, 1:]\n\n\n        tokens = self.token_emb(seq)\n\n        # decoder head\n\n        if exists(self.decoder_head):\n            tokens = self.decoder_head(tokens)\n\n        # determine whether to use a separate sequence for encoding latents\n\n        if exists(seq_for_latents):\n            tokens_for_latents = self.token_emb(seq_for_latents)\n\n            if exists(self.decoder_head):\n                tokens_for_latents = self.decoder_head(tokens_for_latents)\n\n            encoder_mask = seq_for_latents != self.pad_id\n            per_token_latents = False\n        else:\n\n            tokens_for_latents = tokens\n            encoder_mask = seq != self.pad_id\n            per_token_latents = None\n\n        # get latent Z\n\n        latents, kl_loss = self.encode_to_latents(tokens_for_latents, mask = encoder_mask, per_token_latents = per_token_latents, return_kl_loss = True)\n\n        latents = self.latent_dropout(latents)\n\n        condition = self.from_latent_to_condition(latents)\n\n        # decoder tail\n\n        tokens = self.decoder_tail(tokens, self_attn_kv_residuals = condition)\n\n        # cross entropy loss\n\n        logits = self.token_unembed(tokens)\n\n        ar_loss = F.cross_entropy(\n            rearrange(logits, 'b n l -> b l n'),\n            labels,\n            ignore_index = self.pad_id\n        )\n\n        # return losses\n\n        total_loss = (\n            ar_loss +\n            kl_loss * self.kl_loss_weight\n        )\n\n        if not return_all_losses:\n            return total_loss\n\n        losses = (ar_loss, kl_loss)\n\n        return total_loss, losses", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/free_transformer.py", "name": "FreeTransformer", "line": 132}}
{"prompt": "Create a gptvae neural network module", "code": "class GPTVAE(Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        depth,\n        enc_depth,\n        max_seq_len,\n        dim_latent = None,\n        attn_dim_head = 64,\n        heads = 8,\n        enc_kwargs: dict = dict(),\n        dec_kwargs: dict = dict(),\n        vae_kl_loss_weight = 1.,\n        vae_kl_div_floor = 0.,      # what was done in free transformer, which in turn came from Kingma 2016\n        latents_dropout_prob = 0.5, # what percentage of the time to dropout the latents completely\n        pad_id = -1,\n        encoder: Module | None = None,\n        **kwargs\n    ):\n        super().__init__()\n        dim_latent = default(dim_latent, dim)\n\n        if not exists(encoder):\n            encoder = TransformerWrapper(\n                num_tokens = num_tokens,\n                max_seq_len = max_seq_len + 1,\n                return_only_embed = True,\n                average_pool_embed = True,\n                attn_layers = Encoder(\n                    dim = dim,\n                    depth = enc_depth,\n                    attn_dim_head = attn_dim_head,\n                    heads = heads,\n                    **kwargs,\n                    **enc_kwargs\n                ),\n            )\n\n        self.encoder = encoder\n\n        self.to_latent_mean_log_variance = nn.Sequential(\n            nn.Linear(dim, dim_latent * 2),\n            Rearrange('b (two d) -> two b d', two = 2)\n        )\n\n        self.from_latent_to_prepend_token = nn.Sequential(\n            nn.Linear(dim_latent, dim),\n            Rearrange('b d -> b 1 d')\n        )\n\n        self.decoder = TransformerWrapper(\n            num_tokens = num_tokens,\n            max_seq_len = max_seq_len,\n            attn_layers = Decoder(\n                dim = dim,\n                depth = depth,\n                attn_dim_head = attn_dim_head,\n                heads = heads,\n                **kwargs,\n                **dec_kwargs\n            ),\n        )\n\n        self.ar_wrapped_decoder = AutoregressiveWrapper(self.decoder, ignore_index = pad_id)\n\n        self.pad_id = pad_id\n\n        # loss weights - vae kl loss\n\n        self.vae_kl_div_floor = vae_kl_div_floor\n        self.vae_kl_loss_weight = vae_kl_loss_weight\n\n        self.latents_dropout = nn.Dropout(latents_dropout_prob)\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def encode_to_latents(\n        self,\n        seq,\n        return_mean_log_var = False\n    ):\n        mask = seq != self.pad_id\n        pooled = self.encoder(seq, mask = mask)\n\n        latents_mean, latents_log_var = self.to_latent_mean_log_variance(pooled)\n        latents_std = (0.5 * latents_log_var).exp()\n\n        # reparam trick\n\n        latents = latents_mean + latents_std * torch.randn_like(latents_mean)\n\n        if not return_mean_log_var:\n            return latents\n\n        return latents, (latents_mean, latents_log_var)\n\n    @torch.no_grad()\n    def generate(\n        self,\n        prompts,\n        seq_len,\n        latents = None,\n        seq_for_latents = None,\n        **generate_kwargs\n    ):\n        assert prompts.ndim in {1, 2}\n        batch = prompts.shape[0] if prompts.ndim == 2 else 1\n\n        # if seq_for_latents passed in, derive latents from it\n\n        if exists(seq_for_latents):\n            assert not exists(latents), 'latents should not be passed in if given the seq from which to derive them'\n\n            latents = self.encode_to_latents(seq_for_latents)\n\n        # prepend embeds\n\n        prepend_embeds = None\n        if exists(latents):\n            if not is_tensor(latents):\n                latents = tensor(latents, device = self.device)\n\n            if latents.ndim == 1: # repeat latents\n                latents = repeat(latents, 'd -> b d', b = batch)\n\n            prepend_embeds = self.from_latent_to_prepend_token(latents)\n\n        # generated\n\n        generated = self.ar_wrapped_decoder.generate(\n            prompts,\n            seq_len,\n            prepend_embeds = prepend_embeds,\n            **generate_kwargs\n        )\n\n        return generated\n\n    def forward(\n        self,\n        seq,\n        seq_for_latents = None,\n        return_all_losses = False\n    ):\n        batch, device = seq.shape[0], seq.device\n\n        seq_for_latents = default(seq_for_latents, seq)\n\n        latents, (latents_mean, latents_log_var) = self.encode_to_latents(seq_for_latents, return_mean_log_var = True)\n\n        dropped_latents = ~self.latents_dropout(torch.ones((batch,), device = device)).bool()\n\n        prepend_embeds = self.from_latent_to_prepend_token(latents)\n\n        ar_loss = self.ar_wrapped_decoder(\n            seq,\n            prepend_embeds = prepend_embeds,\n            seq_start_pos = dropped_latents.long() # sequence starts at 1 and does not attend to the first style latent\n        )\n\n        # vae kl loss\n\n        vae_kl_loss = 0.5 * (\n            latents_log_var.exp()\n            + latents_mean.square()\n            - latents_log_var\n            - 1.\n        )\n\n        vae_kl_loss = F.relu(vae_kl_loss - self.vae_kl_div_floor)\n\n        vae_kl_loss = vae_kl_loss.sum(dim = -1).mean()\n\n        # return losses\n\n        total_loss = (\n            ar_loss +\n            vae_kl_loss * self.vae_kl_loss_weight\n        )\n\n        if not return_all_losses:\n            return total_loss\n\n        losses = (ar_loss, vae_kl_loss)\n\n        return total_loss, losses", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/gpt_vae.py", "name": "GPTVAE", "line": 32}}
{"prompt": "Create a multi input transformer wrapper neural network module", "code": "class MultiInputTransformerWrapper(Module):\n    def __init__(\n        self,\n        *,\n        num_tokens: Dict[str, int] = dict(),\n        max_seq_len,\n        attn_layers: AttentionLayers,\n        emb_dim = None,\n        max_mem_len = 0,\n        shift_mem_down = 0,\n        emb_dropout = 0.,\n        post_emb_norm = False,\n        num_memory_tokens = None,\n        memory_tokens_interspersed_every = None,\n        return_only_embed = False,\n        use_abs_pos_emb = True,\n        scaled_sinu_pos_emb = False,\n        emb_frac_gradient = 1., # GLM-130B and Cogview successfully used this, set at 0.1\n        attn_z_loss_weight = 1e-4,\n    ):\n        super().__init__()\n\n        dim = attn_layers.dim\n        emb_dim = default(emb_dim, dim)\n        self.emb_dim = emb_dim\n\n        self.max_seq_len = max_seq_len\n        self.max_mem_len = max_mem_len\n        self.shift_mem_down = shift_mem_down\n\n        no_abs_pos_emb = max_seq_len == 0 or not (use_abs_pos_emb and not attn_layers.disable_abs_pos_emb)\n\n        if no_abs_pos_emb:\n            self.pos_emb = always(0)\n        elif scaled_sinu_pos_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(emb_dim)\n        else:\n            self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len)\n\n        # additional embeddings - say type embedding from BERT        \n\n        self.embeds = ModuleDict({f'{name}_embed': nn.Embedding(one_num_tokens, emb_dim) for name, one_num_tokens in num_tokens.items()})\n\n        # fraction of the gradient that should go to the embedding, https://arxiv.org/abs/2105.13290\n\n        self.emb_frac_gradient = emb_frac_gradient\n\n        self.post_emb_norm = LayerNorm(emb_dim) if post_emb_norm else nn.Identity()\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n        self.attn_layers = attn_layers\n\n        # output head, usually to logits of num_tokens\n\n        if return_only_embed:\n            self.to_logits = None\n        else:\n            self.to_logits = ModuleDict({name: nn.Linear(dim, logits_dim, bias = False) for name, logits_dim in num_tokens.items()})\n\n        # memory tokens (like [cls]) from Memory Transformers paper\n\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.num_memory_tokens = num_memory_tokens\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n\n        self.memory_tokens_interspersed_every = memory_tokens_interspersed_every\n\n        # whether can do cached kv decoding\n\n        self.can_cache_kv = self.num_memory_tokens == 0\n        self.can_cache_kv_outside_max_seq_len = no_abs_pos_emb\n\n    def forward(\n        self,\n        x: Dict[str, Tensor],\n        return_embeddings = False,\n        return_logits_and_embeddings = False,\n        return_intermediates = False,\n        mask = None,\n        return_mems = False,\n        return_attn = False,\n        mems = None,\n        mem_masks = None,\n        pos = None,\n        prepend_embeds = None,\n        prepend_mask = None,\n        sum_embeds = None,\n        return_attn_z_loss = False,\n        attn_z_loss_weight = 1e-4,\n        seq_start_pos = None,\n        cache: LayerIntermediates | None = None,\n        **kwargs\n    ):\n        assert not is_empty(x)\n        first_input = list(x.values())[0]\n\n        b, n, device, num_mems, has_memory_tokens, emb_frac_gradient = *first_input.shape, first_input.device, self.num_memory_tokens, self.num_memory_tokens > 0, self.emb_frac_gradient\n\n        return_hiddens = return_mems | return_attn | return_intermediates | return_attn_z_loss\n        return_embeddings = return_embeddings | (not exists(self.to_logits))\n\n        # token embedding\n\n        assert len(x) == len(self.embeds)\n\n        token_emb = 0.\n\n        for name, embed_id in x.items():\n            embed_key = f'{name}_embed'\n\n            assert embed_key in self.embeds\n            embed = self.embeds[embed_key](embed_id)\n\n            token_emb = token_emb + embed\n\n        # absolute positional embedding\n\n        external_pos_emb = exists(pos) and pos.dtype != torch.long\n        pos_emb = self.pos_emb(first_input, pos = pos, seq_start_pos = seq_start_pos) if not external_pos_emb else pos        \n\n        token_emb = token_emb + pos_emb\n\n        # for summing embeddings passed externally - needs this for self-conditioning in non-autoregressive training\n\n        if exists(sum_embeds):\n            token_emb = token_emb + sum_embeds\n\n        # set back to `x`\n\n        x = token_emb\n\n        # post embedding norm, purportedly leads to greater stabilization\n\n        x = self.post_emb_norm(x)\n\n        # whether to append embeds, as in PaLI, for image embeddings\n\n        if exists(prepend_embeds):\n            prepend_seq, prepend_dim = prepend_embeds.shape[1:]\n            assert prepend_dim == x.shape[-1], 'prepended embeddings need to have same dimensions as text model dimensions'\n\n            x = torch.cat((prepend_embeds, x), dim = -2)\n\n            if exists(prepend_mask) or exists(mask):\n                mask = default(mask, lambda: torch.ones((b, n), device = device, dtype = torch.bool))\n                prepend_mask = default(prepend_mask, lambda: torch.ones((b, prepend_seq), device = device, dtype = torch.bool))\n\n                mask = torch.cat((prepend_mask, mask), dim = -1)\n\n        # whether to reduce the gradient going to the embedding, from cogview paper, corroborated by GLM-130B model\n\n        if emb_frac_gradient < 1:\n            assert emb_frac_gradient > 0\n            x = x * emb_frac_gradient + x.detach() * (1 - emb_frac_gradient)\n\n        # embedding dropout\n\n        x = self.emb_dropout(x)\n\n        x = self.project_emb(x)\n\n        if has_memory_tokens:\n            mem_every = self.memory_tokens_interspersed_every\n\n            if exists(mem_every):\n                assert mem_every > 0\n                assert isinstance(self.attn_layers, Decoder), 'only for decoder'\n                next_seq_len = math.ceil(n / mem_every) * mem_every\n\n                x = pad_at_dim(x, (0, next_seq_len - n), dim = -2, value = 0.)\n                x = rearrange(x, 'b (n m) d -> (b n) m d', m = mem_every)\n\n            mem = repeat(self.memory_tokens, 'n d -> b n d', b = x.shape[0])\n            x, mem_packed_shape = pack((mem, x), 'b * d')\n\n            # auto-handle masking after appending memory tokens\n            if not exists(mem_every) and exists(mask):\n                mask = pad_at_dim(mask, (num_mems, 0), dim = -1, value = True)\n\n            if exists(mem_every):\n                x = rearrange(x, '(b n) m d -> b (n m) d', b = b)\n\n        if self.shift_mem_down and exists(mems):\n            mems_l, mems_r = mems[:self.shift_mem_down], mems[self.shift_mem_down:]\n            mems = [*mems_r, *mems_l]\n\n        x, intermediates = self.attn_layers(x, mask = mask, mems = mems, mem_masks = mem_masks, cache = cache, return_hiddens = True, seq_start_pos = seq_start_pos, **kwargs)\n\n        # handle memories post-attention\n\n        if has_memory_tokens:\n            if exists(mem_every):\n                x = rearrange(x, 'b (n m) d -> (b n) m d', m = (mem_every + num_mems))\n\n            mem, x = unpack(x, mem_packed_shape, 'b * d')\n\n            intermediates.memory_tokens = mem\n\n            if exists(mem_every):\n                x = rearrange(x, '(b n) m d -> b (n m) d', b = b)\n\n            x = x[:, :n]\n\n        # projecting to logits\n\n        if not return_embeddings:\n            logits = {name: fn(x) for name, fn in self.to_logits.items()}\n\n        # different returns\n\n        if return_logits_and_embeddings:\n            out = (logits, x)\n        elif return_embeddings:\n            out = x\n        else:\n            out = logits\n\n        # aux loss\n\n        if return_attn_z_loss:\n            pre_softmax_attns = [t.pre_softmax_attn for t in intermediates.attn_intermediates]\n            intermediates.attn_z_loss = calc_z_loss(pre_softmax_attns, weight = attn_z_loss_weight)\n            return_intermediates = True\n\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = [torch.cat(pair, dim = -2) for pair in zip(mems, hiddens)] if exists(mems) else hiddens\n            new_mems = [t[..., -self.max_mem_len:, :].detach() for t in new_mems]\n\n            if not return_intermediates:\n                return out, new_mems\n\n            intermediates.mems = new_mems\n\n        if return_intermediates:\n            return out, intermediates\n\n        if return_attn:\n            attn_maps = [t.post_softmax_attn for t in intermediates.attn_intermediates]\n            return out, attn_maps\n\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/multi_input.py", "name": "MultiInputTransformerWrapper", "line": 34}}
{"prompt": "Create a random fourier embed neural network module", "code": "class RandomFourierEmbed(Module):\n\n    def __init__(self, dim):\n        super().__init__()\n        self.proj = nn.Linear(1, dim)\n        self.proj.requires_grad_(False)\n\n    def forward(\n        self,\n        times,\n    ):\n\n        times = rearrange(times, '... -> ... 1')\n        rand_proj = self.proj(times)\n        return torch.cos(2 * pi * rand_proj)", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/neo_mlp.py", "name": "RandomFourierEmbed", "line": 24}}
{"prompt": "https://openreview.net/forum?id=A8Vuf2e8y6", "code": "class NeoMLP(Module):\n    \"\"\" https://openreview.net/forum?id=A8Vuf2e8y6 \"\"\"\n    \"\"\" https://haian-jin.github.io/projects/LVSM/ \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_hidden,\n        dim_out,\n        dim_model,\n        depth,\n        encoder_kwargs: dict = dict(\n            attn_dim_head = 16,\n            heads = 4\n        )\n    ):\n        super().__init__()\n\n        # input and output embeddings\n\n        self.input_embed = nn.Parameter(torch.zeros(dim_in, dim_model))\n        self.hidden_embed = nn.Parameter(torch.zeros(dim_hidden, dim_model))\n        self.output_embed = nn.Parameter(torch.zeros(dim_out, dim_model))\n\n        nn.init.normal_(self.input_embed, std = 0.02)\n        nn.init.normal_(self.hidden_embed, std = 0.02)\n        nn.init.normal_(self.output_embed, std = 0.02)\n\n        # they use random fourier for continuous features\n\n        self.random_fourier = nn.Sequential(\n            RandomFourierEmbed(dim_model),\n            nn.Linear(dim_model, dim_model)\n        )\n\n        # hidden dimensions of mlp replaced with nodes with message passing\n        # which comes back to self attention as a fully connected graph.\n\n        self.transformer = Encoder(\n            dim = dim_model,\n            depth = depth,\n            **encoder_kwargs\n        )\n\n        # output\n\n        self.to_output_weights = nn.Parameter(torch.randn(dim_out, dim_model))\n        self.to_output_bias = nn.Parameter(torch.zeros(dim_out))\n\n    def forward(\n        self,\n        x,\n        return_embeds = False\n    ):\n        no_batch = x.ndim == 1\n\n        if no_batch:\n            x = rearrange(x, '... -> 1 ...')\n\n        batch = x.shape[0]\n\n        fouriered_input = self.random_fourier(x)\n\n        # add fouriered input to the input embedding\n\n        input_embed = fouriered_input + self.input_embed\n\n        hidden_embed, output_embed = tuple(repeat(t, '... -> b ...', b = batch) for t in (self.hidden_embed, self.output_embed))\n\n        # pack all the inputs into one string of tokens for self attention\n\n        embed, packed_shape = pack([input_embed, hidden_embed, output_embed], 'b * d')\n\n        # attention is all you need\n\n        embed = self.transformer(embed)\n\n        # unpack\n\n        input_embed, hidden_embed, output_embed = unpack(embed, packed_shape, 'b * d')\n\n        # project for output\n\n        output = einsum(output_embed, self.to_output_weights, 'b n d, n d -> b n')\n        output = output + self.to_output_bias\n\n        if no_batch:\n            output = rearrange(output, '1 ... -> ...')\n\n        if not return_embeds:\n            return output\n\n        return output, (input_embed, hidden_embed, output_embed)", "test_code": "", "difficulty": "medium", "category": "mlp", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/neo_mlp.py", "name": "NeoMLP", "line": 42}}
{"prompt": "Create a self critic neural network module", "code": "class SelfCritic(Module):\n    def __init__(self, net):\n        super().__init__()\n        self.net = net\n\n        dim = net.attn_layers.dim\n        self.to_logits = nn.Linear(dim, 1)\n\n    def forward(self, x):\n        embed = self.net(x, return_embeddings = True)\n        return self.to_logits(embed)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/nonautoregressive_wrapper.py", "name": "SelfCritic", "line": 87}}
{"prompt": "https://arxiv.org/abs/1904.09324", "code": "class NonAutoregressiveWrapper(Module):\n    \"\"\"\n    https://arxiv.org/abs/1904.09324\n    https://arxiv.org/abs/2202.04200\n    \"\"\"\n\n    def __init__(\n        self,\n        net,\n        *,\n        mask_id,\n        steps = 18,\n        self_cond = False,\n        self_cond_train_prob = 0.75,\n        no_replace_prob = 0.15,          # which percentage of the tokens masked will stay the same, done in original MLM paper\n        random_token_prob = 0.1,         # which percentage of tokens to be replaced with random token, done in original MLM paper\n        schedule = 'linear',\n        can_mask_prev_unmasked = False,  # when unmasking, whether it can remask previously unmasked\n        token_critic: TransformerWrapper | None = None,\n        self_token_critic = False,\n        critic_loss_weight = 1.,\n        use_simple_mdlm_loss_weight = True # Sahoo et al. https://arxiv.org/abs/2406.07524\n    ):\n        super().__init__()\n        assert not (self_token_critic and exists(token_critic))\n\n        self.net = net\n\n        dim = net.emb_dim\n        self.dim = dim\n        self.num_tokens = net.num_tokens\n\n        self.mask_id = mask_id\n\n        # afaict, maskgit paper did not do this\n        # but may help for self conditioning, as used successfully in original BERT\n\n        self.no_replace_prob = no_replace_prob\n        self.random_token_prob = random_token_prob\n\n        self.max_seq_len = net.max_seq_len\n        self.steps = steps\n\n        if callable(schedule):\n            self.schedule_fn = schedule\n        if schedule == 'linear':\n            self.schedule_fn = linear_schedule\n        elif schedule == 'cosine':\n            self.schedule_fn = cosine_schedule\n        else:\n            raise ValueError(f'invalid schedule {schedule}')\n\n        # whether to use the loss weighting proposed in simple diffusion lm paper\n\n        self.loss_weight_fn = None\n\n        if use_simple_mdlm_loss_weight:\n            grad_and_value_schedule_fn = vmap(grad_and_value(self.schedule_fn))\n\n            # eq (10)\n\n            def loss_weight_fn(times):\n                grad, value = grad_and_value_schedule_fn(times)\n                return grad / (1. - value)\n\n            self.loss_weight_fn = loss_weight_fn\n\n        # whether to mask previous - in the simple mdlm paper, they chose not to\n\n        self.can_mask_prev_unmasked = can_mask_prev_unmasked\n\n        # self conditioning\n\n        self.self_cond = self_cond\n\n        if self_cond:\n            self.null_embed = nn.Parameter(torch.randn(dim))\n            self.to_self_cond = nn.Linear(dim, dim, bias = False) if self_cond else None\n            self.self_cond_train_prob = self_cond_train_prob\n\n        # token critic\n\n        self.token_critic = token_critic\n\n        if self_token_critic:\n            self.token_critic = SelfCritic(net)\n\n        self.critic_loss_weight = critic_loss_weight\n\n    @torch.no_grad()\n    def generate(\n        self,\n        batch_size = None,\n        start_temperature = 1.,\n        filter_thres = 0.7,\n        noise_level_scale = 1.,\n        **kwargs\n    ):\n        sample_one = not exists(batch_size)\n        batch_size = default(batch_size, 1)\n\n        device = next(self.net.parameters()).device\n\n        was_training = self.training\n        self.eval()\n\n        times = torch.linspace(0., 1., self.steps + 1)\n\n        # sequence starts off as all masked\n\n        shape = (batch_size, self.max_seq_len)\n\n        seq = torch.full(shape, self.mask_id, device = device)\n        mask = torch.full(shape, True, device = device)\n\n        # slowly demask\n\n        all_mask_num_tokens = (self.schedule_fn(times[1:]) * self.max_seq_len).long()\n\n        # self conditioning\n\n        has_self_cond = self.self_cond\n        last_embed = self.null_embed if has_self_cond else None\n\n        for mask_num_tokens, steps_until_x0 in zip(all_mask_num_tokens.tolist(), reversed(range(self.steps))):\n\n            self_cond = self.to_self_cond(last_embed) if has_self_cond else None\n\n            logits, embeds = self.net(\n                seq,\n                sum_embeds = self_cond,\n                return_logits_and_embeddings = True,\n                **kwargs\n            )\n\n            if has_self_cond:\n                last_embed = embeds\n\n            if exists(filter_thres):\n                logits = top_k(logits, filter_thres)\n\n            annealing_scale = steps_until_x0 / self.steps\n            temperature = start_temperature * annealing_scale\n\n            probs = (logits / max(temperature, 1e-3)).softmax(dim = -1)\n\n            sampled_ids = gumbel_sample(logits, temperature = max(temperature, 1e-3))\n\n            seq = torch.where(mask, sampled_ids, seq)\n\n            if exists(self.token_critic):\n                scores = self.token_critic(seq)\n                scores = rearrange(scores, 'b n 1 -> b n')\n                scores = scores + noise_level_scale * gumbel_noise(scores) * annealing_scale\n            else:\n                scores = 1 - logits.softmax(dim = -1)\n                scores = scores.gather(2, rearrange(sampled_ids, 'b n -> b n 1'))\n                scores = rearrange(scores, 'b n 1 -> b n')\n\n            if mask_num_tokens == 0:\n                pass\n\n            if not self.can_mask_prev_unmasked:\n                scores = scores.masked_fill(~mask, -torch.finfo(scores.dtype).max)\n\n            mask_indices = scores.topk(mask_num_tokens, dim = -1).indices\n            mask = torch.zeros_like(scores, dtype = torch.bool).scatter(1, mask_indices, True)\n            seq = seq.masked_fill(mask, self.mask_id)\n\n        self.train(was_training)\n\n        if sample_one:\n            seq = rearrange(seq, '1 n -> n')\n\n        return seq\n\n    def forward(\n        self,\n        x,\n        only_train_generator = False,\n        only_train_critic = False,\n        generator_sample_temperature = None,\n        **kwargs\n    ):\n        b, n, device = *x.shape, x.device\n        assert n == self.max_seq_len\n\n        orig_seq = x.clone()\n\n        rand_times = torch.empty(b, device = device).uniform_(0, 1)\n        batched_randperm = torch.rand((b, n), device = device).argsort(dim = -1).float()\n\n        rand_probs = self.schedule_fn(rand_times)\n        num_tokens_mask = (rand_probs * n).clamp(min = 1.)\n        mask = batched_randperm < rearrange(num_tokens_mask, 'b -> b 1')\n\n        # to ensure all tokens produce embeddings, instead of just the ones with [mask] input, as done in seminal BERT MLM paper\n        # potentially needed for self-conditioning (on embedding) to work well\n\n        replace_mask_id_mask = mask.clone()\n        frac_seq_left = 1.\n\n        if self.no_replace_prob > 0. and coin_flip():\n            frac_seq_left -= self.no_replace_prob\n\n            no_replace_prob_mask = get_mask_subset_prob(mask, self.no_replace_prob)\n            replace_mask_id_mask &= ~no_replace_prob_mask\n\n        if self.random_token_prob > 0. and coin_flip():\n            random_token_prob_mask = get_mask_subset_prob(replace_mask_id_mask, self.random_token_prob * frac_seq_left)\n            random_tokens = torch.randint(0, self.num_tokens, (b, n), device = device)\n\n            x = torch.where(random_token_prob_mask, random_tokens, x)\n            replace_mask_id_mask &= ~random_token_prob_mask\n\n        masked = torch.where(replace_mask_id_mask, self.mask_id, x)\n\n        # self conditioning\n\n        if self.self_cond:\n            self_cond = self.null_embed\n\n            if sample_prob(self.self_cond_train_prob):\n                with torch.no_grad():\n                    self_cond = self.net(masked, return_embeddings = True, **kwargs).detach()\n\n            kwargs.update(sum_embeds = self.to_self_cond(self_cond))\n\n        # logits\n\n        context = torch.no_grad if only_train_critic else nullcontext\n\n        with context():\n            logits = self.net(masked, **kwargs)\n\n        loss_fn = F.cross_entropy if not self.net.output_is_log_prob else F.nll_loss\n\n        # loss\n\n        if exists(self.loss_weight_fn):\n            # using simple mdlm loss weighting\n\n            loss = loss_fn(\n                rearrange(logits, 'b n l -> b l n'),\n                orig_seq,\n                reduction = 'none'\n            )\n\n            loss_weights = self.loss_weight_fn(rand_times)     # calculate loss weight\n            loss = einx.multiply('b n, b', loss, loss_weights) # apply loss weights\n\n            loss = loss[mask].mean()\n\n        else:\n            loss = loss_fn(\n                logits[mask],\n                orig_seq[mask],\n            )\n\n        if not exists(self.token_critic) or only_train_generator:\n            return Losses(loss, loss, None)\n\n        sampled_ids = gumbel_sample(logits, temperature = default(generator_sample_temperature, random()))\n        generated = torch.where(mask, sampled_ids, orig_seq)\n\n        critic_logits = self.token_critic(generated)\n        critic_labels = (sampled_ids != orig_seq).float()\n\n        critic_loss = F.binary_cross_entropy_with_logits(\n            rearrange(critic_logits, '... 1 -> ...'),\n            critic_labels\n        )\n\n        # determine losses to be returned based on what researcher wants to train\n\n        if only_train_critic:\n            total_loss = critic_loss\n            loss = None\n        else:\n            total_loss = loss + critic_loss * self.critic_loss_weight\n\n        return Losses(total_loss, loss,  critic_loss)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/nonautoregressive_wrapper.py", "name": "NonAutoregressiveWrapper", "line": 99}}
{"prompt": "Create a synthetic data generator neural network module", "code": "class SyntheticDataGenerator(Module):\n    def __init__(\n        self,\n        dim,\n        num_tokens,\n        max_seq_len = 512,\n        hidden_size = None,\n        use_gru = False,\n        network_klass = None\n    ):\n        super().__init__()\n\n        self.max_seq_len = max_seq_len\n\n        self.embed = nn.Embedding(num_tokens, dim)\n\n        hidden_size = default(hidden_size, dim)\n\n        default_network_klass = partial(LSTM if not use_gru else GRU, batch_first = True)\n        network_klass = default(network_klass, default_network_klass)\n\n        self.net = network_klass(dim, hidden_size)\n\n        self.to_logits = nn.Linear(dim, num_tokens, bias = False)\n\n        self.apply(self.init_)\n\n    def reset_(self):\n        for m in self.modules():\n            if hasattr(m, 'reset_parameters'):\n                m.reset_parameters()\n\n        self.apply(self.init_)\n\n    @torch.no_grad()\n    def init_(self, m):\n        if isinstance(m, nn.Linear):\n            m.weight *= uniform(0., 1.1) # he scales the lstm weights from 0 to 1.1\n\n    @torch.inference_mode()\n    @torch.compile\n    def generate(\n        self,\n        length,\n        seed = None,\n        condition = None,\n        temperature = 1e-4 # he uses a near greedy temperature\n    ):\n        assert exists(seed) or exists(condition)\n        prefix = [*filter(exists, (seed, condition))]\n        seq_len = self.max_seq_len\n\n        seq = torch.cat(prefix, dim = -1)\n\n        net_input = seq\n        hiddens = None\n\n        for _ in range(length):\n\n            logits, hiddens = self.forward(net_input, hiddens)\n\n            last_logit = logits[:, -1]\n            prob = (last_logit / temperature).softmax(dim = -1)\n\n            sampled = torch.multinomial(prob, 1)\n            net_input = sampled\n\n            seq = torch.cat((seq, sampled), dim = -1)\n\n        return seq[:, -seq_len:]\n\n    def forward(\n        self,\n        input,\n        hiddens = None\n    ):\n\n        tokens = self.embed(input)\n\n        embed, hidden = self.net(tokens, hiddens)\n\n        logits = self.to_logits(embed)\n\n        return logits, hidden", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/up_wrapper.py", "name": "SyntheticDataGenerator", "line": 58}}
{"prompt": "Create a universal pretrain wrapper neural network module", "code": "class UniversalPretrainWrapper(Module):\n    def __init__(\n        self,\n        model: TransformerWrapper,\n        data_generator: SyntheticDataGenerator | Module | None = None,\n        buffer_size = None,\n        num_reset = 20,\n        batch_size = 32,\n        seq_len = 512,\n        seed_length = 8,\n        reset_turing_machine_every = 0,\n        keep_buffer_on_cpu = False\n    ):\n        super().__init__()\n\n        self.model = model\n        self.ar_wrapped = AutoregressiveWrapper(model)\n\n        assert model.attn_layers.causal\n\n        num_tokens = model.num_tokens\n        dim = model.attn_layers.dim\n\n        if not exists(data_generator):\n            data_generator = SyntheticDataGenerator(\n                num_tokens = num_tokens,\n                dim = dim,\n                max_seq_len = seq_len\n            )\n\n        self.reset_turing_machine_every = reset_turing_machine_every\n\n        self.seq_len = seq_len\n        self.data_generator = data_generator\n\n        self.seed_length = seed_length\n        self.batch_size = batch_size\n\n        buffer_size = default(buffer_size, batch_size * 20)\n        assert buffer_size > batch_size, f'data buffer size must be greater than batch size'\n\n        assert divisible_by(num_reset, 2)\n        self.num_reset = num_reset\n\n        self.buffer_size = buffer_size\n\n        self.random_sequences_fn = partial(random_sequences, num_tokens, seq_len)\n\n        init_data_buffer = self.random_sequences_fn(buffer_size // 2, buffer_size // 2)\n\n        if keep_buffer_on_cpu:\n            self.synth_data_buffer = init_data_buffer\n        else:\n            self.register_buffer('synth_data_buffer', init_data_buffer)\n\n        self.register_buffer('step', tensor(0))\n\n    @property\n    def device(self):\n        return self.step.device\n\n    def get_rand_sequences_from_buffer(self, size = None):\n        size = default(size, self.batch_size)\n        rand_indices = randperm(self.buffer_size, device = self.device)[:size]\n        return self.synth_data_buffer[rand_indices]\n\n    def forward(self):\n        # following algorithm 1.\n\n        conditions = self.get_rand_sequences_from_buffer()\n\n        # get seeds, which appears to be random sequences with random crops of seed length\n\n        seeds = self.get_rand_sequences_from_buffer()\n\n        seq_arange = torch.arange(self.seed_length)\n        rand_offset = torch.randint(0, self.seq_len - self.seed_length, (self.batch_size,))\n        seq_start_pos = rand_offset[:, None] + seq_arange\n\n        batch_arange = torch.arange(self.batch_size, device = self.device)[:, None]\n        seeds = seeds[batch_arange, seq_start_pos]\n\n        # seed, condition to turing machine\n\n        generated = self.data_generator.generate(\n            self.seq_len,\n            condition = conditions.to(self.device),\n            seed = seeds.to(self.device)\n        )\n\n        self.step.add_(1)\n\n        # maybe reset turing machine\n\n        if self.reset_turing_machine_every > 0 and divisible_by(self.step.item(), self.reset_turing_machine_every):\n            self.data_generator.reset_()\n\n        # reset\n\n        if self.num_reset > 0:\n            buffer_to_reset = self.get_rand_sequences_from_buffer(self.num_reset)\n\n            with torch.no_grad():\n                reset_sequences = self.random_sequences_fn(self.num_reset // 2, self.num_reset // 2, device = self.device)\n                buffer_to_reset.copy_(reset_sequences)\n\n        # place \"enriched\" random generated sequences back\n\n        with torch.no_grad():\n            conditions.copy_(generated)\n\n        # sample yet again according to pseudocode\n\n        data = self.get_rand_sequences_from_buffer().to(self.device)\n\n        return self.ar_wrapped(data)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/up_wrapper.py", "name": "UniversalPretrainWrapper", "line": 145}}
{"prompt": "Create a xl autoregressive wrapper neural network module", "code": "class XLAutoregressiveWrapper(nn.Module):\n    def __init__(\n        self,\n        net,\n        ignore_index = -100,\n        pad_value = 0\n    ):\n        super().__init__()\n        self.pad_value = pad_value\n        self.ignore_index = ignore_index\n\n        self.net = net\n        self.max_seq_len = net.max_seq_len\n\n    @torch.no_grad()\n    @eval_decorator\n    def generate(\n        self,\n        start_tokens,\n        seq_len,\n        eos_token = None,\n        temperature = 1.,\n        filter_logits_fn = top_k,\n        filter_kwargs: dict = dict(),\n        mems = None,\n        **kwargs\n    ):\n        device, max_seq_len = start_tokens.device, self.max_seq_len\n\n        start_tokens, ps = pack([start_tokens], '* n')\n\n        b, t = start_tokens.shape\n\n        *all_leading_tokens, _ = start_tokens.split(max_seq_len, dim = -1)\n\n        # catch the memory up to the current segment\n\n        for leading_tokens in all_leading_tokens:\n            _, mems = self.net(\n                leading_tokens,\n                mems = mems,\n                return_mems = True,\n                **kwargs\n            )\n\n        # now start sampling from the current segment\n\n        curr_pos = len(all_leading_tokens) * max_seq_len\n        curr_mems = mems\n\n        cache = None\n        out = start_tokens\n\n        for _ in range(seq_len):\n            curr_segment_len = out.shape[-1]\n            is_last_segment_tokens = divisible_by(curr_segment_len, max_seq_len)\n\n            x = out[:, curr_pos:]\n\n            logits, cache = self.net(\n                x,\n                mems = curr_mems,\n                cache = cache,\n                return_mems = True,\n                return_intermediates = True,\n                **kwargs\n            )\n\n            mems = cache.mems\n\n            logits = logits[:, -1]\n            filtered_logits = filter_logits_fn(logits, **filter_kwargs)\n            probs = F.softmax(filtered_logits / temperature, dim=-1)\n\n            sample = torch.multinomial(probs, 1)\n\n            if is_last_segment_tokens:\n                curr_pos = curr_segment_len\n                curr_mems = mems\n\n            out = torch.cat((out, sample), dim=-1)\n\n            if exists(eos_token):\n                is_eos_tokens = (out == eos_token)\n\n                if is_eos_tokens.any(dim = -1).all():\n                    # mask out everything after the eos tokens\n                    shifted_is_eos_tokens = F.pad(is_eos_tokens, (1, -1))\n                    mask = shifted_is_eos_tokens.float().cumsum(dim = -1) >= 1\n                    out = out.masked_fill(mask, self.pad_value)\n                    break\n\n        out = out[:, t:]\n\n        out, = unpack(out, ps, '* n')\n\n        return out\n\n    def forward(\n        self,\n        x,\n        mems = None,\n        **kwargs\n    ):\n        ignore_index, max_seq_len = self.ignore_index, self.max_seq_len\n\n        x, labels = x[:, :-1], x[:, 1:]\n\n        seq_len = x.shape[1]\n\n        # prepare chunks\n\n        split_x = x.split(max_seq_len, dim = -1)\n        split_labels = labels.split(max_seq_len, dim = -1)\n        loss_weights = tuple((t.shape[-1] / seq_len) for t in split_x)\n\n        loss_fn = F.cross_entropy if not self.net.output_is_log_prob else F.nll_loss\n\n        # go through each chunk and derive weighted losses\n\n        total_loss = 0.        \n\n        for chunk, chunk_labels, loss_weight in zip(split_x, split_labels, loss_weights):\n\n            logits, mems = self.net(\n                chunk,\n                mems = mems,\n                return_mems = True,\n                **kwargs\n            )\n\n            loss = loss_fn(\n                rearrange(logits, 'b n c -> b c n'),\n                chunk_labels,\n                ignore_index = ignore_index\n            )\n\n            total_loss = total_loss + loss * loss_weight\n\n        return total_loss", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/xl_autoregressive_wrapper.py", "name": "XLAutoregressiveWrapper", "line": 20}}
{"prompt": "Create a so lu neural network module", "code": "class SoLU(Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.norm = LayerNorm(dim)\n\n    def forward(self, x):\n        activated = x.softmax(dim = -1) * x\n        return self.norm(activated)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "SoLU", "line": 293}}
{"prompt": "Create a token embedding neural network module", "code": "class TokenEmbedding(Module):\n    def __init__(self, dim, num_tokens, l2norm_embed = False):\n        super().__init__()\n        self.l2norm_embed = l2norm_embed\n        self.emb = nn.Embedding(num_tokens, dim)\n\n    def forward(self, x):\n        token_emb = self.emb(x.long())\n        return l2norm(token_emb) if self.l2norm_embed else token_emb\n\n    def init_(self):\n        if self.l2norm_embed:\n            nn.init.normal_(self.emb.weight, std=1e-5)\n            return\n        nn.init.kaiming_normal_(self.emb.weight)", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "TokenEmbedding", "line": 304}}
{"prompt": "Create a absolute positional embedding neural network module", "code": "class AbsolutePositionalEmbedding(Module):\n    def __init__(self, dim, max_seq_len, l2norm_embed = False):\n        super().__init__()\n        self.scale = dim ** -0.5 if not l2norm_embed else 1.\n        self.max_seq_len = max_seq_len\n        self.l2norm_embed = l2norm_embed\n        self.emb = nn.Embedding(max_seq_len, dim)\n\n    def forward(\n        self,\n        x,\n        pos = None,\n        seq_start_pos = None,\n        offset = 0\n    ):\n        seq_len, device = x.shape[1], x.device\n        assert seq_len <= self.max_seq_len, f'you are passing in a sequence length of {seq_len} but your absolute positional embedding has a max sequence length of {self.max_seq_len}'\n\n        if not exists(pos):\n            pos = arange(seq_len, device = device) + offset\n\n        if exists(seq_start_pos):\n            pos = (pos - seq_start_pos[..., None]).clamp(min = 0)\n\n        pos_emb = self.emb(pos)\n        pos_emb = pos_emb * self.scale\n        return l2norm(pos_emb) if self.l2norm_embed else pos_emb", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AbsolutePositionalEmbedding", "line": 322}}
{"prompt": "Create a scaled sinusoidal embedding neural network module", "code": "class ScaledSinusoidalEmbedding(Module):\n    def __init__(self, dim, theta = 10000):\n        super().__init__()\n        assert divisible_by(dim, 2)\n        self.scale = nn.Parameter(torch.ones(1) * dim ** -0.5)\n\n        half_dim = dim // 2\n        freq_seq = arange(half_dim).float() / half_dim\n        inv_freq = theta ** -freq_seq\n        self.register_buffer('inv_freq', inv_freq, persistent = False)\n\n    def forward(\n        self,\n        x,\n        pos = None,\n        seq_start_pos = None,\n        offset = 0\n    ):\n        seq_len, device = x.shape[1], x.device\n\n        if not exists(pos):\n            pos = arange(seq_len, device = device) + offset\n\n        if exists(seq_start_pos):\n            pos = pos - seq_start_pos[..., None]\n\n        emb = einsum('i, j -> i j', pos, self.inv_freq)\n        emb = cat((emb.sin(), emb.cos()), dim = -1)\n        return emb * self.scale", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ScaledSinusoidalEmbedding", "line": 350}}
{"prompt": "Create a relative position bias neural network module", "code": "class RelativePositionBias(Module):\n    def __init__(self, scale, causal = False, num_buckets = 32, max_distance = 128, heads = 8):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, causal = True, num_buckets = 32, max_distance = 128):\n        ret = 0\n        n = -relative_position\n        if not causal:\n            num_buckets //= 2\n            ret += (n < 0).long() * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = max_exact + (\n            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n        ).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, i, j):\n        device = self.device\n        q_pos = arange(j - i, j, dtype = torch.long, device = device)\n        k_pos = arange(j, dtype = torch.long, device = device)\n        rel_pos = einx.subtract('j, i -> i j', k_pos, q_pos)\n        rp_bucket = self._relative_position_bucket(rel_pos, causal = self.causal, num_buckets = self.num_buckets, max_distance = self.max_distance)\n        values = self.relative_attention_bias(rp_bucket)\n        bias = rearrange(values, 'i j h -> h i j')\n        return bias * self.scale", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "RelativePositionBias", "line": 380}}
{"prompt": "Appendix B of https://arxiv.org/abs/2405.18719", "code": "class CoPE(Module):\n    \"\"\"\n    Appendix B of https://arxiv.org/abs/2405.18719\n    \"\"\"\n    def __init__ (\n        self,\n        dim,\n        heads,\n        max_pos,\n        soft_onehot = False,\n        talking_heads = False,\n        soft_onehot_temp = 5e-2\n    ):\n        super () . __init__ ()\n        self.max_pos = max_pos\n        self.pos_emb = nn.Parameter(torch.zeros(max_pos, dim))\n\n        self.talking_heads = nn.Conv2d(heads, heads, 1, bias = False) if talking_heads else None\n        self.soft_onehot = soft_onehot\n        self.soft_onehot_temp = soft_onehot_temp\n\n        if not soft_onehot:\n            return\n\n        self.register_buffer('positions', arange(max_pos))\n\n    def forward(self, query, attn_logits):\n\n        if exists(self.talking_heads):\n            i, j = attn_logits.shape[-2:]\n            causal_mask = attn_logits.new_ones(i, j).triu_(j - i + 1).bool()\n\n            attn_logits = self.talking_heads(attn_logits)\n\n            attn_logits = attn_logits.masked_fill(causal_mask, -torch.finfo(attn_logits.dtype).max)\n\n        # compute positions\n\n        gates = attn_logits.sigmoid()\n\n        pos = gates.flip(-1).cumsum(dim = -1).flip(-1)\n        pos = pos.clamp(max = self.max_pos - 1)\n\n        logits_int = einsum('b h n d, p d -> b h n p', query, self.pos_emb)\n\n        if self.soft_onehot:\n            diff_pos = einx.subtract('i, j -> i j', pos, self.positions).abs()\n            soft_onehot_pos = F.softmax(-diff_pos / self.soft_onehot_temp, dim = -1)\n            cope_pos_emb = einsum('b h i j p, b h i p -> b h i j', soft_onehot_pos, logits_int)\n        else:\n            # interpolate from integer positions\n            pos_ceil = pos.ceil().long()\n            pos_floor = pos.floor().long()\n            logits_ceil = logits_int.gather(-1, pos_ceil)\n            logits_floor = logits_int.gather(-1, pos_floor)\n\n            w = pos - pos_floor\n            cope_pos_emb = logits_ceil * w + logits_floor * (1 - w)\n\n        return cope_pos_emb", "test_code": "", "difficulty": "medium", "category": "cnn", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "CoPE", "line": 425}}
{"prompt": "Create a dynamic position bias neural network module", "code": "class DynamicPositionBias(Module):\n    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n        super().__init__()\n        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n        self.log_distance = log_distance\n\n        self.mlp = ModuleList([])\n\n        self.mlp.append(Sequential(\n            nn.Linear(1, dim),\n            LayerNorm(dim) if norm else None,\n            nn.SiLU()\n        ))\n\n        for _ in range(depth - 1):\n            self.mlp.append(Sequential(\n                nn.Linear(dim, dim),\n                nn.LayerNorm(dim) if norm else None,\n                nn.SiLU()\n            ))\n\n        self.mlp.append(nn.Linear(dim, heads))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, i, j):\n        n, device = j, self.device\n\n        # get the (n x n) matrix of distances\n        seq_arange = arange(j - i, j, device = device)\n        context_arange = arange(j, device = device)\n        indices = einx.subtract('i, j -> i j', seq_arange, context_arange)\n        indices += (j - 1)\n\n        # input to continuous positions MLP\n        pos = arange(-j + 1, j, device = device).float()\n        pos = rearrange(pos, '... -> ... 1')\n\n        if self.log_distance:\n            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n\n        for layer in self.mlp:\n            pos = layer(pos)\n\n        # get position biases        \n        bias = pos[indices]\n        bias = rearrange(bias, 'i j h -> h i j')\n        return bias", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "DynamicPositionBias", "line": 486}}
{"prompt": "Create a alibi positional bias neural network module", "code": "class AlibiPositionalBias(Module):\n    def __init__(\n        self,\n        heads,\n        total_heads = None,\n        slopes: list[int] | None = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.heads = heads\n        self.total_heads = default(total_heads, heads)\n\n        slopes = Tensor(default(slopes, self._get_slopes(heads)))\n        slopes = rearrange(slopes, 'h -> h 1 1')\n\n        self.register_buffer('slopes', slopes, persistent = False)\n        self.register_buffer('bias', None, persistent = False)\n    \n    @property\n    def device(self):\n        return next(self.buffers()).device\n\n    @staticmethod\n    def _get_slopes(heads):\n        def get_slopes_power_of_2(n):\n            start = (2**(-2**-(math.log2(n)-3)))\n            ratio = start\n            return [start*ratio**i for i in range(n)]\n\n        if math.log2(heads).is_integer():\n            return get_slopes_power_of_2(heads)\n\n        closest_power_of_2 = 2 ** math.floor(math.log2(heads))\n        return get_slopes_power_of_2(closest_power_of_2) + get_slopes_power_of_2(2 * closest_power_of_2)[0::2][:heads-closest_power_of_2]\n\n    def forward_custom_pos(\n        self,\n        pos_i: Tensor,\n        pos_j: Tensor | None = None\n    ):\n        h, device = self.total_heads, self.device\n\n        pos_j = default(pos_j, pos_i)\n        bias = -einx.subtract('... j, ... i -> ... i j', pos_j, pos_i).abs()\n\n        if bias.ndim == 3:\n            bias = rearrange(bias, 'b i j -> b 1 i j')\n\n        bias = bias * self.slopes\n        num_heads_unalibied = h - bias.shape[-3]\n        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim = -3)\n\n        return bias\n\n    def forward(self, i, j):\n        h, device = self.total_heads, self.device\n\n        if exists(self.bias) and self.bias.shape[-1] >= j and self.bias.shape[-2] >= i:\n            return self.bias[..., -i:, -j:]\n\n        seq_arange = arange(j - i, j, device = device)\n        context_arange = arange(j, device = device)\n        bias = -einx.subtract('j, i -> 1 i j', context_arange, seq_arange).abs()\n\n        bias = bias * self.slopes\n        num_heads_unalibied = h - bias.shape[-3]\n        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim = -3)\n\n        self.register_buffer('bias', bias, persistent = False)\n        return self.bias", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AlibiPositionalBias", "line": 537}}
{"prompt": "https://openreview.net/forum?id=q2Lnyegkr8", "code": "class DataDependentAlibi(Module):\n    \"\"\" https://openreview.net/forum?id=q2Lnyegkr8 \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        heads,\n        causal = True,\n        bias_init = 5.,\n        post_log_scale = 1.,\n    ):\n        super().__init__()\n\n        self.causal = causal\n\n        linear = nn.Linear(dim, heads * (1 if causal else 2))\n\n        self.to_forget_gates = nn.Sequential(\n            linear,\n            Rearrange('b n h -> b h n'),\n            nn.LogSigmoid()\n        )\n\n        nn.init.constant_(linear.bias, bias_init)\n        self.post_log_scale = post_log_scale\n\n    def forward(self, x):\n        bidirectional = not self.causal\n\n        forget_gates = self.to_forget_gates(x) * self.post_log_scale\n\n        forget_gates = forget_gates.cumsum(dim = -1)\n\n        if bidirectional:\n            forget_gates, forget_gates_reversed = forget_gates.chunk(2, dim = 1)\n\n        forget_gates = einx.subtract('b h i, b h j -> b h i j', forget_gates, forget_gates)\n\n        if bidirectional:\n            forget_gates_reversed = einx.subtract('b h j, b h i -> b h i j', forget_gates_reversed, forget_gates_reversed)\n            forget_gates = forget_gates.tril() + forget_gates_reversed.triu()\n\n        return forget_gates", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "DataDependentAlibi", "line": 608}}
{"prompt": "same as data dependent alibi from forgetting transformer, but the forgetting gates are also derived by a queries and keys with a small head dimension", "code": "class PerRowDataDependentAlibi(Module):\n    \"\"\" same as data dependent alibi from forgetting transformer, but the forgetting gates are also derived by a queries and keys with a small head dimension \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        heads,\n        causal = True,\n        dim_head = 8,\n        post_log_scale = 1.\n    ):\n        super().__init__()\n        assert causal, 'bidirectional not supported yet'\n\n        self.scale = dim_head ** -0.5\n\n        linear = nn.Linear(dim, heads * dim_head * 2, bias = False)\n\n        self.to_forget_gates = nn.Sequential(\n            linear,\n            Rearrange('b n (qk h d) -> qk b h n d', qk = 2, d = dim_head)\n        )\n\n        self.post_log_scale = post_log_scale\n\n    def forward(self, x):\n        q, k = self.to_forget_gates(x)\n        forget_gates = einsum('... i d, ... j d -> ... i j', q, k) * self.scale\n\n        forget_gates = F.logsigmoid(forget_gates) * self.post_log_scale\n\n        # mask out upper triangle + diagonal\n\n        n = x.shape[-2]\n        causal_mask = torch.ones((n, n), dtype = torch.bool, device = x.device).triu()\n\n        forget_gates = forget_gates.masked_fill(causal_mask, 0.)\n\n        # reverse cumsum\n\n        forget_gates = forget_gates.flip(dims = (-1,))\n        forget_gates = forget_gates.cumsum(dim = -1)\n        forget_gates = forget_gates.flip(dims = (-1,))\n\n        return forget_gates", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "PerRowDataDependentAlibi", "line": 652}}
{"prompt": "Create a rotary embedding neural network module", "code": "class RotaryEmbedding(Module):\n    def __init__(\n        self,\n        dim,\n        use_xpos = False,\n        scale_base = 512,\n        interpolation_factor = 1.,\n        base = 10000,\n        base_rescale_factor = 1.\n    ):\n        super().__init__()\n        # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n        # has some connection to NTK literature\n        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n        base *= base_rescale_factor ** (dim / (dim - 2))\n\n        inv_freq = 1. / (base ** (arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n        assert interpolation_factor >= 1.\n        self.interpolation_factor = interpolation_factor\n\n        if not use_xpos:\n            self.register_buffer('scale', None)\n            return\n\n        scale = (arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n\n        self.scale_base = scale_base\n        self.register_buffer('scale', scale)\n\n    def forward_from_seq_len(self, seq_len):\n        device = self.inv_freq.device\n\n        t = arange(seq_len, device = device)\n        return self.forward(t)\n\n    @autocast('cuda', enabled = False)\n    def forward(self, t, offset = 0):\n        max_pos = t.max() + 1\n\n        if t.ndim == 1:\n            t = rearrange(t, 'n -> 1 n')\n\n        freqs = torch.einsum('b i , j -> b i j', t.type_as(self.inv_freq), self.inv_freq) / self.interpolation_factor\n        freqs = stack((freqs, freqs), dim = -1)\n        freqs = rearrange(freqs, '... d r -> ... (d r)')\n\n        if not exists(self.scale):\n            return freqs, 1.\n\n        power = (t - (max_pos // 2)) / self.scale_base\n        scale = self.scale ** rearrange(power, '... n -> ... n 1')\n        scale = stack((scale, scale), dim = -1)\n        scale = rearrange(scale, '... d r -> ... (d r)')\n\n        return freqs, scale", "test_code": "", "difficulty": "medium", "category": "embedding", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "RotaryEmbedding", "line": 698}}
{"prompt": "Create a scale neural network module", "code": "class Scale(Module):\n    def __init__(self, value, fn):\n        super().__init__()\n        self.value = value\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n        scale_fn = lambda t: t * self.value\n\n        if not isinstance(out, tuple):\n            return scale_fn(out)\n\n        return (scale_fn(out[0]), *out[1:])", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "Scale", "line": 784}}
{"prompt": "Create a layer norm neural network module", "code": "class LayerNorm(Module):\n    def __init__(\n        self,\n        dim,\n        unit_offset = False\n    ):\n        \"\"\"\n        bias-less layernorm has been shown to be more stable. most newer models have moved towards rmsnorm, also bias-less\n        \"\"\"\n        super().__init__()\n        self.unit_offset = unit_offset\n\n        self.ln = nn.LayerNorm(dim, elementwise_affine = False)\n        self.gamma = nn.Parameter(torch.ones(dim))\n        nn.init.constant_(self.gamma, 1. - float(unit_offset))\n\n    def forward(self, x):\n        normed = self.ln(x)\n        gamma = self.gamma + float(self.unit_offset)\n        return normed * gamma", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "LayerNorm", "line": 799}}
{"prompt": "Create a adaptive layer norm neural network module", "code": "class AdaptiveLayerNorm(Module):\n    def __init__(\n        self,\n        dim,\n        dim_condition = None\n    ):\n        super().__init__()\n        dim_condition = default(dim_condition, dim)\n\n        self.ln = nn.LayerNorm(dim, elementwise_affine = False)\n        self.to_gamma = LinearNoBias(dim_condition, dim)\n        nn.init.zeros_(self.to_gamma.weight)\n\n    def forward(self, x, *, condition):\n        if condition.ndim == 2:\n            condition = rearrange(condition, 'b d -> b 1 d')\n\n        normed = self.ln(x)\n        gamma = self.to_gamma(condition)\n        return normed * (gamma + 1.)", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AdaptiveLayerNorm", "line": 820}}
{"prompt": "Create a scale norm neural network module", "code": "class ScaleNorm(Module):\n    def __init__(\n        self,\n        dim,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.unit_offset = unit_offset\n        self.scale = dim ** 0.5\n\n        self.g = nn.Parameter(torch.zeros(1))\n        nn.init.constant_(self.g, 1. - float(unit_offset))\n\n    def forward(self, x):\n        gamma = self.g + float(self.unit_offset)\n        return F.normalize(x, dim = -1) * self.scale * gamma", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ScaleNorm", "line": 841}}
{"prompt": "Create a rms norm neural network module", "code": "class RMSNorm(Module):\n    def __init__(\n        self,\n        dim,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.unit_offset = unit_offset\n        self.scale = dim ** 0.5\n\n        self.g = nn.Parameter(torch.zeros(dim))\n        nn.init.constant_(self.g, 1. - float(unit_offset))\n\n    def forward(self, x):\n        gamma = self.g + float(self.unit_offset)\n        return F.normalize(x, dim = -1) * self.scale * gamma", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "RMSNorm", "line": 858}}
{"prompt": "Create a adaptive rms norm neural network module", "code": "class AdaptiveRMSNorm(Module):\n    def __init__(\n        self,\n        dim,\n        dim_condition = None\n    ):\n        super().__init__()\n        self.scale = dim ** 0.5\n        dim_condition = default(dim_condition, dim)\n\n        self.to_gamma = LinearNoBias(dim_condition, dim)\n        nn.init.zeros_(self.to_gamma.weight)\n\n    def forward(self, x, *, condition):\n        if condition.ndim == 2:\n            condition = rearrange(condition, 'b d -> b 1 d')\n\n        normed = F.normalize(x, dim = -1)\n        gamma = self.to_gamma(condition)\n        return normed * self.scale * (gamma + 1.)", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AdaptiveRMSNorm", "line": 875}}
{"prompt": "Create a simple rms norm neural network module", "code": "class SimpleRMSNorm(Module):\n    def __init__(\n        self,\n        dim,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim ** 0.5\n\n    def forward(self, x):\n        return F.normalize(x, dim = -1) * self.scale", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "SimpleRMSNorm", "line": 896}}
{"prompt": "Create a multihead rms norm neural network module", "code": "class MultiheadRMSNorm(Module):\n    def __init__(self, dim, heads):\n        super().__init__()\n        self.rmsnorm = SimpleRMSNorm(dim)\n        self.gamma = nn.Parameter(torch.zeros(heads, 1, dim))\n\n    def forward(self, x):\n        return self.rmsnorm(x) * (self.gamma + 1.)", "test_code": "", "difficulty": "medium", "category": "attention", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "MultiheadRMSNorm", "line": 908}}
{"prompt": "https://arxiv.org/abs/2503.10622", "code": "class DynamicTanh(Module):\n    \"\"\" https://arxiv.org/abs/2503.10622 \"\"\"\n    def __init__(\n        self,\n        dim,\n        init_alpha = 1.,\n        gamma = 1.,\n        beta = 0.,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.pre_tanh_scale = nn.Parameter(tensor(init_alpha))\n\n        self.gamma = nn.Parameter(torch.ones(dim))\n        self.beta = nn.Parameter(torch.zeros(dim))\n\n        self.pre_tanh_scale_offset = init_alpha if unit_offset else 0.\n        self.gamma_offset = float(unit_offset)\n\n        nn.init.constant_(self.pre_tanh_scale, 0 if unit_offset else init_alpha)\n        nn.init.constant_(self.gamma, 1. - float(unit_offset))\n\n    def forward(self, x):\n        pre_tanh_scale = self.pre_tanh_scale + self.pre_tanh_scale_offset\n        gamma = self.gamma + self.gamma_offset\n        return (x * pre_tanh_scale).tanh() * gamma + self.beta", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "DynamicTanh", "line": 917}}
{"prompt": "Create a residual neural network module", "code": "class Residual(Module):\n    def __init__(self, dim, scale_residual = False, scale_residual_constant = 1., **kwargs):\n        super().__init__()\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n        self.scale_residual_constant = scale_residual_constant\n\n    def prepare(self, residual):\n        return residual, residual, dict()\n\n    def forward(self, x, residual, **kwargs):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n\n        if self.scale_residual_constant != 1:\n            residual = residual * self.scale_residual_constant\n\n        return x + residual", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "Residual", "line": 946}}
{"prompt": "Create a gru gating neural network module", "code": "class GRUGating(Module):\n    def __init__(self, dim, scale_residual = False, **kwargs):\n        super().__init__()\n        self.gru = nn.GRUCell(dim, dim)\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n\n    def prepare(self, residual):\n        return residual, residual, dict()\n\n    def forward(self, x, residual, **kwargs):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n\n        gated_output = self.gru(\n            rearrange(x, 'b n d -> (b n) d'),\n            rearrange(residual, 'b n d -> (b n) d')\n        )\n\n        return gated_output.reshape_as(x)", "test_code": "", "difficulty": "medium", "category": "rnn", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "GRUGating", "line": 964}}
{"prompt": "Create a hyper connection neural network module", "code": "class HyperConnection(Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        layer_index,\n        num_residual_streams,\n        num_input_views = 1,\n        tanh = True,\n        **kwargs\n    ):\n        \"\"\"\n        https://arxiv.org/abs/2409.19606\n        Appendix J - Algorithm 2, Dynamic only\n        \"\"\"\n        super().__init__()\n\n        self.act = nn.Tanh() if tanh else nn.Identity()\n\n        self.norm = nn.LayerNorm(dim, bias = False)\n\n        self.num_residual_streams = num_residual_streams\n        self.layer_index = layer_index\n\n        self.static_beta = nn.Parameter(torch.ones(num_residual_streams))\n\n        init_alpha0 = torch.zeros((num_residual_streams, num_input_views))\n        init_alpha0[layer_index % num_residual_streams, :] = 1.\n\n        self.static_alpha = nn.Parameter(cat([init_alpha0, torch.eye(num_residual_streams)], dim = 1))\n\n        self.dynamic_alpha_fn = nn.Parameter(torch.zeros(dim, num_residual_streams + num_input_views))\n        self.dynamic_alpha_scale = nn.Parameter(torch.ones(()) * 1e-2)\n\n        self.num_input_views = num_input_views\n\n        self.dynamic_beta_fn = nn.Parameter(torch.zeros(dim))\n        self.dynamic_beta_scale = nn.Parameter(torch.ones(()) * 1e-2)\n\n    def prepare(self, residuals):\n\n        residuals = rearrange(residuals, '(b s) n d -> b n s d', s = self.num_residual_streams)\n\n        normed = self.norm(residuals)\n\n        wc_weight = self.act(normed @ self.dynamic_alpha_fn)\n        dynamic_alpha = wc_weight * self.dynamic_alpha_scale\n        alpha = dynamic_alpha + self.static_alpha\n\n        dc_weight = self.act(normed @ self.dynamic_beta_fn)\n        dynamic_beta = dc_weight * self.dynamic_beta_scale\n        beta = dynamic_beta + self.static_beta\n\n        # width connection\n\n        mix_h = einsum('... s t, ... s d -> ... t d', alpha, residuals)\n\n        views = self.num_input_views\n\n        if views == 1:\n            branch_input, residuals = mix_h[..., 0, :], mix_h[..., 1:, :]\n        else:\n            branch_input, residuals = mix_h[..., :views, :], mix_h[..., views:, :]\n            branch_input = rearrange(branch_input, '... v d -> v ... d')\n\n        return branch_input, residuals, dict(beta = beta)\n\n    def forward(self, x, residuals, *, beta):\n        residuals = einsum('b n d, b n s -> b n s d', x, beta) + residuals\n        return rearrange(residuals, 'b n s d -> (b s) n d')", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "HyperConnection", "line": 986}}
{"prompt": "Create a dynamic li me neural network module", "code": "class DynamicLIMe(Module):\n    def __init__(\n        self,\n        dim,\n        num_layers,\n        num_views = 1,\n        norm = True,\n        use_softmax = True\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.multiple_views = num_views > 1\n\n        self.to_weights = Sequential(\n            RMSNorm(dim) if norm else None,\n            nn.Linear(dim, num_views * num_layers),\n            Rearrange('... (views layers) -> views ... layers', views = num_views),\n            nn.Softmax(dim = -1) if use_softmax else nn.ReLU()\n        )\n\n    def forward(\n        self,\n        x,\n        hiddens\n    ):\n\n        if not is_tensor(hiddens):\n            hiddens = stack(hiddens)\n\n        assert hiddens.shape[0] == self.num_layers, f'expected hiddens to have {self.num_layers} layers but received {tuple(hiddens.shape)} instead (first dimension must be layers)'\n\n        weights = self.to_weights(x)\n\n        out = einsum('l b n d, v b n l -> v b n d', hiddens, weights)\n\n        if self.multiple_views:\n            return out\n\n        return rearrange(out, '1 ... -> ...')", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "DynamicLIMe", "line": 1059}}
{"prompt": "Create a shift tokens neural network module", "code": "class ShiftTokens(Module):\n    def __init__(self, shifts, fn):\n        super().__init__()\n        self.fn = fn\n        self.shifts = tuple(shifts)\n\n    def forward(self, x, **kwargs):\n        mask = kwargs.get('mask', None)\n        shifts = self.shifts\n        segments = len(shifts)\n        feats_per_shift = x.shape[-1] // segments\n        splitted = x.split(feats_per_shift, dim = -1)\n        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n        segments_to_shift = [shift(*args, mask = mask) for args in zip(segments_to_shift, shifts)]\n        x = cat((*segments_to_shift, *rest), dim = -1)\n        return self.fn(x, **kwargs)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ShiftTokens", "line": 1112}}
{"prompt": "Create a fold axially neural network module", "code": "class FoldAxially(Module):\n    def __init__(\n        self,\n        axial_dim,\n        fn: Module\n    ):\n        super().__init__()\n        self.fn = fn\n        self.axial_dim = axial_dim # will fold the sequence as rearrange(\"b (n axial_dim) ... -> (b axial_dim) n ...\")\n\n    def forward(\n        self,\n        x,\n        *args,\n        **kwargs\n    ):\n        if self.axial_dim == 1:\n            return self.fn(x, *args, **kwargs)\n\n        seq_len, axial_dim = x.shape[1], self.axial_dim\n\n        next_multiple = math.ceil(seq_len / axial_dim) * axial_dim\n        x = pad_at_dim(x, (0, next_multiple - seq_len), dim = 1)\n\n        x = rearrange(x, 'b (n axial_dim) ... -> (b axial_dim) n ...', axial_dim = axial_dim)\n\n        out = self.fn(x, *args, **kwargs)\n\n        (out, *rest_out), tree_spec = tree_flatten(out)\n\n        out = rearrange(out, '(b axial_dim) n ... -> b (n axial_dim) ...', axial_dim = axial_dim)\n\n        out = out[:, :seq_len]\n        out = tree_unflatten((out, *rest_out), tree_spec)\n\n        return out", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "FoldAxially", "line": 1129}}
{"prompt": "Create a layer scale neural network module", "code": "class LayerScale(Module):\n    def __init__(\n        self,\n        fn: Module,\n        dim,\n        init_value = 0.,\n        unit_offset = False\n    ):\n        super().__init__()\n        self.unit_offset = unit_offset\n\n        self.fn = fn\n        self.gamma = nn.Parameter(torch.zeros(dim))\n        nn.init.constant_(self.gamma, init_value - float(unit_offset))\n\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n\n        gamma = self.gamma + float(self.unit_offset)\n\n        if isinstance(out, Tensor):\n            return out * gamma\n\n        out, *rest = out\n        return out * gamma, *rest", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "LayerScale", "line": 1168}}
{"prompt": "Create a adaptive layer scale neural network module", "code": "class AdaptiveLayerScale(Module):\n    def __init__(\n        self,\n        fn: Module,\n        dim,\n        dim_condition = None,\n        init_bias_value = -2.\n    ):\n        super().__init__()\n        self.fn = fn\n\n        dim_condition = default(dim_condition, dim)\n        self.to_gamma = nn.Linear(dim_condition, dim)\n\n        nn.init.zeros_(self.to_gamma.weight)\n        nn.init.constant_(self.to_gamma.bias, init_bias_value)\n\n    def forward(self, x, *, condition, **kwargs):\n        if condition.ndim == 2:\n            condition = rearrange(condition, 'b d -> b 1 d')\n\n        out = self.fn(x, **kwargs)\n        gamma = self.to_gamma(condition).sigmoid()\n\n        if isinstance(out, Tensor):\n            return out * gamma\n\n        out, *rest = out\n        return out * gamma, *rest", "test_code": "", "difficulty": "medium", "category": "normalization", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AdaptiveLayerScale", "line": 1194}}
{"prompt": "Create a concat combine neural network module", "code": "class ConcatCombine(Module):\n    def __init__(self, dim, prev_layer_ind):\n        super().__init__()\n        self.prev_layer_ind = prev_layer_ind\n        self.combine = LinearNoBias(dim * 2, dim)\n\n    def forward(self, x, prev_layers: list[Tensor]):\n        skip = prev_layers[self.prev_layer_ind]\n        concatted_skip = cat((skip, x), dim = -1)\n        return self.combine(concatted_skip)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ConcatCombine", "line": 1226}}
{"prompt": "Create a glu neural network module", "code": "class GLU(Module):\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        activation: Callable,\n        mult_bias = False\n    ):\n        super().__init__()\n        self.act = activation\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n        self.mult_bias = nn.Parameter(torch.ones(dim_out)) if mult_bias else 1.\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim = -1)\n        return x * self.act(gate) * self.mult_bias", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "GLU", "line": 1239}}
{"prompt": "Create a feed forward neural network module", "code": "class FeedForward(Module):\n    def __init__(\n        self,\n        dim,\n        dim_out = None,\n        mult = 4,\n        glu = False,\n        glu_mult_bias = False,\n        swish = False,\n        relu_squared = False,\n        solu = False,\n        custom_activation = None,\n        post_act_ln = False,\n        dropout = 0.,\n        sublayer_dropout = 0.,\n        no_bias = False,\n        zero_init_output = False,\n    ):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n\n        assert at_most_one_of(relu_squared, solu)\n\n        if exists(custom_activation):\n            activation = deepcopy(custom_activation)\n        elif relu_squared:\n            activation = ReluSquared()\n        elif solu:\n            activation = SoLU(inner_dim)\n        elif swish:\n            activation = nn.SiLU()\n        else:\n            activation = nn.GELU()\n\n        if glu:\n            proj_in = GLU(dim, inner_dim, activation, mult_bias = glu_mult_bias)\n        else:\n            proj_in = nn.Sequential(\n                nn.Linear(dim, inner_dim, bias = not no_bias),\n                activation\n            )\n\n        proj_out = nn.Linear(inner_dim, dim_out, bias = not no_bias)\n\n        self.ff = Sequential(\n            proj_in,\n            LayerNorm(inner_dim) if post_act_ln else None,\n            nn.Dropout(dropout),\n            proj_out,\n            nn.Dropout(sublayer_dropout) if sublayer_dropout > 0. else None\n        )\n\n        # init last linear layer to 0\n\n        if zero_init_output:\n            init_zero_(proj_out)\n\n    def muon_parameters(self):\n        weights = []\n\n        for m in self.modules():\n            if not isinstance(m, nn.Linear):\n                continue\n\n            weights.append(m.weight)\n\n        return weights\n\n    def forward(\n        self,\n        x,\n        deep_embed = None\n    ):\n        out = self.ff(x)\n\n        if exists(deep_embed):\n            out = out * deep_embed\n\n        return out", "test_code": "", "difficulty": "medium", "category": "mlp", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "FeedForward", "line": 1256}}
{"prompt": "Create a attention neural network module", "code": "class Attention(Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = DEFAULT_DIM_HEAD,\n        dim_context = None,\n        heads = 8,\n        causal = False,\n        flash = False,\n        pre_talking_heads = False,\n        post_talking_heads = False,\n        pre_scale_post_talking_heads = False,\n        head_scale = False,\n        sparse_topk = None,\n        sparse_topk_straight_through = False,\n        num_mem_kv = 0,\n        dropout = 0.,\n        sublayer_dropout = 0.,\n        on_attn = False,\n        gate_value_heads = False,\n        swiglu_values = False,\n        gate_values = False,\n        zero_init_output = False,\n        hard = False,\n        max_attend_past = None,\n        qk_norm = False,\n        qk_norm_groups = 1,\n        qk_norm_scale = 10,\n        qk_norm_dim_scale = False,\n        value_rmsnorm = False,      # used in alphagenome and bytedance's GR3 for further stability\n        l2_distance = False,\n        sigmoid = False,\n        gumbel_softmax = False,\n        gumbel_softmax_temp = 1.,\n        gumbel_softmax_hard = True,\n        selective = False,\n        cog_signed = False,\n        custom_attn_fn: Callable | None = None,\n        hybrid_module: Module | None = None,\n        hybrid_mask_kwarg: str | None = None,\n        hybrid_fold_axial_dim: int | None = None,\n        hybrid_learned_mix = False,\n        one_kv_head = False,\n        kv_heads = None,\n        value_dim_head = None,\n        dim_out = None,\n        add_zero_kv = False,         # same as add_zero_attn in pytorch\n        head_learned_sink = False,\n        rotate_num_heads = None,\n        data_dependent_alibi = False,\n        data_dependent_alibi_per_row = False,\n        data_dependent_alibi_per_row_dim_head = 8,\n        data_dependent_alibi_kwargs: dict = dict(),\n        use_cope = False,\n        cope_max_pos = 16,\n        cope_soft_onehot_pos = False,\n        cope_talking_heads = False,\n        softclamp_logits = False,\n        logit_softclamp_value = 50.,\n        learned_value_residual_mix = False,\n        orthog_projected_values = False,  # https://openreview.net/forum?id=Ard2QzPAUK\n        orthog_projected_values_per_head = False,\n        laser = False,                    # https://arxiv.org/abs/2411.03493v1\n        laser_softclamp_value = 15.,\n        qkv_receive_diff_residuals = False,\n        use_latent_q = False,\n        dim_latent_q = None,\n        use_latent_kv = False,\n        dim_latent_kv = None,\n        latent_rope_subheads = None,\n        onnxable = False,\n        attend_sdp_kwargs: dict = dict(\n            enable_flash = True,\n            enable_math = True,\n            enable_mem_efficient = True\n        )\n    ):\n        super().__init__()\n        dim_kv = default(dim_context, dim)\n\n        self.scale = dim_head ** -0.5\n\n        self.heads = heads\n        self.causal = causal\n        self.max_attend_past = max_attend_past\n\n        assert not (exists(kv_heads) and one_kv_head), 'either attn_one_kv_head is set to True (in which case kv_heads is set to 1), or attn_kv_heads is set, but not both'\n\n        value_dim_head = default(value_dim_head, dim_head)\n        kv_heads = default(kv_heads, heads)\n\n        kv_heads = 1 if one_kv_head else kv_heads\n        assert divisible_by(heads, kv_heads)\n\n        self.kv_heads = kv_heads\n        self.groups = heads // kv_heads\n\n        q_dim = dim_head * heads\n        k_dim = dim_head * kv_heads\n        v_dim = value_dim_head * kv_heads\n        out_dim = value_dim_head * heads\n\n        # determine input dimensions to qkv based on whether intermediate latent q and kv are being used\n        # for eventually supporting multi-latent attention (MLA)\n\n        self.to_latent_q = None\n        self.to_latent_kv = None\n        self.to_rotateable_k = None # for their \"decoupled rope\", subheads of keys that comes directly from base sequence (does not go through latents)\n\n        dim_q_input = dim\n        dim_kv_input = dim_kv\n\n        if use_latent_q:\n            assert exists(dim_latent_q)\n            self.to_latent_q = LinearNoBias(dim, dim_latent_q)\n            dim_q_input = dim_latent_q\n\n        if use_latent_kv:\n            assert exists(dim_latent_kv)\n            self.to_latent_kv = LinearNoBias(dim, dim_latent_kv)\n            dim_kv_input = dim_latent_kv\n\n        if exists(latent_rope_subheads):\n            assert not exists(rotate_num_heads), '`rotate_num_heads` cannot be set when multi-latent attention is being used'\n            rotate_num_heads = latent_rope_subheads\n\n            k_dim = dim_head * (kv_heads - latent_rope_subheads)\n\n            self.to_rotateable_k = LinearNoBias(dim, dim_head * latent_rope_subheads)\n            self.split_rotateable_k_heads = Rearrange('b n (h d) -> b h n d', h = latent_rope_subheads)\n\n        self.use_latent_q = use_latent_q\n        self.use_latent_kv = use_latent_kv\n\n        # query key projection\n\n        self.to_q = LinearNoBias(dim_q_input, q_dim)\n        self.to_k = LinearNoBias(dim_kv_input, k_dim)\n        self.to_v = LinearNoBias(dim_kv_input, v_dim)\n\n        # split and merge of attention heads\n\n        self.split_q_heads = Rearrange('b n (h d) -> b h n d', h = heads)\n        self.split_k_heads = Rearrange('b n (h d) -> b h n d', d = dim_head)\n        self.split_v_heads = Rearrange('b n (h d) -> b h n d', d = value_dim_head)\n\n        self.merge_heads = Rearrange('b h n d -> b n (h d)')\n\n        # whether qkv receives different residual stream combinations from hyper connections or lime\n\n        self.qkv_receive_diff_residuals = qkv_receive_diff_residuals\n\n        # enhancing gradients to attention through exponentiated values\n\n        self.laser = laser\n        self.laser_softclamp_value = laser_softclamp_value\n\n        # add GLU gating for aggregated values, from alphafold2\n\n        self.to_v_gate = None\n        if gate_values:\n            self.to_v_gate = nn.Linear(dim, out_dim)\n            self.to_v_gate_activation = F.silu if swiglu_values else F.sigmoid\n            nn.init.constant_(self.to_v_gate.weight, 0)\n            nn.init.constant_(self.to_v_gate.bias, 10)\n\n        # add per head gating of the output values, from 'Attend to nothing' paper\n\n        self.to_v_head_gate = None\n        if gate_value_heads:\n            self.to_v_head_gate = nn.Linear(dim, heads)\n            nn.init.constant_(self.to_v_head_gate.weight, 0)\n            nn.init.constant_(self.to_v_head_gate.bias, 10)\n\n        # cosine sim attention\n\n        self.qk_norm = qk_norm\n        self.qk_norm_groups = qk_norm_groups\n        self.qk_norm_scale = qk_norm_scale\n\n        # whether to use the rmsnorm (equivalent to cosine sim attention when scale is equal to 1) - https://arxiv.org/abs/2302.05442\n\n        self.qk_norm_dim_scale = qk_norm_dim_scale\n\n        self.qk_norm_q_scale = self.qk_norm_k_scale = 1\n        if qk_norm and qk_norm_dim_scale:\n            self.qk_norm_q_scale = nn.Parameter(torch.ones(heads, 1, dim_head))\n            self.qk_norm_k_scale = nn.Parameter(torch.ones(kv_heads, 1, dim_head))\n\n        assert (not qk_norm) or divisible_by(dim_head, qk_norm_groups), 'dimension per attention head must be divisible by the qk norm groups'\n        assert not (qk_norm and (dim_head // qk_norm_groups) <= 2), 'the group dimension may be too small (2 was too small in my tests, but 4 still works, surprisingly)'\n\n        # value rms norm\n\n        self.value_rmsnorm = MultiheadRMSNorm(dim_head, heads = heads) if value_rmsnorm else None\n\n        # contextual positional encoding\n        # https://arxiv.org/html/2405.18719v2\n\n        cope = None\n\n        if use_cope:\n            assert causal, 'CoPE was designed for causal attention'\n            assert not flash, 'CoPE is not flash attention compatible'\n\n            cope = CoPE(\n                dim = dim_head,\n                heads = heads,\n                max_pos = cope_max_pos,\n                talking_heads = cope_talking_heads,\n                soft_onehot = cope_soft_onehot_pos\n            )\n\n        # data dependent alibi\n        # https://openreview.net/forum?id=q2Lnyegkr8\n\n        self.data_dependent_alibi = None\n\n        if data_dependent_alibi:\n\n            dda_klass = DataDependentAlibi if not data_dependent_alibi_per_row else PerRowDataDependentAlibi\n            dda_kwargs = dict(dim = dim, heads = heads, causal = causal)\n\n            if data_dependent_alibi_per_row:\n                dda_kwargs.update(dim_head = data_dependent_alibi_per_row_dim_head)\n\n            self.data_dependent_alibi = dda_klass(**dda_kwargs, **data_dependent_alibi_kwargs)\n\n        # attend class - includes core attention algorithm + talking heads\n\n        self.attend = Attend(\n            heads = heads,\n            causal = causal,\n            pre_talking_heads = pre_talking_heads,\n            post_talking_heads = post_talking_heads,\n            pre_scale_post_talking_heads = pre_scale_post_talking_heads,\n            dropout = dropout,\n            sparse_topk = sparse_topk,\n            sparse_topk_straight_through = sparse_topk_straight_through,\n            hard = hard,\n            qk_norm = qk_norm,\n            scale = qk_norm_scale if qk_norm else self.scale,\n            l2_distance = l2_distance,\n            sigmoid = sigmoid,\n            gumbel_softmax = gumbel_softmax,\n            gumbel_softmax_temp = gumbel_softmax_temp,\n            gumbel_softmax_hard = gumbel_softmax_hard,\n            selective = selective,\n            cog_signed = cog_signed,\n            custom_attn_fn = custom_attn_fn,\n            add_zero_kv = add_zero_kv,\n            head_learned_sink = head_learned_sink,\n            flash = flash,\n            softclamp_logits = softclamp_logits,\n            logit_softclamp_value = logit_softclamp_value,\n            cope = cope,\n            onnxable = onnxable,\n            sdp_kwargs = attend_sdp_kwargs\n        )\n\n        # head scaling\n\n        self.head_scale = head_scale\n        if head_scale:\n            self.head_scale_params = nn.Parameter(torch.ones(1, heads, 1, 1))\n\n        # explicit topk sparse attention\n\n        self.sparse_topk = sparse_topk\n\n        # add memory key / values\n\n        self.num_mem_kv = num_mem_kv\n        if num_mem_kv > 0:\n            self.mem_k = nn.Parameter(torch.randn(kv_heads, num_mem_kv, dim_head))\n            self.mem_v = nn.Parameter(torch.randn(kv_heads, num_mem_kv, dim_head))\n\n        # maybe learned value residual mixer per token\n\n        self.to_value_residual_mix = nn.Sequential(\n            nn.Linear(dim, heads),\n            nn.Sigmoid(),\n            Rearrange('b n h -> b h n 1')\n         ) if learned_value_residual_mix else always(0.5)\n\n        # attention on attention\n\n        self.attn_on_attn = on_attn\n\n        # return orthogonal projected weighted values on original values\n        # \"belief attention\" - iclr 2026\n\n        self.orthog_projected_values = orthog_projected_values\n        self.orthog_projected_values_per_head = orthog_projected_values_per_head\n\n        out_dim *= max(1, int(orthog_projected_values) + int(orthog_projected_values_per_head))\n\n        # hybrid module, in same vein as hymba https://www.arxiv.org/abs/2411.13676\n\n        hybrid_mix = None\n        hybrid_norms = None\n        hybrid_module = maybe(deepcopy)(hybrid_module)\n\n        if exists(hybrid_module) and exists(hybrid_fold_axial_dim):\n            hybrid_module = FoldAxially(axial_dim = hybrid_fold_axial_dim, fn = hybrid_module)\n            hybrid_mix = LinearNoBias(dim, heads) if hybrid_learned_mix else None\n\n            hybrid_norms = ModuleList([\n                MultiheadRMSNorm(dim_head, heads = heads),\n                MultiheadRMSNorm(dim_head, heads = heads)\n            ])\n\n        self.hybrid_module = hybrid_module\n        self.hybrid_norms = hybrid_norms\n        self.hybrid_mix = hybrid_mix\n        self.hybrid_mask_kwarg = hybrid_mask_kwarg # for bidirectional, can forward `mask` into the hybrid module and let it handle variable lengths\n\n        # output dimension by default same as input, but can be overridden\n\n        dim_out = default(dim_out, dim)\n        self.to_out = nn.Sequential(LinearNoBias(out_dim, dim_out * 2), nn.GLU()) if on_attn else LinearNoBias(out_dim, dim_out)\n\n        # sublayer dropout\n\n        self.sublayer_dropout = nn.Dropout(sublayer_dropout) if sublayer_dropout > 0. else None\n\n        # the number of attention heads to rotate, for decoupled rope in multi-latent attention\n\n        rotate_num_heads = default(rotate_num_heads, heads)\n\n        assert 0 < rotate_num_heads <= heads\n        is_partial_rotate_heads = rotate_num_heads < heads\n        assert not (is_partial_rotate_heads and kv_heads < heads), 'grouped query attention not compatible with partial rotate heads (decoupled rope for multi-latent attention), yet'\n\n        self.rotate_num_heads = rotate_num_heads\n\n        # whether parent can kv cache\n\n        self.can_cache_kv = not selective\n\n        # init output projection 0\n\n        if zero_init_output:\n            init_zero_(self.to_out)\n\n    @torch.no_grad()\n    def qk_clip_(\n        self,\n        pre_softmax_attn: Tensor | Intermediates,\n        tau = 100 # this hyperparameter controls how large the attention logits can be\n    ):\n        \"\"\" proposed by the Moonshot AI team as a solution for Muon training instability \"\"\"\n\n        if not is_tensor(pre_softmax_attn):\n            pre_softmax_attn = pre_softmax_attn.pre_softmax_attn\n\n        attn_logit_maxes = reduce(pre_softmax_attn, 'b h i j -> h', 'max')\n\n        qk_weight_scale = (tau / attn_logit_maxes).clamp(max = 1.).sqrt()\n\n        q_weight = self.to_q.weight\n        k_weight = self.to_k.weight\n\n        qk_dim, heads = q_weight.shape[0], qk_weight_scale.numel()\n\n        qk_weight_scale = repeat(qk_weight_scale, 'h -> (h expand)', expand = qk_dim // heads)\n\n        q_weight.mul_(qk_weight_scale)\n        k_weight.mul_(qk_weight_scale)\n\n    def muon_parameters(self):\n        return chain(self.to_v.parameters(), self.to_out.parameters())\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        context_mask = None,\n        attn_mask = None,\n        rel_pos = None,\n        attn_bias = None,\n        rotary_pos_emb = None,\n        context_rotary_pos_emb = None,\n        pos = None, # for custom alibi positions\n        prev_attn = None,\n        mem = None,\n        mem_mask = None,\n        return_intermediates = False,\n        cache: Intermediates | None = None,\n        value_residual = None,\n        additional_key_values: tuple[Tensor, Tensor] | None = None,\n        additional_key_value_mask = None,\n        kv_input_residual = None,\n    ):\n        b, n, h, kv_h, head_scale, num_mem_kv, device, has_context, qkv_receive_diff_residuals, is_multi_latent_attn = x.shape[0], x.shape[1], self.heads, self.kv_heads, self.head_scale, self.num_mem_kv, x.device, exists(context), self.qkv_receive_diff_residuals, self.use_latent_kv\n\n        # an interesting possibility with hyper connections\n        # having queries, keys, values be routed from different layers\n\n        assert not (qkv_receive_diff_residuals and has_context), 'qkv receiving different sequences can only be used for self attention'\n\n        if qkv_receive_diff_residuals:\n            assert x.ndim == 4 and x.shape[0] == 3\n\n            q_input, k_input, v_input = x\n        else:\n            kv_input = default(context, x)\n            q_input, k_input, v_input = x, kv_input, kv_input\n\n        # done for free transformer\n\n        if exists(kv_input_residual):\n            k_input = k_input + kv_input_residual\n            v_input = v_input + kv_input_residual\n\n        if exists(mem):\n            k_input, mem_packed_shape = pack([mem, k_input], 'b * d')\n            v_input, _ = pack([mem, v_input], 'b * d')\n\n        # multi-latent attention logic\n        # https://arxiv.org/abs/2405.04434 - Deepseek-AI team\n\n        k_sub_heads = None # the rotateable subheads of keys derived from base sequence\n\n        if self.use_latent_q:\n            q_input = self.to_latent_q(q_input)\n\n        if is_multi_latent_attn:\n            assert not qkv_receive_diff_residuals\n            needs_k_sub_heads = exists(self.to_rotateable_k)\n\n            latent_kv_input = self.to_latent_kv(k_input)\n\n            if needs_k_sub_heads:\n                rotateable_k = self.to_rotateable_k(k_input)\n                k_sub_heads = self.split_rotateable_k_heads(rotateable_k)\n\n            if exists(cache):\n                cached_latent_kv, maybe_cached_k_sub_heads = cache.cached_kv\n                latent_kv_input = cat((cached_latent_kv, latent_kv_input), dim = -2)\n\n                if exists(maybe_cached_k_sub_heads):\n                    k_sub_heads = cat((maybe_cached_k_sub_heads, k_sub_heads), dim = -2)\n\n            if return_intermediates:\n                cached_kv = (latent_kv_input, k_sub_heads)\n\n            k_input = v_input = latent_kv_input\n\n        # query, key, value projection\n\n        q = self.to_q(q_input)\n        k = self.to_k(k_input)\n        v = self.to_v(v_input)\n\n        q = self.split_q_heads(q)\n        k = self.split_k_heads(k)\n        v = self.split_v_heads(v)\n\n        # take care of decoupled rope from multi-latent attention\n\n        if exists(k_sub_heads):\n            k = cat((k, k_sub_heads), dim = 1)\n\n        # if previous values passed in for residual, either invoke resformer\n\n        orig_values = v\n\n        # https://arxiv.org/abs/2410.17897v1\n\n        if exists(value_residual):\n            value_residual_mix = self.to_value_residual_mix(q_input)\n            v = value_residual.lerp(v, value_residual_mix)\n\n        # qk normalization\n\n        if self.qk_norm:\n            qk_l2norm = partial(l2norm, groups = self.qk_norm_groups)\n            q, k = map(qk_l2norm, (q, k))\n            scale = self.qk_norm_scale\n\n            q = q * self.qk_norm_q_scale\n            k = k * self.qk_norm_k_scale\n\n        # maybe value rmsnorm\n\n        v = maybe(self.value_rmsnorm)(v)\n\n        # take care of caching\n\n        if not is_multi_latent_attn:\n            if exists(cache):\n                ck, cv = cache.cached_kv\n\n                if exists(mem):\n                    mk, k = unpack(k, mem_packed_shape, 'b h * d')\n                    mv, v = unpack(v, mem_packed_shape, 'b h * d')\n\n                k = cat((ck, k), dim = -2)\n                v = cat((cv, v), dim = -2)\n\n                if exists(mem):\n                    k = cat((mk, k), dim = -2)\n                    v = cat((mv, v), dim = -2)\n\n            if return_intermediates:\n                mem_len = mem.shape[-2] if exists(mem) else 0\n                cached_kv = (k[..., mem_len:, :], v[..., mem_len:, :])\n\n        if exists(rotary_pos_emb):\n            rotate_num_heads = self.rotate_num_heads\n            partial_rotate_heads = rotate_num_heads < h\n\n            freqs, xpos_scale = rotary_pos_emb\n            q_xpos_scale, k_xpos_scale = (xpos_scale, xpos_scale ** -1.) if exists(xpos_scale) else (1., 1.)\n\n            if partial_rotate_heads:\n                q_rest, q = q[:, :-rotate_num_heads], q[:, -rotate_num_heads:]\n                k_rest, k = k[:, :-rotate_num_heads], k[:, -rotate_num_heads:]\n\n            q = apply_rotary_pos_emb(q, freqs, q_xpos_scale)\n\n            if has_context:\n                # override with `context_rotary_pos_emb` if provided\n\n                freqs, xpos_scale = context_rotary_pos_emb\n                _, k_xpos_scale = (xpos_scale, xpos_scale ** -1.) if exists(xpos_scale) else (1., 1.)\n\n            k = apply_rotary_pos_emb(k, freqs, k_xpos_scale)\n\n            if partial_rotate_heads:\n                q = cat((q_rest, q), dim = 1)\n                k = cat((k_rest, k), dim = 1)\n\n        input_mask = context_mask\n\n        if not exists(input_mask) and not has_context:\n            input_mask = mask\n\n            if (exists(input_mask) or exists(mem_mask)) and exists(mem):\n                seq_len, mem_len = n, mem.shape[-2]\n\n                if not exists(mem_mask):\n                    input_mask = pad_at_dim(input_mask, (mem_len, 0), dim = -1, value = True)\n                elif not exists(input_mask):\n                    input_mask = pad_at_dim(mem_mask, (0, seq_len), dim = -1, value = True)\n                else:\n                    input_mask = cat((mem_mask, input_mask), dim = -1)\n\n        # i, j determined for relative positional bias, excluding memory key / values\n\n        i, j = tuple(t.shape[-2] for t in (q, k))\n\n        # maybe append memory key / values\n\n        if num_mem_kv > 0:\n            mem_k, mem_v = tuple(repeat(t, 'h n d -> b h n d', b = b) for t in (self.mem_k, self.mem_v))\n\n            if self.qk_norm:\n                mem_k = l2norm(mem_k)\n                mem_k = mem_k * self.qk_norm_k_scale\n\n            k = cat((mem_k, k), dim = -2)\n            v = cat((mem_v, v), dim = -2)\n\n            if exists(input_mask):\n                input_mask = pad_at_dim(input_mask, (self.num_mem_kv, 0), dim = -1, value = True)\n\n        # maybe append additional key / values\n\n        if exists(additional_key_values):\n            seq_len = k.shape[-2]\n\n            added_k, added_v = additional_key_values\n            added_kv_heads, added_kv_len = added_k.shape[1], added_k.shape[-2]\n\n            # take care of expanding to query heads if mismatch between key / value heads with the ones coming from vlm\n\n            if added_kv_heads != kv_h:\n                assert divisible_by(h, added_kv_heads)\n                k, v, added_k, added_v = tuple(repeat(t, 'b h ... -> b (r h) ...', r = h // t.shape[1]) for t in (k, v, added_k, added_v))\n\n            k = cat((added_k, k), dim = -2)\n            v = cat((added_v, v), dim = -2)\n\n            if (exists(input_mask) or exists(additional_key_value_mask)):\n\n                if not exists(additional_key_value_mask):\n                    input_mask = pad_at_dim(input_mask, (added_kv_len, 0), dim = -1, value = True)\n                elif not exists(input_mask):\n                    input_mask = pad_at_dim(additional_key_value_mask, (0, seq_len), dim = -1, value = True)\n                else:\n                    input_mask = cat((additional_key_value_mask, input_mask), dim = -1)\n\n        # determine masking\n\n        mask_value = max_neg_value(q)\n        masks = []\n        final_attn_mask = None\n\n        if exists(input_mask):\n            input_mask = rearrange(input_mask, 'b j -> b 1 1 j')\n            masks.append(~input_mask)\n\n        if exists(attn_mask):\n            assert 2 <= attn_mask.ndim <= 4, 'attention mask must have greater than 2 dimensions but less than or equal to 4'\n            if attn_mask.ndim == 2:\n                attn_mask = rearrange(attn_mask, 'i j -> 1 1 i j')\n            elif attn_mask.ndim == 3:\n                attn_mask = rearrange(attn_mask, 'h i j -> 1 h i j')\n            masks.append(~attn_mask)\n\n        if exists(self.max_attend_past):\n            range_q = arange(j - i, j, device = device)\n            range_k = arange(j, device = device)\n            dist = einx.subtract('i, j -> 1 1 i j', range_q, range_k)\n            max_attend_past_mask = dist > self.max_attend_past\n            max_attend_past_mask = pad_at_dim(max_attend_past_mask, (num_mem_kv, 0), value = False, dim = -1) # handle memory key / values\n            masks.append(max_attend_past_mask)\n\n        if len(masks) > 0:\n            final_attn_mask = ~or_reduce(masks)\n\n        # prepare relative positional bias, if needed\n\n        if exists(rel_pos):\n            assert not exists(attn_bias)\n\n            if exists(pos):\n                assert isinstance(rel_pos, AlibiPositionalBias), 'only alibi allowed for custom positions at the moment'\n                # allow for custom positions to be passed in\n                attn_bias = rel_pos.forward_custom_pos(pos)\n            else:\n                attn_bias = rel_pos(i, j)\n\n            attn_bias = pad_at_dim(attn_bias, (num_mem_kv, 0)) # handle memory key / values\n\n        # prepare data dependent alibi from forgetting transformers paper, if needed\n\n        if exists(self.data_dependent_alibi):\n            attn_bias = self.data_dependent_alibi(x)\n\n            attn_bias = pad_at_dim(attn_bias, (num_mem_kv, 0))\n\n        if self.laser:\n            v = softclamp(v, self.laser_softclamp_value)\n            v = v.exp()\n\n        # attention is all we need\n\n        out, intermediates = self.attend(\n            q, k, v,\n            mask = final_attn_mask,\n            attn_bias = attn_bias,\n            prev_attn = prev_attn\n        )\n\n        # laser\n\n        if self.laser:\n            out = log(out)\n\n        # store the values for resformer\n\n        intermediates.values = orig_values\n\n        # normformer scaling of heads\n\n        if head_scale:\n            out = out * self.head_scale_params\n\n        # per head gating, from https://arxiv.org/abs/2306.12929\n\n        if exists(self.to_v_head_gate):\n            head_gate = self.to_v_head_gate(x)\n            out = einx.multiply('b n h, b h n d ->b h n d', head_gate.sigmoid(), out)\n\n        # if exists hybrid module, must do a normalization\n\n         # hybrid module\n\n        if exists(self.hybrid_module):\n\n            # hybrid input\n\n            hybrid_forward_kwargs = dict()\n\n            if not self.causal and exists(self.hybrid_mask_kwarg):\n                hybrid_forward_kwargs = {self.hybrid_mask_kwarg: mask}\n\n            # handle maybe hybrid cache\n\n            hybrid_forward_args = ()\n\n            if exists(cache) and exists(cache.hybrid_hidden):\n                hybrid_hiddens = cache.hybrid_hidden\n                hybrid_forward_args = (hybrid_hiddens,)\n\n            # hybrid forward\n\n            hybrid_outputs = self.hybrid_module(x, *hybrid_forward_args, **hybrid_forward_kwargs)\n\n            # handle hybrid out\n\n            (hybrid_out, *rest_hybrid_outs), _ = tree_flatten(hybrid_outputs)\n\n            # handle variable hybrid output and multi rmsnorm before summing to main attention output (also normed)\n\n            if hybrid_out.ndim == 3:\n                hybrid_out = rearrange(hybrid_out, 'b n (h d) -> b h n d', h = h)\n\n            if len(rest_hybrid_outs) > 0:\n                hybrid_hidden = first(rest_hybrid_outs)\n                intermediates.hybrid_hidden = hybrid_hidden\n\n            out_norm, hybrid_out_norm = self.hybrid_norms\n\n            out = out_norm(out)\n            hybrid_out = hybrid_out_norm(hybrid_out)\n\n            if exists(self.hybrid_mix):\n                mix = self.hybrid_mix(x)\n                mix = rearrange(mix, 'b n h -> b h n 1')\n                out = out.lerp(hybrid_out, mix.sigmoid())\n            else:\n                out = 0.5 * (out + hybrid_out)\n\n        # merge heads\n\n        out = self.merge_heads(out)\n\n        # alphafold2 styled gating of the values\n\n        if exists(self.to_v_gate):\n            gates = self.to_v_gate(x)\n            out = out * self.to_v_gate_activation(gates)\n\n        # maybe orthogonal projected weighted values - \"belief\" attention\n\n        if self.orthog_projected_values or self.orthog_projected_values_per_head:\n            orthog_projected = []\n            v_for_proj = repeat(orig_values, 'b h n d -> b n (g h d)', g = self.groups)\n\n            if self.orthog_projected_values:\n                projected = orthog_project(out, v_for_proj)\n                orthog_projected.append(projected)\n\n            if self.orthog_projected_values_per_head:\n                v_for_proj = rearrange(v_for_proj, 'b n (h d) -> b n h d', h = h)\n                out = rearrange(out, 'b n (h d) -> b n h d', h = h)\n                projected = orthog_project(out, v_for_proj)\n                projected = rearrange(projected, 'b n h d -> b n (h d)')\n                orthog_projected.append(projected)\n\n            out = cat(orthog_projected, dim = -1)\n\n        # combine the heads\n\n        out = self.to_out(out)\n\n        # maybe sublayer dropout\n\n        out = maybe(self.sublayer_dropout)(out)\n\n        if exists(mask) and not exists(cache):\n            out = einx.where('b n, b n d, -> b n d', mask, out, 0.)\n\n        if not return_intermediates:\n            return out\n\n        intermediates.cached_kv = cached_kv\n\n        return out, intermediates", "test_code": "", "difficulty": "medium", "category": "attention", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "Attention", "line": 1339}}
{"prompt": "Create a attention layers neural network module", "code": "class AttentionLayers(Module):\n    def __init__(\n        self,\n        dim,\n        depth = None,\n        heads = 8,\n        causal = False,\n        cross_attend = False,\n        only_cross = False,\n        use_scalenorm = False,\n        use_rmsnorm = False,\n        use_dynamic_tanh = False,\n        dynamic_tanh_init_alpha = 1.,\n        use_simple_rmsnorm = False,\n        use_adaptive_layernorm = False,\n        use_adaptive_rmsnorm = False,\n        use_adaptive_layerscale = False, # paired with use_adaptive_layernorm for ada-ln-zero from DiT paper\n        norm_add_unit_offset = True,\n        dim_condition = None,\n        adaptive_condition_mlp = False,\n        adaptive_condition_mlp_expansion = 4,\n        alibi_pos_bias = False,\n        alibi_num_heads = None,\n        rel_pos_bias = False,\n        rel_pos_num_buckets = 32,\n        rel_pos_max_distance = 128,\n        dynamic_pos_bias = False,\n        dynamic_pos_bias_log_distance = False,\n        dynamic_pos_bias_mlp_depth = 2,\n        dynamic_pos_bias_norm = False,\n        rotary_pos_emb = False,\n        rotary_emb_dim = None,\n        rotary_xpos = False,\n        rotary_interpolation_factor = 1.,\n        rotary_xpos_scale_base = 512,\n        rotary_base_rescale_factor = 1.,\n        rotate_num_heads = None,\n        weight_tie_layers = False,\n        custom_layers: tuple[str, ...] | None = None,\n        layers_execute_order: tuple[int, ...] | None = None,\n        sandwich_coef = None,\n        par_ratio = None,\n        residual_attn = False,\n        cross_residual_attn = False,\n        macaron = False,\n        pre_norm = True,\n        pre_norm_has_final_norm = True,\n        gate_residual = False,\n        scale_residual = False,\n        scale_residual_constant = 1.,\n        shift_tokens = 0,\n        sandwich_norm = False,\n        softclamp_output = False,\n        softclamp_output_value = 30.,\n        zero_init_branch_output = False,\n        layer_dropout = 0.,\n        cross_attn_tokens_dropout = 0.,\n        disable_abs_pos_emb = None,\n        use_layerscale = False,\n        layerscale_init_value = 0.,\n        unet_skips = False,\n        integrate_layers = False,\n        layer_integrate_use_softmax = True,\n        num_residual_streams = 1,\n        qkv_receive_diff_residuals = False,\n        reinject_input = False,              # seen first in DEQ paper https://arxiv.org/abs/1909.01377, but later used in a number of papers trying to achieve depthwise generalization https://arxiv.org/abs/2410.03020v1\n        learned_reinject_input_gate = False,\n        add_value_residual = False,          # resformer from Zhou et al - https://arxiv.org/abs/2410.17897v1 - further corroboration by https://arxiv.org/abs/2412.15113 (faster emergence of ICL) - looks like this setting may becoming a necessity for every transformer soon\n        learned_value_residual_mix = True,   # seeing big improvements when the value residual mix value is learned per token - credit goes to @faresobeid for taking the first step with learned scalar mix, then @Blinkdl for taking it a step further with data dependent. here we will use per token learned\n        rel_pos_kwargs: dict = dict(),\n        residual_fn_kwargs: dict = dict(),\n        verbose = True,\n        **kwargs\n    ):\n        super().__init__()\n        rotary_pos_emb = rotary_pos_emb or rotary_xpos\n\n        ff_kwargs, kwargs = groupby_prefix_and_trim('ff_', kwargs)\n        attn_kwargs, kwargs = groupby_prefix_and_trim('attn_', kwargs)\n        cross_attn_kwargs, kwargs = groupby_prefix_and_trim('cross_attn_', kwargs)\n\n        dim_head = attn_kwargs.get('dim_head', DEFAULT_DIM_HEAD)\n        data_dependent_alibi = attn_kwargs.get('data_dependent_alibi', False)\n\n        assert len(kwargs) == 0, f'unrecognized kwargs passed in {kwargs.keys()}'\n\n        self.dim = dim\n        self.causal = causal\n        self.layers = ModuleList([])\n\n        self.attn_heads = heads\n        self.attn_dim_head = dim_head\n\n        # routing related\n        # 1. greater than one residual stream, proposed in Hyper-Connections paper https://arxiv.org/abs/2409.19606\n        # 2. integrating more than one past layer, from LIMe paper https://arxiv.org/abs/2502.09245\n\n        qkv_receive_diff_residuals |= integrate_layers # qkv always receives different views if integrating layers\n\n        # hyper connections\n\n        assert num_residual_streams > 0\n        has_hyper_connections = num_residual_streams > 1\n\n        self.num_residual_streams = num_residual_streams\n        self.stream_emb = nn.Parameter(torch.zeros(num_residual_streams, dim)) if num_residual_streams > 1 else None\n\n        assert not (has_hyper_connections and gate_residual)\n\n        hyper_conn_produce_diff_views = qkv_receive_diff_residuals and not integrate_layers\n\n        # LIMe\n\n        hiddens_counter = 0\n        self.layer_integrators = ModuleList([])\n\n        assert not (qkv_receive_diff_residuals and not (hyper_conn_produce_diff_views or integrate_layers))\n\n        # positions related\n\n        self.disable_abs_pos_emb = default(disable_abs_pos_emb, (rel_pos_bias or rotary_pos_emb))\n\n        rotary_emb_dim = default(rotary_emb_dim, dim_head // 2)\n\n        assert rotary_emb_dim <= dim_head, f'rotary emb dim {rotary_emb_dim} must be less than or equal to attention head dimension {dim_head}'\n\n        if verbose and rotary_emb_dim < 32:\n            logger.warning('when training language model, rotary embedding dimension should be at least 32')\n\n        assert not (rotary_xpos and not causal), 'rotary xpos is not compatible with bidirectional attention'\n        self.rotary_pos_emb = RotaryEmbedding(rotary_emb_dim, use_xpos = rotary_xpos, scale_base = rotary_xpos_scale_base, interpolation_factor = rotary_interpolation_factor, base_rescale_factor = rotary_base_rescale_factor) if rotary_pos_emb else None\n\n        assert at_most_one_of(alibi_pos_bias, rel_pos_bias, data_dependent_alibi), 'you can only choose one of Alibi positional bias, data dependent Alibi (forgetting transformers), dynamic tanh, or T5 relative positional bias'\n        assert rel_pos_num_buckets <= rel_pos_max_distance, 'number of relative position buckets must be less than the relative position max distance'\n\n        # relative positional bias\n\n        flash_attn = attn_kwargs.get('flash', False)\n        assert at_most_one_of(rel_pos_bias, dynamic_pos_bias, alibi_pos_bias), 'you can only choose up to one of t5, alibi, or dynamic positional bias'\n\n        self.rel_pos = None\n\n        if rel_pos_bias:\n            assert not flash_attn, 'flash attention not compatible with t5 relative positional bias'\n            self.rel_pos = RelativePositionBias(scale = dim_head ** 0.5, causal = causal, heads = heads, num_buckets = rel_pos_num_buckets, max_distance = rel_pos_max_distance, **rel_pos_kwargs)\n        elif dynamic_pos_bias:\n            assert not flash_attn, 'flash attention not compatible with dynamic positional bias'\n            self.rel_pos = DynamicPositionBias(dim = dim // 4, heads = heads, log_distance = dynamic_pos_bias_log_distance, depth = dynamic_pos_bias_mlp_depth, norm = dynamic_pos_bias_norm, **rel_pos_kwargs)\n        elif alibi_pos_bias:\n            alibi_num_heads = default(alibi_num_heads, heads)\n            assert alibi_num_heads <= heads, 'number of ALiBi heads must be less than the total number of heads'\n            self.rel_pos = AlibiPositionalBias(heads = alibi_num_heads, total_heads = heads, **rel_pos_kwargs)\n\n        assert not (not pre_norm and sandwich_norm), 'sandwich norm cannot be used when not using prenorm'\n\n        self.pre_norm = pre_norm\n        self.sandwich_norm = sandwich_norm\n\n        self.residual_attn = residual_attn\n        self.cross_residual_attn = cross_residual_attn\n        assert not (flash_attn and (residual_attn or cross_residual_attn)), 'flash attention is not compatible with residual attention'\n\n        self.cross_attend = cross_attend\n\n        # determine norm\n\n        assert at_most_one_of(use_scalenorm, use_rmsnorm, use_dynamic_tanh, use_simple_rmsnorm, use_adaptive_layernorm, use_adaptive_rmsnorm), 'you can only use either scalenorm, rmsnorm, adaptive layernorm, adaptive rmsnorm, or simple rmsnorm'\n\n        norm_need_condition = False\n        dim_condition = default(dim_condition, dim)\n        dim_condition_mult = 1\n\n        if adaptive_condition_mlp:\n            dim_condition_mult = adaptive_condition_mlp_expansion\n\n        if use_scalenorm:\n            norm_class = ScaleNorm\n        elif use_rmsnorm:\n            norm_class = RMSNorm\n        elif use_simple_rmsnorm:\n            norm_class = SimpleRMSNorm\n        elif use_dynamic_tanh:\n            assert pre_norm, 'dynamic tanh norm only tested for pre-norm'\n            norm_class = partial(DynamicTanh, init_alpha = dynamic_tanh_init_alpha)\n        elif use_adaptive_layernorm:\n            norm_need_condition = True\n            norm_class = partial(AdaptiveLayerNorm, dim_condition = dim_condition * dim_condition_mult)\n        elif use_adaptive_rmsnorm:\n            norm_need_condition = True\n            norm_class = partial(AdaptiveRMSNorm, dim_condition = dim_condition * dim_condition_mult)\n        else:\n            norm_class = LayerNorm\n\n        norm_fn = partial(norm_class, dim)\n\n        if not norm_need_condition and norm_add_unit_offset:\n            # researcher Ohad Rubin shares in a blog post by adding an offset to gammas, they can be subjected to weight decay safely\n            norm_fn = partial(norm_fn, unit_offset = True)\n\n        self.norm_need_condition = norm_need_condition\n        self.dim_condition = dim_condition\n\n        # determine default block layer type order\n\n        if cross_attend and not only_cross:\n            default_block = ('a', 'c', 'f')\n        elif cross_attend and only_cross:\n            default_block = ('c', 'f')\n        else:\n            default_block = ('a', 'f')\n\n        if macaron:\n            default_block = ('f',) + default_block\n\n        # determine post branch wrapper\n\n        assert at_most_one_of(use_layerscale, use_adaptive_layerscale)\n\n        post_branch_fn = None\n        post_branch_fn_needs_condition = False\n\n        if use_layerscale:\n            post_branch_fn = partial(LayerScale, dim = dim, init_value = layerscale_init_value)\n        elif use_adaptive_layerscale:\n            post_branch_fn = partial(AdaptiveLayerScale, dim = dim, dim_condition = dim_condition * dim_condition_mult)\n            post_branch_fn_needs_condition = True\n\n        self.post_branch_fn_needs_condition = post_branch_fn_needs_condition\n\n        if exists(post_branch_fn) and not post_branch_fn_needs_condition and norm_add_unit_offset:\n            post_branch_fn = partial(post_branch_fn, unit_offset = True)\n\n        # setup mlp for conditioning\n\n        self.need_condition = norm_need_condition or post_branch_fn_needs_condition\n\n        self.adaptive_mlp = nn.Identity()\n\n        if self.need_condition and adaptive_condition_mlp:\n            self.adaptive_mlp = nn.Sequential(\n                LinearNoBias(dim_condition, dim_condition * dim_condition_mult),\n                nn.SiLU()\n            )\n\n        # zero init\n\n        if zero_init_branch_output:\n            attn_kwargs = {**attn_kwargs, 'zero_init_output':  True}\n            ff_kwargs = {**ff_kwargs, 'zero_init_output':  True}\n\n        # setup weight tying, which is a special case of `layer_execute_order`\n\n        assert not (exists(layers_execute_order) and exists(custom_layers) and exists(depth)), 'depth should not be passed in if using custom layers and custom layer execution order'\n\n        assert not (weight_tie_layers and any([*map(exists, (custom_layers, par_ratio, sandwich_coef))]))\n\n        if weight_tie_layers:\n            assert exists(depth), 'depth must be passed in with `weight_tie_layers` = True'\n            assert not exists(layers_execute_order)\n            layers_execute_order = tuple(range(len(default_block))) * depth\n            depth = 1\n\n        # calculate layer block order\n\n        len_default_block = 1\n\n        if exists(custom_layers):\n            layer_types = custom_layers\n        elif exists(par_ratio):\n            par_depth = depth * len(default_block)\n            assert 1 < par_ratio <= par_depth, 'par ratio out of range'\n            default_block = tuple(filter(not_equals('f'), default_block))\n            par_attn  = par_depth // par_ratio\n            depth_cut = par_depth * 2 // 3  # 2 / 3 attention layer cutoff suggested by PAR paper\n            par_width = (depth_cut + depth_cut // par_attn) // par_attn\n            assert len(default_block) <= par_width, 'default block is too large for par_ratio'\n            par_block = default_block + ('f',) * (par_width - len(default_block))\n            par_head = par_block * par_attn\n            layer_types = par_head + ('f',) * (par_depth - len(par_head))\n        elif exists(sandwich_coef):\n            assert sandwich_coef > 0 and sandwich_coef <= depth, 'sandwich coefficient should be less than the depth'\n            layer_types = ('a',) * sandwich_coef + default_block * (depth - sandwich_coef) + ('f',) * sandwich_coef\n        else:\n            assert exists(depth), '`depth` must be passed in for `Decoder` or `Encoder`'\n            layer_types = default_block * depth\n            len_default_block = len(default_block)\n\n        self.layer_types = layer_types\n        self.layers_execute_order = default(layers_execute_order, tuple(range(len(layer_types))))\n\n        assert all([i < len(self.layer_types) for i in self.layers_execute_order])\n\n        self.num_attn_layers = len(list(filter(equals('a'), layer_types)))\n\n        # set the depth\n\n        depth = default(depth, len(self.layers_execute_order))\n        self.depth = depth\n\n        # stochastic depth\n\n        self.layer_dropouts = cast_tuple(layer_dropout, len(layer_types))\n\n        # structured dropout for cross attending\n\n        self.cross_attn_tokens_dropout = cross_attn_tokens_dropout\n\n        # calculate token shifting\n\n        shift_tokens = cast_tuple(shift_tokens, len(layer_types))\n\n        # optional soft clamping just before the final norm\n        # used in gemma 2\n\n        self.softclamp_output = softclamp_output\n        self.softclamp_output_value = softclamp_output_value\n\n        # whether it has post norm\n\n        self.final_norm = norm_fn() if pre_norm and pre_norm_has_final_norm else nn.Identity()\n\n        # whether unet or not\n\n        self.unet_skips = unet_skips\n        num_skips = self.depth // len_default_block\n\n        assert not (unet_skips and num_skips == 0), 'must have depth of at least 2 for unet skip connections'\n\n        skip_indices = [i * len_default_block for i in range(num_skips)]\n\n        self.skip_combines = ModuleList([])\n\n        # whether there is reinjection of input at every layer\n\n        self.reinject_input = reinject_input\n        self.reinject_input_proj = nn.Linear(dim, dim, bias = False) if reinject_input else None\n        self.learned_reinject_input_gate = nn.Linear(dim, 1, bias = False) if learned_reinject_input_gate else None\n\n        # add the value from the first self attention block to all latter projected self attention values as a residual\n\n        self.add_value_residual = add_value_residual\n\n        is_first_self_attn = True\n        is_first_cross_attn = True\n        learned_value_residual_mix &= add_value_residual\n\n        # iterate and construct layers\n\n        for ind, (layer_type, layer_shift_tokens) in enumerate(zip(self.layer_types, shift_tokens)):\n\n            # `ind` is the index of each module - attention, feedforward, cross attention\n            # but `block_ind` refers to the typical enumeration of a transformer block (attn + ff + [optional] cross attn)\n\n            block_begin = divisible_by(ind, len_default_block)\n            block_ind = ind // len_default_block\n\n            is_last_layer = ind == (len(self.layer_types) - 1)\n\n            # attention, cross attention, feedforward\n\n            layer_qkv_receives_diff_view = layer_type == 'a' and qkv_receive_diff_residuals and not (is_first_self_attn and integrate_layers)\n\n            if layer_type == 'a':\n                self_attn_learned_value_residual = learned_value_residual_mix and not is_first_self_attn\n\n                layer = Attention(dim, heads = heads, causal = causal, qkv_receive_diff_residuals = layer_qkv_receives_diff_view, learned_value_residual_mix = self_attn_learned_value_residual, rotate_num_heads = rotate_num_heads, **attn_kwargs)\n                is_first_self_attn = False\n\n            elif layer_type == 'c':\n                layer = Attention(dim, heads = heads, **{**attn_kwargs, **cross_attn_kwargs})\n                is_first_cross_attn = False\n\n            elif layer_type == 'f':\n                layer = FeedForward(dim, **ff_kwargs)\n                layer = layer if not macaron else Scale(0.5, layer)\n\n            else:\n                raise Exception(f'invalid layer type {layer_type}')\n\n            if layer_shift_tokens > 0:\n                shift_range_upper = layer_shift_tokens + 1\n                shift_range_lower = -layer_shift_tokens if not causal else 0\n                layer = ShiftTokens(range(shift_range_lower, shift_range_upper), layer)\n\n            if exists(post_branch_fn):\n                layer = post_branch_fn(layer)\n\n            layer_integrate = None\n\n            if integrate_layers:\n                num_layer_hiddens = ind + 1\n                layer_integrate_num_view = 3 if layer_qkv_receives_diff_view else 1\n\n                layer_integrate = DynamicLIMe(dim, num_layer_hiddens, num_views = layer_integrate_num_view, use_softmax = layer_integrate_use_softmax)\n\n            if has_hyper_connections:\n                residual_fn = partial(HyperConnection, num_residual_streams = num_residual_streams)\n\n                if layer_type == 'a' and hyper_conn_produce_diff_views:\n                    residual_fn = partial(residual_fn, num_input_views = 3)\n\n            elif gate_residual:\n                residual_fn = GRUGating\n            else:\n                residual_fn = Residual\n\n            residual = residual_fn(dim, layer_index = ind, scale_residual = scale_residual, scale_residual_constant = scale_residual_constant, **residual_fn_kwargs)\n\n            # handle unet skip connection\n\n            skip_combine = None\n            is_latter_half = block_begin and block_ind >= (self.depth / 2)\n\n            if self.unet_skips and is_latter_half:\n                skip_combine = ConcatCombine(dim, skip_indices.pop())\n\n            # all normalizations of the layer\n\n            pre_branch_norm = norm_fn() if pre_norm else None\n            post_branch_norm = norm_fn() if sandwich_norm else None\n            post_main_norm = norm_fn() if not pre_norm else None\n\n            norms = ModuleList([\n                pre_branch_norm,\n                post_branch_norm,\n                post_main_norm\n            ])\n\n            self.skip_combines.append(skip_combine)\n\n            self.layer_integrators.append(layer_integrate)\n\n            self.layers.append(ModuleList([\n                norms,\n                layer,\n                residual\n            ]))\n\n        # determine whether can cache kv\n\n        self.can_cache_kv = all([module.can_cache_kv for module in self.modules() if isinstance(module, Attention)])\n\n    def attn_qk_clip_(\n        self,\n        intermediates: LayerIntermediates,\n        tau = 100.\n    ):\n        # pairs up the attention intermediates with each attention module and does qk clip proposed by kimi team\n\n        layer_and_layer_types = (self.layers, self.layer_types)\n\n        attn_layers = [layer for (_, layer, _), layer_type in zip(self.layers, self.layer_types) if layer_type in ('a', 'c')]\n        attn_intermeds = intermediates.attn_intermediates\n\n        assert len(attn_layers) == len(attn_intermeds)\n\n        for attn_layer, attn_inter in zip(attn_layers, attn_intermeds):\n            attn_layer.qk_clip_(attn_inter, tau = tau)\n\n    def muon_parameters(self):\n        params = []\n\n        for m in self.modules():\n            if not isinstance(m, (Attention, FeedForward)):\n                continue\n\n            params.extend(list(m.muon_parameters()))\n\n        return params\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        context_mask = None,\n        attn_mask = None,\n        self_attn_kv_mask = None,\n        mems = None,\n        mem_masks = None,\n        seq_start_pos: Tensor | None = None,\n        seq_pos_offset: int = 0,\n        cache: LayerIntermediates | None = None,\n        input_not_include_cache = False,\n        cache_age = 1,\n        return_hiddens = False,\n        rotary_pos_emb = None,\n        pos = None,\n        context_pos = None,\n        attn_bias = None,\n        deep_embeds_and_ids: tuple[nn.Parameter, Tensor] | None = None,\n        self_attn_additional_kv: (\n            LayerIntermediates |\n            list[tuple[Tensor, Tensor]]\n            | None\n        ) = None,\n        additional_kv_mask = None,\n        detach_additional_kv = False,\n        route_additional_kv_to_top = True,\n        condition = None,\n        in_attn_cond = None, # https://arxiv.org/abs/2105.04090\n        layers_execute_order: tuple[int, ...] | None = None,\n        self_attn_kv_residuals: Tensor | None = None,\n        cross_attn_kv_residuals: Tensor | None = None\n    ):\n        assert not (self.cross_attend ^ exists(context)), 'context must be passed in if cross_attend is set to True'\n        assert not (exists(condition) ^ self.need_condition), 'condition needs to be passed in if using adaptive layernorm or vice versa'\n\n        # handle condition\n\n        if exists(condition):\n            assert condition.shape[-1] == self.dim_condition, f'expected condition dimension of {self.dim_condition} but received {condition.shape[-1]}'\n\n            assert condition.ndim in {2, 3}\n\n            if condition.ndim == 2:\n                condition = rearrange(condition, 'b d -> b 1 d')\n\n            condition = self.adaptive_mlp(condition)\n\n        # setup maybe layernorm kwarg\n\n        norm_kwargs = dict()\n\n        if self.norm_need_condition:\n            norm_kwargs.update(condition = condition)\n\n        # maybe post branch fn conditioning (DiT paper's ada-ln-zero)\n\n        block_forward_kwargs = dict()\n\n        if self.post_branch_fn_needs_condition:\n            block_forward_kwargs.update(condition = condition)\n\n        # initialize accums\n\n        hiddens = []\n        layer_hiddens = []\n        intermediates = []\n\n        prev_attn = None\n        prev_cross_attn = None\n\n        mems = mems.copy() if exists(mems) else [None] * self.num_attn_layers\n        mem_masks = mem_masks.copy() if exists(mem_masks) else [None] * self.num_attn_layers\n\n        # handle left padded sequences\n\n        if exists(seq_start_pos):\n            seq_arange = arange(x.shape[-2], device = x.device, dtype = torch.long)\n            left_pad_mask = seq_arange >= seq_start_pos[..., None]\n\n            if exists(self_attn_kv_mask):\n                self_attn_kv_mask = self_attn_kv_mask & left_pad_mask\n            else:\n                self_attn_kv_mask = left_pad_mask\n\n        # rotary positions\n\n        cross_attn_rotary_pos_emb = dict()\n\n        if exists(self.rotary_pos_emb):\n            if not exists(rotary_pos_emb):\n                maybe_mem = first(mems, None) # todo - handle edge case where different layers get different memory lengths. don't think this will ever come up but who knows\n                mem_len = maybe_mem.shape[1] if exists(maybe_mem) else 0\n\n                if not exists(pos):\n                    pos = arange(x.shape[1] + mem_len + seq_pos_offset, device = x.device) - mem_len\n\n                rotary_pos_emb = self.rotary_pos_emb(pos)\n\n            # allow for rotary positions for context if provided\n\n            if exists(context_pos):\n                assert self.cross_attend\n                context_rotary_pos_emb = self.rotary_pos_emb(context_pos)\n\n                cross_attn_rotary_pos_emb.update(\n                    rotary_pos_emb = rotary_pos_emb,\n                    context_rotary_pos_emb = context_rotary_pos_emb\n                )\n\n        # assume cached key / values\n\n        prev_cache_length = 0\n\n        attn_cache = []\n\n        if exists(cache):\n            assert self.causal and not exists(attn_mask)\n\n            prev_cache_length = cache.cache_length\n\n            if exists(context):\n                context = context[:, :0]\n\n            if cache_age > 0:\n                x = x[:, -cache_age:] # for spec decoding, may be greater than 1\n\n                if exists(deep_embeds_and_ids):\n                    deep_embeds, token_ids = deep_embeds_and_ids\n                    token_ids = token_ids[:, -cache_age:]\n                    deep_embeds_and_ids = (deep_embeds, token_ids)\n\n            attn_cache = cache.attn_intermediates\n\n        next_cache_length = x.shape[1]\n\n        iter_attn_cache = iter(attn_cache)\n\n        # handle deep embeds if needed\n\n        deep_embeds = []\n\n        if exists(deep_embeds_and_ids):\n            deep_embeds, token_ids = deep_embeds_and_ids\n            deep_embeds_across_depth = deep_embeds[token_ids]\n            deep_embeds = rearrange(deep_embeds_across_depth, 'b n l d -> l b n d')\n\n        deep_embeds_iter = iter(deep_embeds)\n\n        # setup multistreams if needed\n\n        streams = self.num_residual_streams\n        is_multistream = streams > 1\n\n        if is_multistream:\n            x = einx.add('b n d, s d -> (b s) n d', x, self.stream_emb)\n\n        # get layers to be executed\n\n        layer_variables = (\n            self.layer_types,\n            self.skip_combines,\n            self.layers,\n            self.layer_dropouts,\n            self.layer_integrators\n        )\n\n        # able to override the layers execution order on forward, for trying to depth extrapolate\n\n        layers_execute_order = default(layers_execute_order, self.layers_execute_order)\n        layer_variables = tuple(tuple(layer_variable[i] for i in layers_execute_order) for layer_variable in layer_variables)\n\n        # additional self attn key / values - say coming from vlm\n\n        if exists(self_attn_additional_kv) and route_additional_kv_to_top:\n\n            if isinstance(self_attn_additional_kv, LayerIntermediates):\n                self_attn_additional_kv = get_cached_kvs(self_attn_additional_kv)\n\n            if detach_additional_kv:\n                self_attn_additional_kv = detach_all(self_attn_additional_kv)\n\n            num_self_attns = sum([layer_type == 'a' for layer_type in first(layer_variables)])\n\n            self_attn_additional_kv = self_attn_additional_kv[-num_self_attns:]\n            self_attn_additional_kv = [None] * (num_self_attns - len(self_attn_additional_kv)) + self_attn_additional_kv\n\n        iter_self_attn_kv = iter(default(self_attn_additional_kv, ()))\n\n        # derived input for reinjection if needed\n\n        inp_inject = None\n\n        if self.reinject_input:\n            assert not exists(in_attn_cond)\n            inp_inject = self.reinject_input_proj(x)\n\n        elif exists(in_attn_cond):\n            # handle in-attention conditioning, which serves the same purpose of having the network learn the residual\n            inp_inject = in_attn_cond if in_attn_cond.ndim == 3 else rearrange(in_attn_cond, 'b d -> b 1 d')\n\n        if exists(inp_inject) and exists(self.learned_reinject_input_gate):\n            inp_inject_gate = self.learned_reinject_input_gate(x).sigmoid()\n            inp_inject = inp_inject * inp_inject_gate\n\n        # store all hiddens for skips\n\n        skip_hiddens = []\n\n        # for residuals to key value inputs for self and cross attention\n\n        self_attn_kv_residuals_iter = iter((None,))\n        cross_attn_kv_residuals_iter = iter((None,))\n\n        if exists(self_attn_kv_residuals):\n            if self_attn_kv_residuals.ndim == 3:\n                self_attn_kv_residuals = rearrange(self_attn_kv_residuals, '... ->  1 ...')\n\n            self_attn_kv_residuals_iter = iter(self_attn_kv_residuals)\n\n        if exists(cross_attn_kv_residuals):\n            if cross_attn_kv_residuals.ndim == 3:\n                cross_attn_kv_residuals = rearrange(cross_attn_kv_residuals, '... ->  1 ...')\n\n            cross_attn_kv_residuals_iter = iter(cross_attn_kv_residuals)\n\n        # for value residuals\n\n        first_self_attn_inter = None\n        first_cross_attn_inter = None\n\n        # go through the attention and feedforward layers\n\n        for ind, (layer_type, skip_combine, (norm, block, residual_fn), layer_dropout, layer_integrator) in enumerate(zip(*layer_variables)):\n            is_last = ind == (len(self.layers) - 1)\n\n            # handle skip connections\n\n            skip_hiddens.append(x)\n\n            if exists(skip_combine):\n                x = skip_combine(x, skip_hiddens)\n\n            # layer dropout\n\n            if self.training and layer_dropout > 0. and random() < layer_dropout:\n                continue\n\n            if layer_type == 'a':\n                if return_hiddens:\n                    hiddens.append(x)\n\n                layer_mem = mems.pop(0) if mems else None\n                layer_mem_mask = mem_masks.pop(0) if mem_masks else None\n\n            if layer_type == 'c':\n                if self.training and self.cross_attn_tokens_dropout > 0.:\n                    context, context_mask = dropout_seq(context, context_mask, self.cross_attn_tokens_dropout)\n\n            x, inner_residual, residual_kwargs = residual_fn.prepare(x)\n\n            layer_hiddens.append(x)\n\n            if exists(layer_integrator):\n                x = layer_integrator(x, layer_hiddens)\n\n            pre_norm, post_branch_norm, post_main_norm = norm\n\n            if self.need_condition:\n                pre_norm = maybe(partial)(pre_norm, **norm_kwargs)\n                post_branch_norm = maybe(partial)(post_branch_norm, **norm_kwargs)\n                post_main_norm = maybe(partial)(post_main_norm, **norm_kwargs)\n\n            if exists(inp_inject):\n                x = x + inp_inject\n\n            if exists(pre_norm):\n                x = pre_norm(x)\n\n                if layer_type == 'a' and exists(layer_mem):\n                    layer_mem = pre_norm(layer_mem)\n\n            block = partial(block, **block_forward_kwargs)\n\n            # handle maybe value residuals\n\n            maybe_self_attn_value_residual = None\n            maybe_cross_attn_value_residual = None\n\n            if self.add_value_residual:\n                if exists(first_self_attn_inter):\n                    maybe_self_attn_value_residual = first_self_attn_inter.values\n\n                if exists(first_cross_attn_inter):\n                    maybe_cross_attn_value_residual = first_cross_attn_inter.values\n\n            # forward depending on layer type\n\n            if layer_type == 'a':\n                out, inter = block(x, mask = mask, context_mask = self_attn_kv_mask, attn_mask = attn_mask, rel_pos = self.rel_pos, pos = pos, rotary_pos_emb = rotary_pos_emb, additional_key_values = next(iter_self_attn_kv, None), additional_key_value_mask = additional_kv_mask, prev_attn = prev_attn, cache = next(iter_attn_cache, None), mem = layer_mem, mem_mask = layer_mem_mask, attn_bias = attn_bias, kv_input_residual = next(self_attn_kv_residuals_iter, None), value_residual = maybe_self_attn_value_residual, return_intermediates = True)\n            elif layer_type == 'c':\n                out, inter = block(x, context = context, mask = mask, context_mask = context_mask, prev_attn = prev_cross_attn, cache = next(iter_attn_cache, None), kv_input_residual = next(cross_attn_kv_residuals_iter, None), value_residual = maybe_cross_attn_value_residual, **cross_attn_rotary_pos_emb, return_intermediates = True)\n            elif layer_type == 'f':\n                out = block(x, deep_embed = next(deep_embeds_iter, None))\n\n            # store first self or cross attention intermediate for value residual\n\n            if not exists(first_self_attn_inter) and layer_type == 'a':\n                first_self_attn_inter = inter\n\n            if not exists(first_cross_attn_inter) and layer_type == 'c':\n                first_cross_attn_inter = inter\n\n            if exists(post_branch_norm):\n                out = post_branch_norm(out)\n\n            x = residual_fn(out, inner_residual, **residual_kwargs)\n\n            if layer_type in ('a', 'c') and return_hiddens:\n                inter.layer_type = layer_type\n                intermediates.append(inter)\n\n            if layer_type == 'a' and self.residual_attn:\n                prev_attn = inter.pre_softmax_attn\n            elif layer_type == 'c' and self.cross_residual_attn:\n                prev_cross_attn = inter.pre_softmax_attn\n\n            if exists(post_main_norm):\n                x = post_main_norm(x)\n\n        if return_hiddens:\n            layer_hiddens.append(x)\n\n        if self.softclamp_output:\n            x = softclamp(x, self.softclamp_output_value)\n\n        final_norm = self.final_norm\n\n        if self.need_condition:\n            final_norm = maybe(partial)(final_norm, **norm_kwargs)\n\n        # take care of multistreams if needed, use sum for now\n\n        if is_multistream:\n            x = reduce(x, '(b s) n d -> b n d', 'sum', s = streams)\n\n        x = final_norm(x)\n\n        if not return_hiddens:\n            return x\n\n        intermediates = LayerIntermediates(\n            hiddens = hiddens,\n            last_hidden = x,\n            attn_intermediates = intermediates,\n            layer_hiddens = layer_hiddens,\n            cache_length = next_cache_length + prev_cache_length\n        )\n\n        return x, intermediates", "test_code": "", "difficulty": "medium", "category": "attention", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AttentionLayers", "line": 2114}}
{"prompt": "Create a attention pool neural network module", "code": "class AttentionPool(Module):\n    def __init__(\n        self,\n        dim,\n        num_pooled_tokens = 1,\n        dim_context = None,\n        add_residual = False,\n        depth = 1,\n        heads = 8,\n        dim_head = 64,\n        use_transformer_blocks = None,\n        squeeze_output = None,\n        attn_kwargs: dict = dict()\n    ):\n        super().__init__()\n        dim_context = default(dim_context, dim)\n\n        squeeze_output = default(squeeze_output, False)\n        assert not (squeeze_output and num_pooled_tokens > 1)\n\n        use_transformer_blocks = default(use_transformer_blocks, depth > 1)\n        assert use_transformer_blocks or depth == 1\n\n        self.queries = nn.Parameter(torch.randn(num_pooled_tokens, dim) * 1e-2)\n\n        if use_transformer_blocks:\n            assert not add_residual, 'residual already in effect when doing a full cross attention based transformer for pooling'\n            attn_kwargs = {f'attn_{k}': v for k, v in attn_kwargs.items()}\n\n            self.pooler = CrossAttender(dim = dim, cross_attn_dim_context = dim_context, depth = depth, heads = heads, attn_dim_head = dim_head, )\n        else:\n            self.pooler = Attention(dim = dim, dim_context = dim_context, heads = heads, dim_head = dim_head, **attn_kwargs)\n\n        self.add_residual = add_residual\n        self.squeeze_output = squeeze_output\n\n    def forward(self, context, mask = None):\n        batch = context.shape[0]\n\n        queries = repeat(self.queries, 'n d -> b n d', b = batch)\n\n        pooled = self.pooler(queries, context, context_mask = mask)\n\n        if self.add_residual:\n            pooled = pooled + queries\n\n        if self.squeeze_output:\n            pooled = rearrange(pooled, 'b 1 d -> b d')\n\n        return pooled", "test_code": "", "difficulty": "medium", "category": "attention", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "AttentionPool", "line": 2991}}
{"prompt": "Create a vi transformer wrapper neural network module", "code": "class ViTransformerWrapper(Module):\n    def __init__(\n        self,\n        *,\n        image_size,\n        patch_size,\n        attn_layers: Encoder,\n        channels = 3,\n        num_classes = None,\n        post_emb_norm = False,\n        num_register_tokens = 0,\n        emb_dropout = 0.\n    ):\n        super().__init__()\n        assert divisible_by(image_size, patch_size), 'image dimensions must be divisible by the patch size'\n        dim = attn_layers.dim\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_size ** 2\n\n        self.patch_size = patch_size\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n\n        has_register_tokens = num_register_tokens > 0\n        self.has_register_tokens = has_register_tokens\n\n        if has_register_tokens:\n            self.register_tokens = nn.Parameter(torch.randn(num_register_tokens, dim))\n\n        self.patch_to_embedding = nn.Sequential(\n            LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim),\n            LayerNorm(dim)\n        )\n\n        self.post_emb_norm = LayerNorm(dim) if post_emb_norm else nn.Identity()\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.attn_layers = attn_layers\n\n        self.mlp_head = nn.Linear(dim, num_classes) if exists(num_classes) else nn.Identity()\n\n    def forward(\n        self,\n        img,\n        return_embeddings = False,\n        return_logits_and_embeddings = False\n    ):\n        b, p = img.shape[0], self.patch_size\n\n        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n        x = self.patch_to_embedding(x)\n        n = x.shape[1]\n\n        x = x + self.pos_embedding[:, :n]\n\n        x = self.post_emb_norm(x)\n        x = self.dropout(x)\n\n        if self.has_register_tokens:\n            r = repeat(self.register_tokens, 'n d -> b n d', b = b)\n            x, ps = pack((x, r), 'b * d')\n\n        embed = self.attn_layers(x)\n\n        if self.has_register_tokens:\n            embed, _ = unpack(embed, ps, 'b * d')\n\n        assert at_most_one_of(return_embeddings, return_logits_and_embeddings)\n\n        if not exists(self.mlp_head) or return_embeddings:\n            return embed\n\n        pooled = embed.mean(dim = -2)\n        logits = self.mlp_head(pooled)\n\n        if not return_logits_and_embeddings:\n            return logits\n\n        return logits, embed", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "ViTransformerWrapper", "line": 3042}}
{"prompt": "Create a transformer wrapper neural network module", "code": "class TransformerWrapper(Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        max_seq_len,\n        attn_layers: AttentionLayers,\n        embed_num_tokens: dict[str, int] = dict(),\n        emb_dim = None,\n        max_mem_len = 0,\n        shift_mem_down = 0,\n        emb_dropout = 0.,\n        post_emb_norm = False,\n        num_memory_tokens = None,\n        memory_tokens_interspersed_every = None,\n        tie_embedding = False,\n        logits_dim = None,\n        return_only_embed = False,\n        num_output_heads = 1,\n        use_abs_pos_emb = True,\n        scaled_sinu_pos_emb = False,\n        l2norm_embed = False,\n        recycling = False,            # from Jumper et al. - Alphafold2\n        train_max_recycle_steps = 4,  # saw a benefit for language modeling up to 3 recycling steps, so let's default this to 4\n        emb_frac_gradient = 1.,       # GLM-130B and Cogview successfully used this, set at 0.1\n        attn_z_loss_weight = 1e-4,\n        average_pool_embed = False,\n        use_cls_token = False,\n        num_cls_tokens = 1,\n        attn_pool = False,\n        num_pooled_tokens = 1,\n        attn_pool_depth = 1,\n        dim_pooled_tokens = None,\n        squeeze_out_last_dim = False,\n        token_emb: TokenEmbedding | None = None,\n        mixture_of_softmax = False,\n        mixture_of_softmax_k = 4,\n        sigsoftmax_logits = False,\n        ff_deep_embed = False,\n        to_logits: Module | None = None,\n        add_continuous_pred_head = False\n    ):\n        super().__init__()\n\n        dim = attn_layers.dim\n        depth = attn_layers.depth\n\n        emb_dim = default(emb_dim, dim)\n        self.emb_dim = emb_dim\n        self.num_tokens = num_tokens\n        self.num_cls_tokens = num_cls_tokens\n\n        self.max_seq_len = max_seq_len\n        self.max_mem_len = max_mem_len\n        self.shift_mem_down = shift_mem_down\n\n        self.l2norm_embed = l2norm_embed\n\n        if not exists(token_emb):\n            token_emb = TokenEmbedding(emb_dim, num_tokens, l2norm_embed = l2norm_embed)\n\n        self.token_emb = token_emb\n\n        no_abs_pos_emb = max_seq_len == 0 or not (use_abs_pos_emb and not attn_layers.disable_abs_pos_emb)\n\n        if no_abs_pos_emb:\n            self.pos_emb = always(0)\n        elif scaled_sinu_pos_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(emb_dim)\n        else:\n            self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len, l2norm_embed = l2norm_embed)\n\n        # additional embeddings - say type embedding from BERT\n\n        self.embeds = None\n\n        if len(embed_num_tokens) > 0:\n            self.embeds = ModuleDict({f'{name}_embed': nn.Embedding(num_tokens, emb_dim) for name, num_tokens in embed_num_tokens.items()})\n\n        # deep embed\n\n        # credit goes to Braden Koszarsky for first devising value embeddings in nanogpt-speedrun project\n        # then Bo Peng for coming up with this alternate design in feedforward for RWKV 8\n        # improvements were clearest to me (on my toy setup) with multiplying on output of feedforward, will try with attention at future date\n\n        self.ff_deep_embed = None\n        if ff_deep_embed:\n            self.ff_deep_embed = nn.Parameter(torch.ones(num_tokens, depth, dim))\n\n        # fraction of the gradient that should go to the embedding, https://arxiv.org/abs/2105.13290\n\n        self.emb_frac_gradient = emb_frac_gradient\n\n        self.post_emb_norm = LayerNorm(emb_dim) if post_emb_norm else nn.Identity()\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n        self.attn_layers = attn_layers\n\n        self.init_()\n\n        assert num_output_heads > 0\n\n        assert at_most_one_of(average_pool_embed, use_cls_token)\n\n        # maybe recycling\n\n        self.recycling = recycling\n        self.recycled_proj = LinearNoBias(dim, dim) if recycling else None\n\n        self.train_max_recycle_steps = train_max_recycle_steps\n\n        # either cls token or attn pool, but not both\n\n        assert not (use_cls_token and attn_pool)\n\n        # classic cls token from the bert days\n\n        self.cls_token = None\n\n        if use_cls_token:\n            self.cls_token = nn.Parameter(torch.zeros(num_cls_tokens, dim))\n            nn.init.normal_(self.cls_token, std = 0.02)\n\n        # attn pool\n\n        self.attn_pool = None\n\n        if attn_pool:\n            self.attn_pool = AttentionPool(dim = default(dim_pooled_tokens, dim), dim_context = dim, num_pooled_tokens = num_pooled_tokens, depth = attn_pool_depth, heads = self.attn_layers.attn_heads, dim_head = self.attn_layers.attn_dim_head)\n\n        # whether to average pool the embed (`global average pool`)\n\n        self.average_pool_embed = average_pool_embed\n\n        # output type\n\n        self.output_is_log_prob = mixture_of_softmax\n\n        self.to_mixture = None\n        self.combine_mixture = None\n\n        if mixture_of_softmax:\n            assert num_output_heads == 1\n\n            self.to_mixture = Sequential(\n                LinearNoBias(dim, dim * mixture_of_softmax_k),\n                Rearrange('... (k d) -> ... k d', k = mixture_of_softmax_k)\n            )\n\n            self.combine_mixture = LinearNoBias(dim, mixture_of_softmax_k)\n\n        # sig softmax\n\n        self.sigsoftmax_logits = sigsoftmax_logits\n\n        # output head, usually to logits of num_tokens\n\n        logits_dim = default(logits_dim, num_tokens)\n\n        self.has_multiple_heads = num_output_heads > 1\n\n        if return_only_embed:\n            self.to_logits = None\n        elif tie_embedding:\n            assert isinstance(token_emb, TokenEmbedding), 'can only tie embedding if using `TokenEmbedding`'\n            self.to_logits = lambda t: t @ self.token_emb.emb.weight.t()\n        elif num_output_heads > 1:\n            self.to_logits = ModuleList([LinearNoBias(dim, logits_dim) for _ in range(num_output_heads)])\n        else:\n            self.to_logits = LinearNoBias(dim, logits_dim) if not exists(to_logits) else to_logits\n\n        # add a head that predicts the embedding of the next step\n\n        self.add_continuous_pred_head = add_continuous_pred_head\n\n        if add_continuous_pred_head:\n\n            self.to_next_embed_pred = nn.Sequential(\n                LinearNoBias(dim, dim),\n                nn.SiLU(),\n                LinearNoBias(dim, dim)\n            )\n\n        # memory tokens (like [cls]) from Memory Transformers paper\n\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.num_memory_tokens = num_memory_tokens\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n\n        self.memory_tokens_interspersed_every = memory_tokens_interspersed_every\n\n        # squeeze out last dimension if possible\n\n        self.squeeze_out_last_dim = squeeze_out_last_dim\n\n        # whether can do cached kv decoding\n\n        self.can_cache_kv = self.num_memory_tokens == 0 and not recycling and self.attn_layers.can_cache_kv\n        self.can_cache_kv_outside_max_seq_len = no_abs_pos_emb\n\n    def init_(self):\n        if hasattr(self.token_emb, 'init_'):\n            self.token_emb.init_()\n\n        if self.l2norm_embed:\n            if not isinstance(self.pos_emb, always):\n                nn.init.normal_(self.pos_emb.emb.weight, std = 1e-5)\n\n    def attn_qk_clip_(\n        self,\n        intermediates: LayerIntermediates,\n        tau = 100.\n    ):\n        self.attn_layers.attn_qk_clip_(intermediates, tau = tau)\n\n    def muon_parameters(self):\n        return self.attn_layers.muon_parameters()\n\n    def forward(\n        self,\n        x,\n        return_embeddings = False,\n        return_logits_and_embeddings = False,\n        return_intermediates = False,\n        return_embeddings_and_intermediates = False,\n        return_logit_entropies = False,\n        return_next_embed_pred = False,\n        mask = None,\n        return_mems = False,\n        return_attn = False,\n        mems = None,\n        mem_masks = None,\n        recycle_steps = None,\n        pos = None,\n        prepend_embeds = None,\n        prepend_mask = None,\n        embed_ids: dict[str, Tensor] = dict(),\n        sum_embeds = None,\n        return_attn_z_loss = False,\n        attn_z_loss_weight = 1e-4,\n        seq_start_pos = None,\n        cache: LayerIntermediates | None = None,\n        input_not_include_cache = False,\n        token_emb_kwargs = dict(),\n        to_logits_kwargs = dict(),\n        **kwargs,\n    ):\n\n        # if sequence is None, auto create an empty one if `prepend_embeds` was supplied\n\n        if not exists(x):\n            assert exists(prepend_embeds)\n            x = prepend_embeds.new_empty((prepend_embeds.shape[0], 0), dtype = torch.long)\n\n        # shapes and variables\n\n        b, n, device, token_ids, num_mems, has_memory_tokens, emb_frac_gradient, orig_mask = x.shape[0], x.shape[1], x.device, x, self.num_memory_tokens, self.num_memory_tokens > 0, self.emb_frac_gradient, mask\n\n        return_hiddens = return_mems | return_attn | return_intermediates | return_attn_z_loss | return_embeddings_and_intermediates\n        return_embeddings = return_embeddings | (not exists(self.to_logits)) | return_embeddings_and_intermediates\n\n        # take care of position embedding offsets in the presence of cache and sequence is less than cache length (not full sequence)\n\n        seq_pos_offset = 0\n\n        if exists(cache) and input_not_include_cache:\n            seq_pos_offset = cache.cache_length\n\n        # absolute positional embedding\n\n        external_pos_emb = exists(pos) and pos.dtype != torch.long\n        pos_emb = self.pos_emb(x, pos = pos, seq_start_pos = seq_start_pos, offset = seq_pos_offset) if not external_pos_emb else pos\n        x = self.token_emb(x, **token_emb_kwargs) + pos_emb\n\n        # add additional embeddings\n\n        assert not (exists(self.embeds) ^ (len(embed_ids) > 0)), '`embed_num_tokens` must be defined on `TransformerWrapper`'\n\n        if exists(self.embeds):\n            assert len(embed_ids) == len(self.embeds)\n\n            for name, embed_id in embed_ids.items():\n                embed_key = f'{name}_embed'\n\n                assert embed_key in self.embeds\n                embed = self.embeds[embed_key](embed_id)\n\n                x = x + embed\n\n        # for summing embeddings passed externally - needs this for self-conditioning in non-autoregressive training\n\n        if exists(sum_embeds):\n            x = x + sum_embeds\n\n        # post embedding norm, purportedly leads to greater stabilization\n\n        x = self.post_emb_norm(x)\n\n        # whether to append embeds, as in PaLI, for image embeddings\n\n        if exists(prepend_embeds):\n            prepend_seq, prepend_dim = prepend_embeds.shape[1:]\n            assert prepend_dim == x.shape[-1], 'prepended embeddings need to have same dimensions as text model dimensions'\n\n            x = cat((prepend_embeds, x), dim = -2)\n\n            if exists(prepend_mask) or exists(mask):\n                mask = default(mask, lambda: torch.ones((b, n), device = device, dtype = torch.bool))\n                prepend_mask = default(prepend_mask, lambda: torch.ones((b, prepend_seq), device = device, dtype = torch.bool))\n\n                mask = cat((prepend_mask, mask), dim = -1)\n\n        # whether to reduce the gradient going to the embedding, from cogview paper, corroborated by GLM-130B model\n\n        if emb_frac_gradient < 1:\n            assert emb_frac_gradient > 0\n            x = x * emb_frac_gradient + x.detach() * (1 - emb_frac_gradient)\n\n        # init embed\n\n        init_embed = x\n\n        # embedding dropout\n\n        x = self.emb_dropout(x)\n\n        x = self.project_emb(x)\n\n        # maybe deep embeds\n\n        deep_embed_and_ids = None\n\n        if exists(self.ff_deep_embed):\n            deep_embed_and_ids = (self.ff_deep_embed, token_ids)\n\n        # maybe cls token\n\n        if exists(self.cls_token):\n            cls_tokens = repeat(self.cls_token, '... -> b ...', b = b)\n            x, cls_packed_shape = pack([cls_tokens, x], 'b * d')\n\n            if exists(mask):\n                mask = F.pad(mask, (self.num_cls_tokens, 0), value = True)\n\n        # maybe memory / register tokens\n\n        if has_memory_tokens:\n            mem_seq = x.shape[-2]\n            mem_every = self.memory_tokens_interspersed_every\n\n            if exists(mem_every):\n                assert mem_every > 0\n                assert isinstance(self.attn_layers, Decoder), 'only for decoder'\n                next_seq_len = math.ceil(n / mem_every) * mem_every\n\n                x = pad_at_dim(x, (0, next_seq_len - n), dim = -2, value = 0.)\n                x = rearrange(x, 'b (n m) d -> (b n) m d', m = mem_every)\n\n            mem = repeat(self.memory_tokens, 'n d -> b n d', b = x.shape[0])\n            x, mem_packed_shape = pack((mem, x), 'b * d')\n\n            # auto-handle masking after appending memory tokens\n            if not exists(mem_every) and exists(mask):\n                mask = pad_at_dim(mask, (num_mems, 0), dim = -1, value = True)\n\n            if exists(mem_every):\n                x = rearrange(x, '(b n) m d -> b (n m) d', b = b)\n\n        # handle maybe shifting of memories\n\n        if self.shift_mem_down and exists(mems):\n            mems_l, mems_r = mems[:self.shift_mem_down], mems[self.shift_mem_down:]\n            mems = [*mems_r, *mems_l]\n\n        # attn layers kwargs\n\n        kwargs = dict(\n            **kwargs,\n            pos = pos,\n            seq_pos_offset = seq_pos_offset,\n            seq_start_pos = seq_start_pos,\n            input_not_include_cache = input_not_include_cache\n        )\n\n        # attention layers\n\n        if not self.recycling:\n            assert not exists(recycle_steps) or recycle_steps == 1, 'you did not train with recycling'\n\n            # regular\n\n            attended, intermediates = self.attn_layers(x, mask = mask, mems = mems, mem_masks = mem_masks, cache = cache, deep_embeds_and_ids = deep_embed_and_ids, return_hiddens = True, **kwargs)\n\n        else:\n            # recycling\n\n            recycle_steps = default(recycle_steps, (randrange(self.train_max_recycle_steps) + 1) if self.training else None)\n            assert exists(recycle_steps) and recycle_steps > 0, '`recycle_steps` must be provided on forward if recycling is turned on and not training'\n\n            for i in range(recycle_steps):\n                first_step = i == 0\n                last_step = i == (recycle_steps - 1)\n\n                context = nullcontext if last_step else torch.no_grad\n\n                with context():\n                    maybe_recycled = self.recycled_proj(attended.detach()) if not first_step else 0.\n\n                    attended, intermediates = self.attn_layers(x + maybe_recycled, mask = mask, mems = mems, mem_masks = mem_masks, cache = cache, return_hiddens = True, **kwargs)\n\n        x = attended\n\n        # handle memories post-attention\n\n        if has_memory_tokens:\n            if exists(mem_every):\n                x = rearrange(x, 'b (n m) d -> (b n) m d', m = (mem_every + num_mems))\n\n            mem, x = unpack(x, mem_packed_shape, 'b * d')\n\n            intermediates.memory_tokens = mem\n\n            if exists(mem_every):\n                x = rearrange(x, '(b n) m d -> b (n m) d', b = b)\n\n            x = x[:, :mem_seq]\n\n        # store last layer hiddens, for access in case of cls token or attention pooling\n\n        intermediates.last_layer_hiddens = x\n\n        # store initial embed\n\n        intermediates.initial_embed = init_embed\n\n        # global average pool\n\n        if self.average_pool_embed:\n            x = masked_mean(x, mask = orig_mask, dim = 1)\n\n        # cls token(s)\n\n        if exists(self.cls_token):\n            x, last_layer_hiddens = unpack(x, cls_packed_shape, 'b * d')\n\n            intermediates.last_layer_hiddens = last_layer_hiddens\n\n            if x.shape[1] == 1:\n                x = rearrange(x, 'b 1 d -> b d')  # Remove sequence dimension if num_cls_tokens=1 to keep previous behavior\n\n        # attention pool\n\n        is_encoder = not self.attn_layers.causal\n        return_pooled_tokens = exists(self.attn_pool) and is_encoder\n\n        if (\n            exists(self.attn_pool) and\n            (return_intermediates or is_encoder) # in a new paper, they use attention pooling on decoder - so we'll default to returning pooled tokens if encoder, but for decoder, they must set `return_intermediates`\n        ):\n\n            attn_pooled_tokens = self.attn_pool(x, mask = mask)\n\n            intermediates.attn_pooled_tokens = attn_pooled_tokens\n\n        # handle expansion to mixture if needed (for mixture of softmax)\n\n        combine_mixture = None\n\n        if exists(self.to_mixture):\n            combine_mixture = self.combine_mixture(x).softmax(dim = -1)\n            x = self.to_mixture(x)\n\n        # projecting to logits\n\n        if not return_embeddings:\n            if self.has_multiple_heads:\n                logits = tuple(fn(x, **to_logits_kwargs) for fn in self.to_logits)\n            else:\n                logits = self.to_logits(x, **to_logits_kwargs)\n\n        # maybe sig softmax\n\n        if self.sigsoftmax_logits:\n            logits = logits + logits.sigmoid().log()\n\n        # handle maybe combine mixture\n\n        if exists(combine_mixture):\n            with autocast('cuda', enabled = False):\n                prob = logits.softmax(dim = -1)\n                mos = einsum('... k d, ... k -> ... d', prob, combine_mixture)\n                logits = log(mos)\n\n        # maybe squeeze out last dimension of logits\n\n        if self.squeeze_out_last_dim:\n            logits = tuple((rearrange(t, '... 1 -> ...') if t.shape[-1] == 1 else t) for t in cast_tuple(logits))\n\n            if not self.has_multiple_heads:\n                logits = first(logits)\n\n        # different returns\n\n        if return_logits_and_embeddings:\n            out = (logits, x)\n        elif return_embeddings_and_intermediates:\n            out = (x, intermediates)\n        elif return_embeddings:\n            out = x\n        elif return_pooled_tokens:\n            intermediates.logits = logits\n            out = attn_pooled_tokens\n        else:\n            out = logits\n\n        # maybe next embed pred\n\n        if return_next_embed_pred:\n            assert self.add_continuous_pred_head\n            next_embed_out = self.to_next_embed_pred(x)\n\n            out = (out, (next_embed_out, init_embed))\n\n        # logit entropies\n\n        if return_logit_entropies:\n            intermediates.logit_entropies = calc_entropy(logits)\n            return_intermediates = True\n\n        # aux loss\n\n        if return_attn_z_loss:\n            pre_softmax_attns = [t.pre_softmax_attn for t in  intermediates.attn_intermediates]\n            intermediates.attn_z_loss = calc_z_loss(pre_softmax_attns, weight = attn_z_loss_weight)\n            return_intermediates = True\n\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = [cat(pair, dim = -2) for pair in zip(mems, hiddens)] if exists(mems) else hiddens\n            new_mems = [t[..., -self.max_mem_len:, :].detach() for t in new_mems]\n\n            if not return_intermediates:\n                return out, new_mems\n\n            intermediates.mems = new_mems\n\n        if return_intermediates:\n            return out, intermediates\n\n        if return_attn:\n            attn_maps = [t.post_softmax_attn for t in intermediates.attn_intermediates]\n            return out, attn_maps\n\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "TransformerWrapper", "line": 3123}}
{"prompt": "Create a x transformer neural network module", "code": "class XTransformer(Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        tie_token_emb = False,\n        ignore_index = -100,\n        pad_value = 0,\n        cross_attn_tokens_dropout = 0.,\n        **kwargs\n    ):\n        super().__init__()\n        enc_kwargs, kwargs = groupby_prefix_and_trim('enc_', kwargs)\n        dec_kwargs, kwargs = groupby_prefix_and_trim('dec_', kwargs)\n\n        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs, 'dimension of either encoder or decoder must be set with `dim` keyword'\n        enc_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], enc_kwargs)\n        enc_transformer_kwargs['emb_dropout'] = enc_kwargs.pop('emb_dropout', 0)\n        enc_transformer_kwargs['num_memory_tokens'] = enc_kwargs.pop('num_memory_tokens', None)\n        enc_transformer_kwargs['scaled_sinu_pos_emb'] = enc_kwargs.pop('scaled_sinu_pos_emb', False)\n        enc_transformer_kwargs['use_abs_pos_emb'] = enc_kwargs.pop('use_abs_pos_emb', True)\n\n        dec_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], dec_kwargs)\n        dec_transformer_kwargs['emb_dropout'] = dec_kwargs.pop('emb_dropout', 0)\n        dec_transformer_kwargs['scaled_sinu_pos_emb'] = dec_kwargs.pop('scaled_sinu_pos_emb', False)\n        dec_transformer_kwargs['use_abs_pos_emb'] = dec_kwargs.pop('use_abs_pos_emb', True)\n\n        self.cross_attn_tokens_dropout = cross_attn_tokens_dropout  # how many tokens from the encoder to dropout when cross attending from decoder - seen in a couple papers, including Perceiver AR - this will also be very effective regularization when cross attending to very long memories\n\n        self.encoder = TransformerWrapper(\n            **enc_transformer_kwargs,\n            return_only_embed = True,\n            attn_layers = Encoder(dim = dim, **enc_kwargs)\n        )\n\n        self.decoder = TransformerWrapper(\n            **dec_transformer_kwargs,\n            attn_layers = Decoder(dim = dim, cross_attend = True, **dec_kwargs)\n        )\n\n        if tie_token_emb:\n            self.decoder.token_emb = self.encoder.token_emb\n\n        self.decoder = AutoregressiveWrapper(self.decoder, ignore_index=ignore_index, pad_value=pad_value)\n\n    @torch.no_grad()\n    def generate(self, seq_in, seq_out_start, seq_len, mask = None, attn_mask = None, **kwargs):\n        encodings = self.encoder(seq_in, mask = mask, attn_mask = attn_mask, return_embeddings = True)\n        return self.decoder.generate(seq_out_start, seq_len, context = encodings, context_mask = mask, **kwargs)\n\n    def forward(self, src, tgt, mask = None, attn_mask = None, src_prepend_embeds = None):\n\n        enc = self.encoder(src, mask = mask, attn_mask = attn_mask, prepend_embeds = src_prepend_embeds, return_embeddings = True)\n\n        if exists(src_prepend_embeds) and exists(mask):\n            mask = pad_at_dim(mask, (src_prepend_embeds.shape[-2], 0), dim = -1, value = True)\n\n        if self.training and self.cross_attn_tokens_dropout > 0:\n            enc, mask = dropout_seq(enc, mask, self.cross_attn_tokens_dropout)\n\n        out = self.decoder(tgt, context = enc, context_mask = mask)\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/x_transformers.py", "name": "XTransformer", "line": 3680}}
{"prompt": "Create a x val transformer wrapper neural network module", "code": "class XValTransformerWrapper(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        max_seq_len,\n        numerical_token_id,\n        attn_layers: AttentionLayers,\n        emb_dim = None,\n        logits_dim = None,\n        tie_embedding = False,\n        max_mem_len = 0,\n        num_memory_tokens = None,\n        emb_dropout = 0.,\n        use_abs_pos_emb = True,\n        scaled_sinu_pos_emb = False\n    ):\n        super().__init__()\n        dim = attn_layers.dim\n        emb_dim = default(emb_dim, dim)\n\n        self.emb_dim = emb_dim\n        self.token_emb = TokenEmbedding(emb_dim, num_tokens)\n\n        self.numerical_token_id = numerical_token_id\n\n        self.max_seq_len = max_seq_len\n\n        self.max_mem_len = max_mem_len\n\n        if not (use_abs_pos_emb and not attn_layers.disable_abs_pos_emb):\n            self.pos_emb = always(0)\n        elif scaled_sinu_pos_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(dim)\n        else:\n            self.pos_emb = AbsolutePositionalEmbedding(dim, max_seq_len)\n\n        self.emb_dropout = nn.Dropout(emb_dropout)\n\n        # memory tokens\n\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.has_memory_tokens = num_memory_tokens > 0\n\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n\n        # attention layers\n\n        self.attn_layers = attn_layers\n\n        # to logits\n\n        logits_dim = default(logits_dim, num_tokens)\n        self.to_logits = nn.Linear(dim, logits_dim) if not tie_embedding else lambda t: t @ self.token_emb.emb.weight.t()\n\n        self.to_numerical_output = nn.Sequential(\n            nn.Linear(dim, 1),\n            Rearrange('... 1 -> ...')\n        )\n\n    def forward(\n        self,\n        x: Tensor,\n        x_num: Tensor,\n        return_embeddings = False,\n        return_intermediates = False,\n        return_mems = False,\n        mask = None,\n        return_attn = False,\n        mems = None,\n        pos = None,\n        prepend_embeds = None,\n        **kwargs\n    ):\n        assert x.shape == x_num.shape\n\n        batch = x.shape[0]\n\n        is_number_mask = x == self.numerical_token_id\n\n        x = self.token_emb(x)\n\n        scale = torch.where(is_number_mask, x_num, 1.)\n        scale = rearrange(scale, '... -> ... 1')\n\n        x = x * scale\n\n        x = x + self.pos_emb(x, pos = pos)\n\n        # memory tokens\n\n        if self.has_memory_tokens:\n            m = repeat(self.memory_tokens, 'm d -> b m d', b = batch)\n            x, mem_ps = pack([m, x], 'b * d')\n\n            if exists(mask):\n                num_mems = m.shape[-2]\n                mask = pad_at_dim(mask, (num_mems, 0), dim = -1, value = True)\n\n        # whether to append embeds, as in PaLI, for image embeddings\n\n        if exists(prepend_embeds):\n            _, prepend_dim = prepend_embeds.shape[1:]\n            assert prepend_dim == x.shape[-1], 'prepended embeddings need to have same dimensions as model dimensions'\n\n            x = torch.cat((prepend_embeds, x), dim = -2)\n\n        x = self.emb_dropout(x)\n\n        # attention layers\n\n        x, intermediates = self.attn_layers(x, mask = mask, mems = mems, return_hiddens = True, **kwargs)\n\n        # splice out memory tokens\n\n        if self.has_memory_tokens:\n            m, x = unpack(x, mem_ps, 'b * d')\n            intermediates.memory_tokens = m\n\n        if not return_embeddings:\n            logits = self.to_logits(x)\n            numerical_pred = self.to_numerical_output(x)\n            out = (logits, numerical_pred)\n        else:\n            out = x\n\n        if return_intermediates:\n            return out, intermediates\n\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = tuple(t[..., -self.max_mem_len:, :].detach() for t in hiddens)\n            return out, new_mems\n\n        if return_attn:\n            attn_maps = tuple(t.post_softmax_attn for t in intermediates.attn_intermediates)\n            return out, attn_maps\n\n        return out", "test_code": "", "difficulty": "medium", "category": "transformer", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/xval.py", "name": "XValTransformerWrapper", "line": 48}}
{"prompt": "Create a x val autoregressive wrapper neural network module", "code": "class XValAutoregressiveWrapper(nn.Module):\n    def __init__(\n        self,\n        net: XValTransformerWrapper,\n        ignore_index = -100,\n        pad_value = 0,\n        numerical_loss_weight = 1.\n    ):\n        super().__init__()\n        self.net = net\n        self.max_seq_len = net.max_seq_len\n        self.numerical_loss_weight = numerical_loss_weight\n        self.ignore_index = ignore_index\n\n    @torch.no_grad()\n    def generate(\n        self,\n        start_tokens: Tensor,\n        start_numbers: Tensor,\n        seq_len,\n        filter_logits_fn: Callable = top_k,\n        filter_kwargs: dict = dict(),\n        temperature = 1.,\n        **kwargs\n    ):\n        device = start_tokens.device\n        was_training = self.net.training\n        num_dims = len(start_tokens.shape)\n\n        assert num_dims >= 2, 'number of dimensions of your start tokens must be greater or equal to 2'\n        assert start_tokens.shape == start_numbers.shape\n\n        b, t, device = *start_tokens.shape, start_tokens.device\n\n        self.net.eval()\n        out = start_tokens\n        num_out = start_numbers\n\n        for _ in range(seq_len):\n            x = out[:, -self.max_seq_len:]\n            x_num = num_out[:, -self.max_seq_len:]\n\n            logits, numerical_pred = self.net(x, x_num, **kwargs)\n\n            last_logits = logits[:, -1]\n            last_num_pred = numerical_pred[:, -1:]\n\n            filtered_logits = filter_logits_fn(last_logits, **filter_kwargs)\n\n            probs = F.softmax(filtered_logits / temperature, dim=-1)\n\n            sample = torch.multinomial(probs, 1)\n\n            out = torch.cat((out, sample), dim = -1)\n            num_out = torch.cat((num_out, last_num_pred), dim = -1)\n\n        out = out[:, t:]\n        num_out = num_out[:, t:]\n\n        is_number = out == self.net.numerical_token_id\n        num_out = torch.where(is_number, num_out, float('nan'))\n\n        self.net.train(was_training)\n        return GenerateReturn(out, num_out, is_number)\n\n    def forward(\n        self,\n        x: Tensor,\n        x_num: Tensor,\n        return_loss_breakdown = False,\n        **kwargs\n    ):\n        inp, target = x[:, :-1], x[:, 1:]\n        x_num_inp, x_num_target = x_num[:, :-1], x_num[:, 1:]\n\n        # ignore index\n\n        target_mask = target != self.ignore_index\n\n        # key padding mask\n\n        mask = kwargs.get('mask', None)\n        if exists(mask):\n            target_mask &= mask\n\n            if mask.shape[1] == x.shape[1]:\n                mask = mask[:, :-1]\n                kwargs['mask'] = mask\n\n        logits, numerical_pred = self.net(inp, x_num_inp, **kwargs)\n\n        logits = rearrange(logits, 'b n c -> b c n')\n\n        cross_entropy_loss = F.cross_entropy(logits, target, reduction = 'none', ignore_index = self.ignore_index)\n\n        # protect against nan in `x_num` input tensor\n\n        target_is_number_mask = target == self.net.numerical_token_id\n        x_num_target = x_num_target.masked_fill(~target_is_number_mask, 0.)\n\n        # numerical mse loss\n\n        numerical_mse_loss = F.mse_loss(numerical_pred, x_num_target, reduction = 'none')\n\n        numerical_mse_loss = numerical_mse_loss * target_mask\n        numerical_mse_loss = numerical_mse_loss.masked_fill(~target_is_number_mask, 0.)\n\n        # combine losses\n\n        loss = cross_entropy_loss + numerical_mse_loss * self.numerical_loss_weight\n\n        loss = loss[target_mask]\n        loss = loss.mean()\n\n        if not return_loss_breakdown:\n            return loss\n\n        return loss, LossBreakdown(cross_entropy_loss, numerical_mse_loss)", "test_code": "", "difficulty": "medium", "category": "other", "source": "github:lucidrains/x-transformers", "metadata": {"source_file": "x_transformers/xval.py", "name": "XValAutoregressiveWrapper", "line": 189}}
